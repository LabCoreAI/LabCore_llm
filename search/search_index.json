{"config":{"lang":["en","fr"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"LabCore LLM","text":"<p>Practical decoder-only GPT framework for local training, evaluation, and deployment.</p> <p> </p>"},{"location":"#quick-install","title":"Quick Install","text":"<pre><code>pip install -e \".[torch]\"\n</code></pre>"},{"location":"#quick-example","title":"Quick Example","text":"<p>Tiny Shakespeare with the char tokenizer:</p> <pre><code>python scripts/prepare_data.py --dataset tinyshakespeare --tokenizer char --output-format txt\npython train.py --config configs/base.toml --tokenizer char --max-iters 200\npython generate.py --checkpoint checkpoints/ckpt_last.pt --meta data/processed/meta.json --tokenizer char --prompt \"To be\"\n</code></pre>"},{"location":"#what-you-get","title":"What You Get","text":"<ul> <li>Decoder-only GPT training stack with config-driven workflows.</li> <li>Optional RoPE positional encoding.</li> <li>Optional FlashAttention/SDPA attention path.</li> <li>Memory-mapped binary data pipeline (<code>train.bin</code>/<code>val.bin</code>).</li> <li>Hugging Face export workflow.</li> <li>GGUF conversion and quantization path.</li> <li>LoRA fine-tuning entrypoint for instruction workflows.</li> </ul>"},{"location":"#end-to-end-flow","title":"End-to-End Flow","text":"<pre><code>Raw text corpus\n  -&gt; scripts/prepare_data.py\n  -&gt; train.py (+ config preset)\n  -&gt; checkpoints/ckpt_last.pt\n  -&gt; generate.py / demo_gradio.py\n  -&gt; scripts/export_hf.py\n  -&gt; scripts/quantize_gguf.py (optional)\n  -&gt; scripts/fine_tune_instruction.py (optional)\n</code></pre>"},{"location":"#documentation-map","title":"Documentation Map","text":""},{"location":"#guides","title":"Guides","text":"<ul> <li>Getting Started: environment setup and first run.</li> <li>Data Pipeline: build <code>txt</code> or <code>bin</code> datasets + metadata.</li> <li>Training: run training presets and monitor checkpoints.</li> <li>Inference &amp; Demo: CLI generation and Gradio workflows.</li> <li>Fine-Tuning: LoRA instruction-tuning workflow.</li> <li>Export &amp; Deployment: HF export and GGUF path.</li> </ul>"},{"location":"#reference","title":"Reference","text":"<ul> <li>Configuration: TOML sections, defaults, and examples.</li> <li>Operations: release/security/support operations.</li> <li>Troubleshooting: common errors and quick fixes.</li> <li>Benchmarks: template for performance tracking.</li> </ul>"},{"location":"#development","title":"Development","text":"<ul> <li>Developer Guide: local dev workflow, tests, and standards.</li> </ul>"},{"location":"#quick-next-steps","title":"Quick Next Steps","text":"<ul> <li>Getting Started</li> <li>Training</li> <li>Export &amp; Deployment</li> <li>Operations for quality checks, hardware guidance, license, and disclaimer.</li> </ul>"},{"location":"benchmarks/","title":"Benchmarks","text":"<p>Use this page to track reproducible performance numbers across presets and devices.</p>"},{"location":"benchmarks/#benchmark-template","title":"Benchmark Template","text":"Preset Device Batch / Seq / Accum Throughput (it/s or tok/s) Max VRAM Notes <code>configs/base.toml</code> <code>cpu</code> <code>32 / 128 / 1</code> <code>0.00</code> <code>0.0 GB</code> placeholder"},{"location":"benchmarks/#minimal-example-entry","title":"Minimal Example Entry","text":"<p>Use one line per run with exact settings:</p> Preset Device Batch / Seq / Accum Throughput (it/s or tok/s) Max VRAM Notes <code>configs/bpe_rope_flash/bpe_30M_rope_flash.toml</code> <code>cuda</code> <code>8 / 512 / 8</code> <code>TBD</code> <code>TBD</code> fill after first stable run"},{"location":"benchmarks/#benchmark-tips","title":"Benchmark Tips","text":"<ul> <li>Warm up for a few iterations before recording.</li> <li>Keep dataset, tokenizer, and precision consistent between runs.</li> <li>Log GPU model, driver, and PyTorch version in notes.</li> </ul>"},{"location":"configuration-reference/","title":"Configuration Reference","text":"<p><code>train.py</code> reads TOML configs and applies defaults through <code>load_config</code>.</p>"},{"location":"configuration-reference/#file-sections","title":"File Sections","text":""},{"location":"configuration-reference/#general","title":"<code>[general]</code>","text":"<ul> <li><code>run_name</code>: free-form experiment name.</li> <li><code>seed</code>: random seed value.</li> <li><code>tokenizer</code>: <code>char</code> or <code>bpe</code>.</li> </ul>"},{"location":"configuration-reference/#data","title":"<code>[data]</code>","text":"<ul> <li><code>dataset</code>: logical dataset label.</li> <li><code>processed_dir</code>: path where prepared artifacts live.</li> </ul>"},{"location":"configuration-reference/#model","title":"<code>[model]</code>","text":"<ul> <li><code>vocab_size</code></li> <li><code>block_size</code></li> <li><code>n_layer</code></li> <li><code>n_head</code></li> <li><code>n_embd</code></li> <li><code>dropout</code></li> <li><code>bias</code></li> <li><code>use_rope</code></li> <li><code>use_flash</code></li> </ul>"},{"location":"configuration-reference/#optimizer","title":"<code>[optimizer]</code>","text":"<ul> <li><code>learning_rate</code></li> <li><code>weight_decay</code></li> <li><code>beta1</code></li> <li><code>beta2</code></li> <li><code>grad_clip</code></li> </ul>"},{"location":"configuration-reference/#training","title":"<code>[training]</code>","text":"<ul> <li><code>batch_size</code></li> <li><code>gradient_accumulation_steps</code></li> <li><code>max_iters</code></li> <li><code>warmup_iters</code></li> <li><code>lr_decay_iters</code></li> <li><code>min_lr</code></li> <li><code>eval_interval</code></li> <li><code>eval_iters</code></li> <li><code>log_interval</code></li> <li><code>save_interval</code></li> <li><code>device</code></li> <li><code>checkpoint_dir</code></li> <li><code>data_format</code> (<code>txt</code> or <code>bin</code>)</li> </ul>"},{"location":"configuration-reference/#generation","title":"<code>[generation]</code>","text":"<p>Used mainly for defaults in docs/examples:</p> <ul> <li><code>max_new_tokens</code></li> <li><code>temperature</code></li> <li><code>top_k</code></li> </ul>"},{"location":"configuration-reference/#built-in-defaults","title":"Built-in Defaults","text":"<p><code>load_config</code> currently ensures:</p> <ul> <li><code>general.tokenizer = \"char\"</code> if missing</li> <li><code>data.processed_dir = \"data/processed\"</code> if missing</li> <li><code>model.use_rope = false</code> if missing</li> <li><code>model.use_flash = false</code> if missing</li> <li><code>training.data_format = \"txt\"</code> if missing</li> </ul>"},{"location":"configuration-reference/#example-minimal-config","title":"Example: Minimal Config","text":"<pre><code>[data]\nprocessed_dir = \"data/processed\"\n\n[model]\nblock_size = 128\nn_layer = 6\nn_head = 8\nn_embd = 256\nvocab_size = 65\n\n[training]\nbatch_size = 32\nmax_iters = 1000\ndevice = \"cpu\"\n</code></pre>"},{"location":"configuration-reference/#example-50m-ropeflash-style","title":"Example: 50M RoPE/Flash Style","text":"<p>Use <code>configs/bpe_rope_flash/bpe_50M_rope_flash.toml</code> as the baseline for binary pipelines.</p>"},{"location":"data-pipeline/","title":"Data Pipeline","text":"<p>LabCore supports two dataset output formats:</p> <ul> <li><code>txt</code> format for text + numpy training flows</li> <li><code>bin</code> format for memory-mapped binary training flows</li> </ul>"},{"location":"data-pipeline/#main-command","title":"Main Command","text":"<pre><code>python scripts/prepare_data.py \\\n  --dataset tinyshakespeare \\\n  --tokenizer char \\\n  --output-format txt \\\n  --raw-dir data/raw \\\n  --output-dir data/processed \\\n  --val-ratio 0.1\n</code></pre>"},{"location":"data-pipeline/#cli-options","title":"CLI Options","text":"<ul> <li><code>--dataset</code>: <code>tinyshakespeare</code> or <code>wikitext</code></li> <li><code>--tokenizer</code>: <code>char</code> or <code>bpe</code></li> <li><code>--output-format</code>: <code>txt</code> or <code>bin</code></li> <li><code>--raw-dir</code>: cache directory for raw corpus files</li> <li><code>--output-dir</code>: artifact output directory</li> <li><code>--val-ratio</code>: train/validation split ratio</li> </ul>"},{"location":"data-pipeline/#output-layout","title":"Output Layout","text":""},{"location":"data-pipeline/#txt-format-output-dir-dataprocessed","title":"<code>txt</code> format (<code>output-dir = data/processed</code>)","text":"<ul> <li><code>train.txt</code></li> <li><code>val.txt</code></li> <li><code>corpus.txt</code></li> <li><code>train.npy</code></li> <li><code>val.npy</code></li> <li><code>meta.json</code></li> </ul>"},{"location":"data-pipeline/#bin-format","title":"<code>bin</code> format","text":"<p>If output dir ends with <code>processed</code>, binaries are written to parent:</p> <ul> <li><code>data/train.bin</code></li> <li><code>data/val.bin</code></li> <li><code>data/meta.json</code></li> </ul> <p>Otherwise binaries are written directly in your <code>--output-dir</code>.</p>"},{"location":"data-pipeline/#metajson-fields","title":"<code>meta.json</code> Fields","text":"<ul> <li><code>dataset</code></li> <li><code>vocab_size</code></li> <li><code>tokenizer</code></li> <li><code>dtype</code></li> <li><code>output_format</code></li> <li>Optional file fields:</li> <li><code>train_text_file</code>, <code>val_text_file</code> for <code>txt</code></li> <li><code>train_bin_file</code>, <code>val_bin_file</code> for <code>bin</code></li> </ul>"},{"location":"data-pipeline/#tokenizer-notes","title":"Tokenizer Notes","text":"<ul> <li><code>char</code>: vocabulary is fitted on full corpus and stored in metadata.</li> <li><code>bpe</code>: uses GPT-2 encoding (<code>tiktoken</code>) and stores tokenizer type + encoding name.</li> </ul>"},{"location":"data-pipeline/#validation-tips","title":"Validation Tips","text":"<ul> <li>Confirm token counts printed by <code>prepare_data.py</code>.</li> <li>Ensure <code>meta.json</code> exists before launching <code>train.py</code>.</li> <li>For <code>bin</code> runs, keep <code>training.data_format = \"bin\"</code> in config.</li> </ul>"},{"location":"data-pipeline/#next-step","title":"Next Step","text":"<p>Continue with Training.</p>"},{"location":"developer-guide/","title":"Developer Guide","text":"<p>This page is for contributors working on the codebase itself.</p>"},{"location":"developer-guide/#repository-map","title":"Repository Map","text":"<pre><code>src/labcore_llm/\n  config/      # TOML loader and defaults\n  data/        # dataset abstractions\n  model/       # GPT model implementation\n  tokenizer/   # char + BPE tokenizers\n  trainer/     # training loop, scheduler, checkpointing\n\nscripts/       # data prep, export, quantize, fine-tune helpers\nconfigs/       # TOML presets\ntests/         # unit tests\n</code></pre>"},{"location":"developer-guide/#local-dev-environment","title":"Local Dev Environment","text":"<pre><code>python -m venv .venv\n## PowerShell\n.\\.venv\\Scripts\\Activate.ps1\npython -m pip install --upgrade pip\npython -m pip install -e \".[torch,dev]\"\n</code></pre>"},{"location":"developer-guide/#validation-commands","title":"Validation Commands","text":"<p>Run tests:</p> <pre><code>python -m pytest -q\n</code></pre> <p>Run lint rules aligned with CI:</p> <pre><code>ruff check src scripts tests train.py generate.py demo_gradio.py --select E9,F63,F7,F82\n</code></pre>"},{"location":"developer-guide/#ci-workflows","title":"CI Workflows","text":"<ul> <li><code>.github/workflows/ci.yml</code>: lint + tests</li> <li><code>.github/workflows/docs.yml</code>: MkDocs build and deploy</li> </ul>"},{"location":"developer-guide/#contribution-quality-bar","title":"Contribution Quality Bar","text":"<ul> <li>Keep commits focused and atomic.</li> <li>Update docs for behavior/CLI changes.</li> <li>Add tests for bug fixes and new logic.</li> <li>Do not commit large data/model artifacts.</li> </ul>"},{"location":"developer-guide/#packaging-notes","title":"Packaging Notes","text":"<ul> <li>Project uses <code>src/</code> layout with setuptools.</li> <li>Optional dependency groups are defined in <code>pyproject.toml</code>.</li> <li>Entry scripts are plain Python files, not console-script wrappers.</li> </ul>"},{"location":"export-and-deployment/","title":"Export and Deployment","text":"<p>This section covers export to HF-compatible artifacts and optional GGUF quantization.</p>"},{"location":"export-and-deployment/#export-checkpoint-to-hf-layout","title":"Export Checkpoint to HF Layout","text":"<pre><code>python scripts/export_hf.py \\\n  --checkpoint checkpoints/ckpt_last.pt \\\n  --meta data/processed/meta.json \\\n  --output-dir outputs/hf_export\n</code></pre> <p>Generated files:</p> <ul> <li><code>model.safetensors</code></li> <li><code>config.json</code></li> <li><code>tokenizer.json</code></li> <li><code>README.md</code></li> </ul>"},{"location":"export-and-deployment/#push-to-hugging-face-hub","title":"Push to Hugging Face Hub","text":"<pre><code>python scripts/export_hf.py \\\n  --checkpoint checkpoints/ckpt_last.pt \\\n  --meta data/processed/meta.json \\\n  --output-dir outputs/hf_export \\\n  --push \\\n  --repo-id GhostPunishR/labcore-llm-50M\n</code></pre> <p>Requirements:</p> <ul> <li><code>huggingface_hub</code></li> <li>valid HF authentication (<code>huggingface-cli login</code> or token in env)</li> </ul>"},{"location":"export-and-deployment/#gguf-conversion-and-quantization","title":"GGUF Conversion and Quantization","text":"<p>Prepare llama.cpp first, then run:</p> <pre><code>python scripts/quantize_gguf.py \\\n  --hf-dir outputs/hf_export \\\n  --llama-cpp-dir third_party/llama.cpp \\\n  --output-dir outputs/gguf \\\n  --quant-type Q4_K_M\n</code></pre> <p>Supported quant choices:</p> <ul> <li><code>Q4_K_M</code></li> <li><code>Q5_K_M</code></li> <li><code>all</code></li> </ul>"},{"location":"export-and-deployment/#deployment-checklist","title":"Deployment Checklist","text":"<ol> <li>Verify <code>generate.py</code> works on exported artifacts.</li> <li>Keep <code>config.json</code> and tokenizer metadata in sync with checkpoint.</li> <li>Version artifacts by model family and date.</li> <li>Publish model card with known limits and intended usage.</li> </ol>"},{"location":"export-and-deployment/#next-step","title":"Next Step","text":"<p>Continue with Fine-Tuning.</p>"},{"location":"fine-tuning/","title":"Fine-Tuning","text":"<p>LabCore includes a LoRA instruction fine-tuning script for HF-compatible CausalLM models.</p>"},{"location":"fine-tuning/#main-command","title":"Main Command","text":"<pre><code>python scripts/fine_tune_instruction.py \\\n  --model-id GhostPunishR/labcore-llm-50M \\\n  --dataset yahma/alpaca-cleaned \\\n  --dataset-split train \\\n  --output-dir outputs/lora_instruction \\\n  --config configs/bpe_rope_flash/bpe_50M_rope_flash.toml \\\n  --max-samples 20000 \\\n  --epochs 1\n</code></pre>"},{"location":"fine-tuning/#required-dependencies","title":"Required Dependencies","text":"<pre><code>pip install -e \".[torch,hf,finetune]\"\n</code></pre> <p>Or for development + fine-tuning:</p> <pre><code>pip install -e \".[torch,dev,hf,finetune]\"\n</code></pre>"},{"location":"fine-tuning/#script-behavior","title":"Script Behavior","text":"<ul> <li>Loads base model and tokenizer from <code>--model-id</code>.</li> <li>Auto-picks LoRA target modules (<code>q_proj</code>, <code>k_proj</code>, <code>v_proj</code>, <code>o_proj</code>) when available.</li> <li>Normalizes examples to a stable instruction template.</li> <li>Tokenizes with fixed max sequence length.</li> <li>Trains with HF <code>Trainer</code>.</li> <li>Saves LoRA adapter and tokenizer to <code>--output-dir</code>.</li> </ul>"},{"location":"fine-tuning/#dataset-field-mapping","title":"Dataset Field Mapping","text":"<p>The script supports common naming variants:</p> <ul> <li>instruction: <code>instruction</code> / <code>question</code> / <code>prompt</code></li> <li>input: <code>input</code> / <code>context</code></li> <li>output: <code>output</code> / <code>response</code> / <code>answer</code></li> </ul>"},{"location":"fine-tuning/#practical-tips","title":"Practical Tips","text":"<ul> <li>Start with small <code>--max-samples</code> for validation.</li> <li>Tune <code>--batch-size</code> and <code>--max-seq-len</code> based on VRAM.</li> <li>Keep dataset quality high; noisy instruction pairs degrade output fast.</li> </ul>"},{"location":"fine-tuning/#next-step","title":"Next Step","text":"<p>Review Configuration Reference for full tuning controls.</p>"},{"location":"getting-started/","title":"Getting Started","text":"<p>This page covers environment setup and the shortest path to a successful first run.</p>"},{"location":"getting-started/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python <code>3.11+</code></li> <li><code>pip</code></li> <li>Optional CUDA GPU for practical training speed</li> </ul> <p>Install PyTorch first from the official selector:</p> <p>https://pytorch.org/get-started/locally/</p>"},{"location":"getting-started/#installation-profiles","title":"Installation Profiles","text":"<pre><code>pip install -e .\n</code></pre> <p>Recommended for local training and development:</p> <pre><code>pip install -e \".[torch,dev]\"\n</code></pre> <p>Optional extras:</p> <pre><code>pip install -e \".[torch,hf,demo]\"\npip install -e \".[gguf]\"\npip install -e \".[torch,hf,finetune]\"\n</code></pre> <p>Install all extras:</p> <pre><code>pip install -e \".[all]\"\n</code></pre>"},{"location":"getting-started/#quick-smoke-test-char-pipeline","title":"Quick Smoke Test (Char Pipeline)","text":"<pre><code>python scripts/prepare_data.py --dataset tinyshakespeare --tokenizer char --output-format txt\npython train.py --config configs/base.toml --tokenizer char --max-iters 200\npython generate.py --checkpoint checkpoints/ckpt_last.pt --meta data/processed/meta.json --tokenizer char --prompt \"To be\"\n</code></pre> <p>Expected artifacts:</p> <ul> <li><code>data/processed/meta.json</code></li> <li><code>checkpoints/ckpt_last.pt</code></li> <li><code>checkpoints/train_log.json</code></li> </ul>"},{"location":"getting-started/#gpucpu-device-rules","title":"GPU/CPU Device Rules","text":"<ul> <li><code>train.py</code> accepts <code>--device</code> and falls back to CPU if CUDA is unavailable.</li> <li><code>generate.py</code> and <code>demo_gradio.py</code> have the same fallback behavior.</li> <li>Start with <code>configs/base.toml</code> on CPU.</li> </ul>"},{"location":"getting-started/#common-setup-issues","title":"Common Setup Issues","text":""},{"location":"getting-started/#modulenotfounderror-no-module-named-torch","title":"<code>ModuleNotFoundError: No module named 'torch'</code>","text":"<p>Use:</p> <pre><code>pip install -e \".[torch,dev]\"\n</code></pre>"},{"location":"getting-started/#dataset-download-errors","title":"Dataset download errors","text":"<p><code>scripts/prepare_data.py</code> uses <code>datasets</code> for HF dataset loading. For <code>wikitext</code>, install HF dependencies:</p> <pre><code>pip install -e \".[hf]\"\n</code></pre> <p><code>tinyshakespeare</code> has an HTTP fallback path if <code>datasets</code> is unavailable.</p>"},{"location":"getting-started/#next-step","title":"Next Step","text":"<p>Continue with Data Pipeline.</p>"},{"location":"inference-and-demo/","title":"Inference and Demo","text":"<p>LabCore supports two interactive inference paths:</p> <ul> <li>CLI generation with <code>generate.py</code></li> <li>Gradio web UI with <code>demo_gradio.py</code></li> </ul>"},{"location":"inference-and-demo/#cli-generation","title":"CLI Generation","text":"<pre><code>python generate.py \\\n  --checkpoint checkpoints/ckpt_last.pt \\\n  --meta data/processed/meta.json \\\n  --tokenizer char \\\n  --prompt \"To be\" \\\n  --max-new-tokens 200 \\\n  --temperature 0.9 \\\n  --top-k 40 \\\n  --device cpu\n</code></pre> <p>Key flags:</p> <ul> <li><code>--checkpoint</code></li> <li><code>--meta</code></li> <li><code>--config</code> (optional tokenizer fallback)</li> <li><code>--tokenizer</code> (<code>char</code> or <code>bpe</code>)</li> <li><code>--prompt</code></li> <li><code>--max-new-tokens</code></li> <li><code>--temperature</code></li> <li><code>--top-k</code></li> <li><code>--device</code></li> </ul>"},{"location":"inference-and-demo/#gradio-demo-local-checkpoint","title":"Gradio Demo (Local Checkpoint)","text":"<pre><code>python demo_gradio.py \\\n  --source local \\\n  --checkpoint checkpoints/ckpt_last.pt \\\n  --meta data/processed/meta.json \\\n  --device cpu \\\n  --port 7860\n</code></pre>"},{"location":"inference-and-demo/#gradio-demo-hf-hub","title":"Gradio Demo (HF Hub)","text":"<pre><code>python demo_gradio.py \\\n  --source hf \\\n  --repo-id GhostPunishR/labcore-llm-50M \\\n  --device cpu \\\n  --port 7860\n</code></pre>"},{"location":"inference-and-demo/#sampling-behavior","title":"Sampling Behavior","text":"<p><code>demo_gradio.py</code> supports:</p> <ul> <li>temperature scaling</li> <li>top-k filtering</li> <li>top-p filtering</li> </ul> <p><code>generate.py</code> supports:</p> <ul> <li>temperature</li> <li>top-k</li> </ul>"},{"location":"inference-and-demo/#troubleshooting","title":"Troubleshooting","text":""},{"location":"inference-and-demo/#missing-tokenizer-metadata-for-char-runs","title":"Missing tokenizer metadata for char runs","text":"<p><code>generate.py</code> requires char vocab in <code>meta.json</code>. Regenerate data with char tokenizer:</p> <pre><code>python scripts/prepare_data.py --dataset tinyshakespeare --tokenizer char --output-format txt\n</code></pre>"},{"location":"inference-and-demo/#hf-model-loading-dependencies","title":"HF model loading dependencies","text":"<p>For remote demo, install:</p> <pre><code>pip install -e \".[hf,demo]\"\n</code></pre>"},{"location":"inference-and-demo/#next-step","title":"Next Step","text":"<p>Continue with Export and Deployment.</p>"},{"location":"operations/","title":"Operations","text":"<p>This page centralizes release flow, security reporting, and runtime troubleshooting.</p>"},{"location":"operations/#security","title":"Security","text":"<p>Do not report vulnerabilities in public issues. Follow the process in <code>SECURITY.md</code>:</p> <ul> <li>Prefer GitHub Security Advisories</li> <li>Share impact, affected components, and reproducible steps</li> </ul>"},{"location":"operations/#release-process","title":"Release Process","text":"<p>Reference: <code>RELEASE.md</code></p> <ol> <li>Ensure CI is green.</li> <li>Run local tests.</li> <li>Bump version in <code>pyproject.toml</code>.</li> <li>Update <code>CHANGELOG.md</code>.</li> <li>Tag release and push tag.</li> <li>Publish GitHub Release.</li> </ol>"},{"location":"operations/#support-channels","title":"Support Channels","text":"<p>Reference: <code>SUPPORT.md</code></p> <ul> <li>Usage questions: Discussion/Issue with <code>support</code> label</li> <li>Bugs: issue template</li> <li>Features: feature request template</li> </ul>"},{"location":"operations/#quality-and-validation","title":"Quality and Validation","text":"<p>Primary local checks:</p> <pre><code>python -m pytest -q\nruff check src scripts tests train.py generate.py demo_gradio.py --select E9,F63,F7,F82\n</code></pre> <p>CI workflows:</p> <ul> <li><code>.github/workflows/ci.yml</code> (lint + tests)</li> <li><code>.github/workflows/docs.yml</code> (docs build/deploy)</li> </ul>"},{"location":"operations/#hardware-guidance","title":"Hardware Guidance","text":"<ul> <li>CPU-only is fine for smoke tests and very short runs.</li> <li>CUDA GPU is recommended for practical training speed.</li> <li>For ~8 GB VRAM setups, start with the 50M preset families and tune batch/sequence settings as needed.</li> </ul>"},{"location":"operations/#troubleshooting-matrix","title":"Troubleshooting Matrix","text":""},{"location":"operations/#modulenotfounderror-torch","title":"<code>ModuleNotFoundError: torch</code>","text":"<p>Install:</p> <pre><code>pip install -e \".[torch,dev]\"\n</code></pre>"},{"location":"operations/#char-tokenizer-requires-vocab-in-metajson","title":"<code>Char tokenizer requires vocab in meta.json</code>","text":"<p>Regenerate data with char tokenizer and use matching metadata path.</p>"},{"location":"operations/#binary-shards-not-found","title":"<code>Binary shards not found</code>","text":"<p>Prepare binary dataset:</p> <pre><code>python scripts/prepare_data.py --dataset wikitext --tokenizer bpe --output-format bin --output-dir data\n</code></pre>"},{"location":"operations/#cuda-fallback-warning","title":"CUDA fallback warning","text":"<p>If CUDA is unavailable, scripts fall back to CPU automatically.</p>"},{"location":"operations/#docs-operations","title":"Docs Operations","text":"<p>Build docs locally:</p> <pre><code>pip install -r docs/requirements.txt\npython -m mkdocs serve\n</code></pre> <p>Deploy docs via GitHub Actions:</p> <ul> <li>Push to <code>master</code> or <code>main</code></li> <li><code>docs.yml</code> builds and deploys <code>site/</code> to <code>gh-pages</code></li> </ul>"},{"location":"operations/#license","title":"License","text":"<p>This project is licensed under GPL-3.0. See LICENSE.</p>"},{"location":"operations/#disclaimer","title":"Disclaimer","text":"<p>This project is intended for educational and research purposes. It is not optimized for large-scale production deployment.</p>"},{"location":"training/","title":"Training","text":"<p>Training is driven by TOML configs plus optional CLI overrides.</p>"},{"location":"training/#main-command","title":"Main Command","text":"<pre><code>python train.py --config configs/base.toml --tokenizer char --max-iters 2000\n</code></pre> <p>CLI flags:</p> <ul> <li><code>--config</code>: path to TOML preset</li> <li><code>--max-iters</code>: override training max iterations</li> <li><code>--device</code>: <code>cpu</code> or <code>cuda</code></li> <li><code>--tokenizer</code>: <code>char</code> or <code>bpe</code></li> </ul>"},{"location":"training/#presets","title":"Presets","text":"<ul> <li><code>configs/base.toml</code>: small char baseline (<code>txt</code> data format)</li> <li><code>configs/bpe_medium/bpe_medium_50M.toml</code>: BPE preset with 50M-class model (<code>txt</code>)</li> <li><code>configs/bpe_rope_flash/bpe_50M_rope_flash.toml</code>: BPE + RoPE + Flash with <code>bin</code> format</li> <li>Additional size variants are available in <code>configs/bpe_medium/</code> and <code>configs/bpe_rope_flash/</code> (<code>5M</code> to <code>50M</code>).</li> </ul>"},{"location":"training/#runtime-behavior","title":"Runtime Behavior","text":"<ul> <li>Loads config via <code>labcore_llm.config.load_config</code>.</li> <li>Resolves <code>meta.json</code> from configured processed directory.</li> <li>Loads tokenizer from metadata.</li> <li>Builds dataset loaders based on <code>training.data_format</code>:</li> <li><code>txt</code> or <code>npy</code> path</li> <li><code>bin</code> memory-mapped path</li> <li>Applies warmup + cosine LR schedule.</li> <li>Saves checkpoints and train logs.</li> </ul>"},{"location":"training/#checkpoints-and-logs","title":"Checkpoints and Logs","text":"<p>Trainer saves:</p> <ul> <li><code>checkpoints/ckpt_last.pt</code></li> <li><code>checkpoints/train_log.json</code></li> </ul> <p><code>ckpt_last.pt</code> includes:</p> <ul> <li>model config and model weights</li> <li>optimizer state</li> <li>trainer config</li> <li>latest loss snapshot</li> <li>tokenizer metadata</li> </ul>"},{"location":"training/#performance-guidance","title":"Performance Guidance","text":"<ul> <li>For CPU smoke tests, use <code>configs/base.toml</code> and low <code>--max-iters</code>.</li> <li>For 8GB VRAM setups, start with provided 50M presets.</li> <li>Keep <code>gradient_accumulation_steps</code> aligned with memory budget.</li> </ul>"},{"location":"training/#common-errors","title":"Common Errors","text":""},{"location":"training/#binary-shards-not-found","title":"<code>Binary shards not found</code>","text":"<p>You selected <code>training.data_format = \"bin\"</code> without <code>train.bin</code> and <code>val.bin</code>. Re-run <code>prepare_data.py</code> with <code>--output-format bin</code>.</p>"},{"location":"training/#unable-to-infer-vocab_size","title":"<code>Unable to infer vocab_size</code>","text":"<p>Ensure one source provides vocab size:</p> <ul> <li><code>model.vocab_size</code> in TOML, or</li> <li><code>meta.json</code> contains <code>vocab_size</code>, or</li> <li>tokenizer metadata is available.</li> </ul>"},{"location":"training/#next-step","title":"Next Step","text":"<p>Continue with Inference and Demo.</p>"},{"location":"troubleshooting/","title":"Troubleshooting","text":"<p>Common setup and runtime issues with quick fixes.</p>"},{"location":"troubleshooting/#installing-pytorch-cpucuda","title":"Installing PyTorch (CPU/CUDA)","text":"<p>Use the official selector for the right command on your OS and CUDA version:</p> <p>https://pytorch.org/get-started/locally/</p> <p>Typical pattern:</p> <pre><code>pip install -e \".[torch]\"\n</code></pre>"},{"location":"troubleshooting/#cuda-not-detected-torchcudais_available-false","title":"CUDA Not Detected (<code>torch.cuda.is_available() == False</code>)","text":"<p>Check:</p> <ul> <li>Your installed PyTorch build matches your CUDA runtime/driver.</li> <li>NVIDIA driver is installed and up to date.</li> <li>You are running the same Python/venv where PyTorch was installed.</li> </ul> <p>Quick check:</p> <pre><code>python -c \"import torch; print(torch.__version__, torch.cuda.is_available())\"\n</code></pre>"},{"location":"troubleshooting/#oom-what-to-lower-first","title":"OOM: What to Lower First","text":"<p>When you hit out-of-memory errors, reduce in this order:</p> <ol> <li><code>training.batch_size</code></li> <li><code>model.block_size</code> (sequence length)</li> <li><code>training.gradient_accumulation_steps</code></li> <li>Precision strategy (if you introduce mixed precision in your environment)</li> </ol> <p>If needed, move to a smaller preset model size.</p>"},{"location":"troubleshooting/#metajson-location-confusion","title":"<code>meta.json</code> Location Confusion","text":"<p><code>meta.json</code> location depends on output format and output directory:</p> <ul> <li><code>txt</code> pipeline: usually <code>data/processed/meta.json</code></li> <li><code>bin</code> pipeline: usually <code>data/meta.json</code></li> </ul> <p>If <code>train.py</code> cannot infer metadata, verify <code>processed_dir</code> in your TOML config and the actual output path from <code>prepare_data.py</code>.</p>"},{"location":"troubleshooting/#flashattention-not-available","title":"FlashAttention Not Available","text":"<p>LabCore falls back automatically:</p> <ul> <li>Preferred: FlashAttention kernel (when available)</li> <li>Fallback: PyTorch SDPA</li> <li>Last path: standard causal attention</li> </ul> <p>You can keep <code>use_flash = true</code>; runtime fallback handles unsupported environments.</p>"},{"location":"troubleshooting/#windows-notes","title":"Windows Notes","text":"<ul> <li>Use an activated venv before all commands.</li> <li>Prefer quoted paths when directories include spaces.</li> <li>Run commands from the repository root to keep relative paths stable.</li> <li>If execution policy blocks scripts, use direct <code>python ...</code> commands.</li> </ul>"},{"location":"fr/","title":"LabCore LLM","text":"<p>Framework GPT decoder-only pragmatique pour l'entra\u00eenement, l'inf\u00e9rence et le d\u00e9ploiement en local.</p> <p> </p>"},{"location":"fr/#installation-rapide","title":"Installation rapide","text":"<pre><code>pip install -e \".[torch]\"\n</code></pre>"},{"location":"fr/#exemple-rapide","title":"Exemple rapide","text":"<p>Tiny Shakespeare avec tokenizer char:</p> <pre><code>python scripts/prepare_data.py --dataset tinyshakespeare --tokenizer char --output-format txt\npython train.py --config configs/base.toml --tokenizer char --max-iters 200\npython generate.py --checkpoint checkpoints/ckpt_last.pt --meta data/processed/meta.json --tokenizer char --prompt \"To be\"\n</code></pre>"},{"location":"fr/#ce-que-vous-obtenez","title":"Ce que vous obtenez","text":"<ul> <li>Stack GPT decoder-only pilot\u00e9e par configuration TOML.</li> <li>RoPE optionnel.</li> <li>Chemin d'attention FlashAttention/SDPA optionnel.</li> <li>Pipeline binaire memory-mapped (<code>train.bin</code>/<code>val.bin</code>).</li> <li>Export Hugging Face.</li> <li>Conversion GGUF.</li> <li>Point d'entr\u00e9e LoRA pour l'instruction tuning.</li> </ul>"},{"location":"fr/#flux-de-bout-en-bout","title":"Flux de bout en bout","text":"<pre><code>Corpus texte brut\n  -&gt; scripts/prepare_data.py\n  -&gt; train.py (+ preset config)\n  -&gt; checkpoints/ckpt_last.pt\n  -&gt; generate.py / demo_gradio.py\n  -&gt; scripts/export_hf.py\n  -&gt; scripts/quantize_gguf.py (optionnel)\n  -&gt; scripts/fine_tune_instruction.py (optionnel)\n</code></pre>"},{"location":"fr/#plan-de-la-documentation","title":"Plan de la documentation","text":""},{"location":"fr/#guides","title":"Guides","text":"<ul> <li>Getting Started: configuration de l'environnement et premier run.</li> <li>Data Pipeline: g\u00e9n\u00e9ration de datasets <code>txt</code> ou <code>bin</code> + metadata.</li> <li>Training: entra\u00eenement avec presets et suivi des checkpoints.</li> <li>Inference &amp; Demo: g\u00e9n\u00e9ration CLI et interface Gradio.</li> <li>Fine-Tuning: workflow LoRA pour l'instruction tuning.</li> <li>Export &amp; Deployment: export Hugging Face et chemin GGUF.</li> </ul>"},{"location":"fr/#reference","title":"R\u00e9f\u00e9rence","text":"<ul> <li>Configuration: sections TOML, defaults et exemples.</li> <li>Operations: proc\u00e9dures release/s\u00e9curit\u00e9/support.</li> <li>Troubleshooting: erreurs fr\u00e9quentes et correctifs rapides.</li> <li>Benchmarks: mod\u00e8le de suivi des performances.</li> </ul>"},{"location":"fr/#developpement","title":"D\u00e9veloppement","text":"<ul> <li>Developer Guide: workflow dev local, tests et standards.</li> </ul>"},{"location":"fr/#prochaines-etapes-rapides","title":"Prochaines \u00e9tapes rapides","text":"<ul> <li>Getting Started</li> <li>Training</li> <li>Export &amp; Deployment</li> <li>Operations pour checks qualit\u00e9, guidance hardware, license et disclaimer.</li> </ul>"},{"location":"fr/benchmarks/","title":"Benchmarks","text":"<p>Utilisez cette page pour suivre des mesures de performance reproductibles selon preset et mat\u00e9riel.</p>"},{"location":"fr/benchmarks/#modele-de-tableau","title":"Mod\u00e8le de Tableau","text":"Preset Device Batch / Seq / Accum D\u00e9bit (it/s ou tok/s) VRAM Max Notes <code>configs/base.toml</code> <code>cpu</code> <code>32 / 128 / 1</code> <code>0.00</code> <code>0.0 GB</code> placeholder"},{"location":"fr/benchmarks/#exemple-minimal","title":"Exemple Minimal","text":"<p>Une ligne par run avec param\u00e8tres exacts :</p> Preset Device Batch / Seq / Accum D\u00e9bit (it/s ou tok/s) VRAM Max Notes <code>configs/bpe_rope_flash/bpe_30M_rope_flash.toml</code> <code>cuda</code> <code>8 / 512 / 8</code> <code>TBD</code> <code>TBD</code> compl\u00e9ter apr\u00e8s run stable"},{"location":"fr/benchmarks/#conseils","title":"Conseils","text":"<ul> <li>Faire quelques it\u00e9rations de warmup avant mesure.</li> <li>Garder dataset, tokenizer et pr\u00e9cision constants entre runs.</li> <li>Noter GPU, driver et version PyTorch dans les notes.</li> </ul>"},{"location":"fr/configuration-reference/","title":"Reference Config","text":"<p><code>train.py</code> lit les fichiers TOML puis applique des valeurs par defaut.</p>"},{"location":"fr/configuration-reference/#sections-supportees","title":"Sections supportees","text":""},{"location":"fr/configuration-reference/#general","title":"<code>[general]</code>","text":"<ul> <li><code>run_name</code></li> <li><code>seed</code></li> <li><code>tokenizer</code> (<code>char</code> ou <code>bpe</code>)</li> </ul>"},{"location":"fr/configuration-reference/#data","title":"<code>[data]</code>","text":"<ul> <li><code>dataset</code></li> <li><code>processed_dir</code></li> </ul>"},{"location":"fr/configuration-reference/#training","title":"<code>[training]</code>","text":"<ul> <li><code>batch_size</code></li> <li><code>gradient_accumulation_steps</code></li> <li><code>max_iters</code></li> <li><code>warmup_iters</code></li> <li><code>lr_decay_iters</code></li> <li><code>min_lr</code></li> <li><code>eval_interval</code></li> <li><code>eval_iters</code></li> <li><code>log_interval</code></li> <li><code>save_interval</code></li> <li><code>device</code></li> <li><code>checkpoint_dir</code></li> <li><code>data_format</code> (<code>txt</code> ou <code>bin</code>)</li> </ul>"},{"location":"fr/configuration-reference/#generation","title":"<code>[generation]</code>","text":"<ul> <li><code>max_new_tokens</code></li> <li><code>temperature</code></li> <li><code>top_k</code></li> </ul>"},{"location":"fr/configuration-reference/#defauts-automatiques","title":"Defauts automatiques","text":"<p>Le loader applique:</p> <ul> <li><code>general.tokenizer = \"char\"</code></li> <li><code>data.processed_dir = \"data/processed\"</code></li> <li><code>model.use_rope = false</code></li> <li><code>model.use_flash = false</code></li> <li><code>training.data_format = \"txt\"</code></li> </ul>"},{"location":"fr/configuration-reference/#exemple-minimal","title":"Exemple minimal","text":"<pre><code>[data]\nprocessed_dir = \"data/processed\"\n\n[model]\nblock_size = 128\nn_layer = 6\nn_head = 8\nn_embd = 256\nvocab_size = 65\n\n[training]\nbatch_size = 32\nmax_iters = 1000\ndevice = \"cpu\"\n</code></pre>"},{"location":"fr/data-pipeline/","title":"Pipeline Data","text":"<p>LabCore gere deux formats de sortie:</p> <ul> <li><code>txt</code> pour flux texte + numpy</li> <li><code>bin</code> pour flux binaire memmap</li> </ul>"},{"location":"fr/data-pipeline/#commande-principale","title":"Commande Principale","text":"<pre><code>python scripts/prepare_data.py \\\n  --dataset tinyshakespeare \\\n  --tokenizer char \\\n  --output-format txt \\\n  --raw-dir data/raw \\\n  --output-dir data/processed \\\n  --val-ratio 0.1\n</code></pre>"},{"location":"fr/data-pipeline/#options-cli","title":"Options CLI","text":"<ul> <li><code>--dataset</code>: <code>tinyshakespeare</code> ou <code>wikitext</code></li> <li><code>--tokenizer</code>: <code>char</code> ou <code>bpe</code></li> <li><code>--output-format</code>: <code>txt</code> ou <code>bin</code></li> <li><code>--raw-dir</code>: cache corpus brut</li> <li><code>--output-dir</code>: dossier de sortie</li> <li><code>--val-ratio</code>: ratio validation</li> </ul>"},{"location":"fr/data-pipeline/#arborescence-de-sortie","title":"Arborescence de sortie","text":""},{"location":"fr/data-pipeline/#format-txt","title":"Format <code>txt</code>","text":"<ul> <li><code>train.txt</code></li> <li><code>val.txt</code></li> <li><code>corpus.txt</code></li> <li><code>train.npy</code></li> <li><code>val.npy</code></li> <li><code>meta.json</code></li> </ul>"},{"location":"fr/data-pipeline/#format-bin","title":"Format <code>bin</code>","text":"<p>Si <code>output-dir</code> finit par <code>processed</code>, les binaires partent dans le parent:</p> <ul> <li><code>data/train.bin</code></li> <li><code>data/val.bin</code></li> <li><code>data/meta.json</code></li> </ul>"},{"location":"fr/data-pipeline/#champs-metajson","title":"Champs <code>meta.json</code>","text":"<ul> <li><code>dataset</code></li> <li><code>vocab_size</code></li> <li><code>tokenizer</code></li> <li><code>dtype</code></li> <li><code>output_format</code></li> <li>champs de fichiers texte ou bin selon format</li> </ul>"},{"location":"fr/data-pipeline/#notes-tokenizer","title":"Notes tokenizer","text":"<ul> <li><code>char</code>: vocab appris sur tout le corpus.</li> <li><code>bpe</code>: encodeur GPT-2 via <code>tiktoken</code>.</li> </ul>"},{"location":"fr/data-pipeline/#bonnes-pratiques","title":"Bonnes pratiques","text":"<ul> <li>verifier les compteurs de tokens affiches</li> <li>verifier presence de <code>meta.json</code></li> <li>aligner <code>training.data_format</code> avec les artifacts generes</li> </ul>"},{"location":"fr/data-pipeline/#suite","title":"Suite","text":"<p>Voir Entrainement.</p>"},{"location":"fr/developer-guide/","title":"Guide Dev","text":"<p>Page dediee aux contributeurs code.</p>"},{"location":"fr/developer-guide/#structure-repo","title":"Structure repo","text":"<pre><code>src/labcore_llm/\n  config/\n  data/\n  model/\n  tokenizer/\n  trainer/\n\nscripts/\nconfigs/\ntests/\n</code></pre>"},{"location":"fr/developer-guide/#environment-local","title":"Environment local","text":"<pre><code>python -m venv .venv\n.\\.venv\\Scripts\\Activate.ps1\npython -m pip install --upgrade pip\npython -m pip install -e \".[torch,dev]\"\n</code></pre>"},{"location":"fr/developer-guide/#validation-locale","title":"Validation locale","text":"<p>Tests:</p> <pre><code>python -m pytest -q\n</code></pre> <p>Lint:</p> <pre><code>ruff check src scripts tests train.py generate.py demo_gradio.py --select E9,F63,F7,F82\n</code></pre>"},{"location":"fr/developer-guide/#ci","title":"CI","text":"<ul> <li><code>.github/workflows/ci.yml</code></li> <li><code>.github/workflows/docs.yml</code></li> </ul>"},{"location":"fr/developer-guide/#regles-contribution","title":"Regles contribution","text":"<ul> <li>commits atomiques</li> <li>docs mises a jour en meme temps que le code</li> <li>tests ajoutes quand logique modifiee</li> <li>pas d'artifacts lourds dans git</li> </ul>"},{"location":"fr/export-and-deployment/","title":"Export et Deploiement","text":"<p>Cette section couvre l'export HF puis la quantization GGUF.</p>"},{"location":"fr/export-and-deployment/#export-hf","title":"Export HF","text":"<pre><code>python scripts/export_hf.py \\\n  --checkpoint checkpoints/ckpt_last.pt \\\n  --meta data/processed/meta.json \\\n  --output-dir outputs/hf_export\n</code></pre> <p>Fichiers generes:</p> <ul> <li><code>model.safetensors</code></li> <li><code>config.json</code></li> <li><code>tokenizer.json</code></li> <li><code>README.md</code></li> </ul>"},{"location":"fr/export-and-deployment/#push-vers-hugging-face-hub","title":"Push vers Hugging Face Hub","text":"<pre><code>python scripts/export_hf.py \\\n  --checkpoint checkpoints/ckpt_last.pt \\\n  --meta data/processed/meta.json \\\n  --output-dir outputs/hf_export \\\n  --push \\\n  --repo-id GhostPunishR/labcore-llm-50M\n</code></pre>"},{"location":"fr/export-and-deployment/#conversion-gguf","title":"Conversion GGUF","text":"<pre><code>python scripts/quantize_gguf.py \\\n  --hf-dir outputs/hf_export \\\n  --llama-cpp-dir third_party/llama.cpp \\\n  --output-dir outputs/gguf \\\n  --quant-type Q4_K_M\n</code></pre> <p>Valeurs <code>--quant-type</code>:</p> <ul> <li><code>Q4_K_M</code></li> <li><code>Q5_K_M</code></li> <li><code>all</code></li> </ul>"},{"location":"fr/export-and-deployment/#checklist-de-publication","title":"Checklist de publication","text":"<ol> <li>valider generation locale</li> <li>verifier coherence <code>config.json</code> + tokenizer</li> <li>versionner artifacts</li> <li>publier model card claire</li> </ol>"},{"location":"fr/export-and-deployment/#suite","title":"Suite","text":"<p>Voir Fine-Tuning.</p>"},{"location":"fr/fine-tuning/","title":"Fine-Tuning","text":"<p>Le projet inclut un script LoRA pour fine-tuning instruction de modeles CausalLM compatibles HF.</p>"},{"location":"fr/fine-tuning/#commande-principale","title":"Commande principale","text":"<pre><code>python scripts/fine_tune_instruction.py \\\n  --model-id GhostPunishR/labcore-llm-50M \\\n  --dataset yahma/alpaca-cleaned \\\n  --dataset-split train \\\n  --output-dir outputs/lora_instruction \\\n  --config configs/bpe_rope_flash/bpe_50M_rope_flash.toml \\\n  --max-samples 20000 \\\n  --epochs 1\n</code></pre>"},{"location":"fr/fine-tuning/#dependances","title":"Dependances","text":"<pre><code>pip install -e \".[torch,hf,finetune]\"\n</code></pre> <p>Ou:</p> <pre><code>pip install -e \".[torch,dev,hf,finetune]\"\n</code></pre>"},{"location":"fr/fine-tuning/#ce-que-fait-le-script","title":"Ce que fait le script","text":"<ul> <li>charge model/tokenizer base</li> <li>detecte modules cibles LoRA</li> <li>normalise les exemples instruction</li> <li>tokenize avec taille sequence fixe</li> <li>entraine via HF <code>Trainer</code></li> <li>sauvegarde adapter + tokenizer</li> </ul>"},{"location":"fr/fine-tuning/#mapping-des-champs-dataset","title":"Mapping des champs dataset","text":"<ul> <li>instruction: <code>instruction</code> / <code>question</code> / <code>prompt</code></li> <li>input: <code>input</code> / <code>context</code></li> <li>output: <code>output</code> / <code>response</code> / <code>answer</code></li> </ul>"},{"location":"fr/fine-tuning/#conseils","title":"Conseils","text":"<ul> <li>commencer avec peu de samples</li> <li>ajuster <code>batch-size</code> et <code>max-seq-len</code> selon VRAM</li> <li>privilegier un dataset propre et homogene</li> </ul>"},{"location":"fr/fine-tuning/#suite","title":"Suite","text":"<p>Voir Reference Config.</p>"},{"location":"fr/getting-started/","title":"Demarrage","text":"<p>Cette page couvre le setup de base et le premier run fonctionnel.</p>"},{"location":"fr/getting-started/#prerequis","title":"Prerequis","text":"<ul> <li>Python <code>3.11+</code></li> <li><code>pip</code></li> <li>GPU CUDA optionnel (recommande pour entrainement reel)</li> </ul> <p>Installer PyTorch via le selecteur officiel:</p> <p>https://pytorch.org/get-started/locally/</p>"},{"location":"fr/getting-started/#installation","title":"Installation","text":"<p>Minimal:</p> <pre><code>pip install -e .\n</code></pre> <p>Recommande pour dev + training:</p> <pre><code>pip install -e \".[torch,dev]\"\n</code></pre> <p>Extras:</p> <pre><code>pip install -e \".[torch,hf,demo]\"\npip install -e \".[gguf]\"\npip install -e \".[torch,hf,finetune]\"\n</code></pre>"},{"location":"fr/getting-started/#smoke-test-rapide","title":"Smoke Test Rapide","text":"<pre><code>python scripts/prepare_data.py --dataset tinyshakespeare --tokenizer char --output-format txt\npython train.py --config configs/base.toml --tokenizer char --max-iters 200\npython generate.py --checkpoint checkpoints/ckpt_last.pt --meta data/processed/meta.json --tokenizer char --prompt \"To be\"\n</code></pre> <p>Artifacts attendus:</p> <ul> <li><code>data/processed/meta.json</code></li> <li><code>checkpoints/ckpt_last.pt</code></li> <li><code>checkpoints/train_log.json</code></li> </ul>"},{"location":"fr/getting-started/#notes-device","title":"Notes Device","text":"<ul> <li><code>--device cuda</code> bascule auto sur CPU si CUDA indisponible.</li> <li>Pour CPU: commencer avec <code>configs/base.toml</code>.</li> </ul>"},{"location":"fr/getting-started/#erreurs-frequentes","title":"Erreurs frequentes","text":""},{"location":"fr/getting-started/#no-module-named-torch","title":"<code>No module named 'torch'</code>","text":"<pre><code>pip install -e \".[torch,dev]\"\n</code></pre>"},{"location":"fr/getting-started/#probleme-download-dataset","title":"Probleme download dataset","text":"<p><code>prepare_data.py</code> utilise <code>datasets</code> pour charger les jeux HF. Pour <code>wikitext</code>, installer:</p> <pre><code>pip install -e \".[hf]\"\n</code></pre> <p><code>tinyshakespeare</code> garde un fallback HTTP si <code>datasets</code> est indisponible.</p>"},{"location":"fr/getting-started/#suite","title":"Suite","text":"<p>Voir Pipeline Data.</p>"},{"location":"fr/inference-and-demo/","title":"Inference et Demo","text":"<p>Deux modes principaux:</p> <ul> <li>generation CLI (<code>generate.py</code>)</li> <li>interface web Gradio (<code>demo_gradio.py</code>)</li> </ul>"},{"location":"fr/inference-and-demo/#generation-cli","title":"Generation CLI","text":"<pre><code>python generate.py \\\n  --checkpoint checkpoints/ckpt_last.pt \\\n  --meta data/processed/meta.json \\\n  --tokenizer char \\\n  --prompt \"To be\" \\\n  --max-new-tokens 200 \\\n  --temperature 0.9 \\\n  --top-k 40 \\\n  --device cpu\n</code></pre>"},{"location":"fr/inference-and-demo/#demo-gradio-locale","title":"Demo Gradio locale","text":"<pre><code>python demo_gradio.py \\\n  --source local \\\n  --checkpoint checkpoints/ckpt_last.pt \\\n  --meta data/processed/meta.json \\\n  --device cpu \\\n  --port 7860\n</code></pre>"},{"location":"fr/inference-and-demo/#demo-gradio-depuis-hf","title":"Demo Gradio depuis HF","text":"<pre><code>python demo_gradio.py \\\n  --source hf \\\n  --repo-id GhostPunishR/labcore-llm-50M \\\n  --device cpu \\\n  --port 7860\n</code></pre>"},{"location":"fr/inference-and-demo/#sampling","title":"Sampling","text":"<p><code>demo_gradio.py</code> supporte:</p> <ul> <li>temperature</li> <li>top-k</li> <li>top-p</li> </ul> <p><code>generate.py</code> supporte:</p> <ul> <li>temperature</li> <li>top-k</li> </ul>"},{"location":"fr/inference-and-demo/#depannage","title":"Depannage","text":""},{"location":"fr/inference-and-demo/#tokenizer-char-sans-vocab","title":"tokenizer char sans vocab","text":"<p>Regenerer la data char + <code>meta.json</code>:</p> <pre><code>python scripts/prepare_data.py --dataset tinyshakespeare --tokenizer char --output-format txt\n</code></pre>"},{"location":"fr/inference-and-demo/#dependances-hf-demo-manquantes","title":"dependances HF demo manquantes","text":"<pre><code>pip install -e \".[hf,demo]\"\n</code></pre>"},{"location":"fr/inference-and-demo/#suite","title":"Suite","text":"<p>Voir Export et Deploiement.</p>"},{"location":"fr/operations/","title":"Operations","text":"<p>Page centralisant release, securite et depannage.</p>"},{"location":"fr/operations/#securite","title":"Securite","text":"<p>Ne pas publier les vuln dans les issues publiques. Suivre <code>SECURITY.md</code>:</p> <ul> <li>GitHub Security Advisories en priorite</li> <li>details impact + composants + reproduction</li> </ul>"},{"location":"fr/operations/#process-release","title":"Process release","text":"<p>Reference: <code>RELEASE.md</code></p> <ol> <li>CI verte</li> <li>tests locaux ok</li> <li>bump version dans <code>pyproject.toml</code></li> <li>update <code>CHANGELOG.md</code></li> <li>tag release</li> <li>publier GitHub Release</li> </ol>"},{"location":"fr/operations/#support","title":"Support","text":"<p>Reference: <code>SUPPORT.md</code></p> <ul> <li>usage: discussion/issue label <code>support</code></li> <li>bug: template bug</li> <li>feature: template feature request</li> </ul>"},{"location":"fr/operations/#qualite-et-validation","title":"Qualite et validation","text":"<p>Checks locaux principaux:</p> <pre><code>python -m pytest -q\nruff check src scripts tests train.py generate.py demo_gradio.py --select E9,F63,F7,F82\n</code></pre> <p>Workflows CI:</p> <ul> <li><code>.github/workflows/ci.yml</code> (lint + tests)</li> <li><code>.github/workflows/docs.yml</code> (build/deploy docs)</li> </ul>"},{"location":"fr/operations/#guidance-hardware","title":"Guidance hardware","text":"<ul> <li>CPU-only: suffisant pour smoke tests et runs courts.</li> <li>GPU CUDA recommande pour un entrainement pratique.</li> <li>Pour ~8 GB VRAM, commencer avec les familles 50M puis ajuster batch/sequence.</li> </ul>"},{"location":"fr/operations/#depannage-rapide","title":"Depannage rapide","text":""},{"location":"fr/operations/#no-module-named-torch","title":"<code>No module named 'torch'</code>","text":"<pre><code>pip install -e \".[torch,dev]\"\n</code></pre>"},{"location":"fr/operations/#char-tokenizer-requires-vocab-in-metajson","title":"<code>Char tokenizer requires vocab in meta.json</code>","text":"<p>Regenerer metadata avec tokenizer char.</p>"},{"location":"fr/operations/#binary-shards-not-found","title":"<code>Binary shards not found</code>","text":"<pre><code>python scripts/prepare_data.py --dataset wikitext --tokenizer bpe --output-format bin --output-dir data\n</code></pre>"},{"location":"fr/operations/#warning-cuda-fallback","title":"Warning CUDA fallback","text":"<p>Les scripts basculent auto CPU si CUDA indisponible.</p>"},{"location":"fr/operations/#operations-docs","title":"Operations docs","text":"<p>Build local:</p> <pre><code>pip install -r docs/requirements.txt\npython -m mkdocs serve\n</code></pre> <p>Deploiement:</p> <ul> <li>push sur <code>master</code>/<code>main</code></li> <li>workflow docs construit <code>site/</code> puis deploy sur <code>gh-pages</code></li> </ul>"},{"location":"fr/operations/#license","title":"License","text":"<p>Le projet est sous GPL-3.0. Voir LICENSE.</p>"},{"location":"fr/operations/#disclaimer","title":"Disclaimer","text":"<p>Projet destine a l'education et la recherche. Non optimise pour un deploiement production a grande echelle.</p>"},{"location":"fr/training/","title":"Entrainement","text":"<p>L'entrainement est pilote par config TOML avec overrides CLI.</p>"},{"location":"fr/training/#commande","title":"Commande","text":"<pre><code>python train.py --config configs/base.toml --tokenizer char --max-iters 2000\n</code></pre> <p>Flags:</p> <ul> <li><code>--config</code></li> <li><code>--max-iters</code></li> <li><code>--device</code></li> <li><code>--tokenizer</code></li> </ul>"},{"location":"fr/training/#presets-disponibles","title":"Presets disponibles","text":"<ul> <li><code>configs/base.toml</code></li> <li><code>configs/bpe_medium/bpe_medium_50M.toml</code></li> <li><code>configs/bpe_rope_flash/bpe_50M_rope_flash.toml</code></li> <li>Variantes de taille disponibles dans <code>configs/bpe_medium/</code> et <code>configs/bpe_rope_flash/</code> (<code>5M</code> a <code>50M</code>).</li> </ul>"},{"location":"fr/training/#comportement-runtime","title":"Comportement runtime","text":"<ul> <li>charge config et metadata</li> <li>charge tokenizer</li> <li>construit dataloaders selon <code>training.data_format</code></li> <li>applique warmup + cosine decay LR</li> <li>sauvegarde checkpoints + logs</li> </ul>"},{"location":"fr/training/#fichiers-produits","title":"Fichiers produits","text":"<ul> <li><code>checkpoints/ckpt_last.pt</code></li> <li><code>checkpoints/train_log.json</code></li> </ul> <p>Le checkpoint inclut:</p> <ul> <li>config model</li> <li>poids model</li> <li>etat optimizer</li> <li>config trainer</li> <li>pertes recentes</li> <li>metadata tokenizer</li> </ul>"},{"location":"fr/training/#conseils-perf","title":"Conseils perf","text":"<ul> <li>CPU: preset <code>base.toml</code> + peu d'iterations</li> <li>GPU 8GB: utiliser presets 50M</li> <li>ajuster <code>gradient_accumulation_steps</code> selon VRAM</li> </ul>"},{"location":"fr/training/#erreurs-connues","title":"Erreurs connues","text":""},{"location":"fr/training/#binary-shards-not-found","title":"<code>Binary shards not found</code>","text":"<p>Regenerer dataset en <code>bin</code>.</p>"},{"location":"fr/training/#unable-to-infer-vocab_size","title":"<code>Unable to infer vocab_size</code>","text":"<p>Definir <code>model.vocab_size</code> ou verifier <code>meta.json</code>.</p>"},{"location":"fr/training/#suite","title":"Suite","text":"<p>Voir Inference et Demo.</p>"},{"location":"fr/troubleshooting/","title":"D\u00e9pannage","text":"<p>Probl\u00e8mes fr\u00e9quents de setup/runtime avec correctifs rapides.</p>"},{"location":"fr/troubleshooting/#installation-pytorch-cpucuda","title":"Installation PyTorch (CPU/CUDA)","text":"<p>Utilisez le s\u00e9lecteur officiel selon OS + version CUDA :</p> <p>https://pytorch.org/get-started/locally/</p> <p>Commande type :</p> <pre><code>pip install -e \".[torch]\"\n</code></pre>"},{"location":"fr/troubleshooting/#cuda-non-detecte-torchcudais_available-false","title":"CUDA Non D\u00e9tect\u00e9 (<code>torch.cuda.is_available() == False</code>)","text":"<p>V\u00e9rifiez :</p> <ul> <li>Build PyTorch compatible avec votre runtime/driver CUDA.</li> <li>Driver NVIDIA install\u00e9 et \u00e0 jour.</li> <li>M\u00eame venv/Python utilis\u00e9 pour installer et ex\u00e9cuter PyTorch.</li> </ul> <p>V\u00e9rification rapide :</p> <pre><code>python -c \"import torch; print(torch.__version__, torch.cuda.is_available())\"\n</code></pre>"},{"location":"fr/troubleshooting/#oom-quoi-reduire-en-premier","title":"OOM: Quoi R\u00e9duire en Premier","text":"<p>En cas d'erreur m\u00e9moire, r\u00e9duire dans cet ordre :</p> <ol> <li><code>training.batch_size</code></li> <li><code>model.block_size</code> (longueur de s\u00e9quence)</li> <li><code>training.gradient_accumulation_steps</code></li> <li>Strat\u00e9gie de pr\u00e9cision (si vous ajoutez mixed precision)</li> </ol> <p>Si n\u00e9cessaire, utiliser un preset de taille plus petite.</p>"},{"location":"fr/troubleshooting/#emplacement-de-metajson","title":"Emplacement de <code>meta.json</code>","text":"<p>L'emplacement d\u00e9pend du format de sortie :</p> <ul> <li>Pipeline <code>txt</code>: g\u00e9n\u00e9ralement <code>data/processed/meta.json</code></li> <li>Pipeline <code>bin</code>: g\u00e9n\u00e9ralement <code>data/meta.json</code></li> </ul> <p>Si <code>train.py</code> ne trouve pas la metadata, v\u00e9rifier <code>processed_dir</code> dans le TOML et le dossier de sortie r\u00e9el de <code>prepare_data.py</code>.</p>"},{"location":"fr/troubleshooting/#flashattention-non-disponible","title":"FlashAttention Non Disponible","text":"<p>LabCore applique un fallback automatiquement :</p> <ul> <li>Priorit\u00e9: kernel FlashAttention (si dispo)</li> <li>Fallback: PyTorch SDPA</li> <li>Dernier chemin: attention causale standard</li> </ul> <p>Vous pouvez garder <code>use_flash = true</code>, le fallback g\u00e8re les environnements non support\u00e9s.</p>"},{"location":"fr/troubleshooting/#notes-windows","title":"Notes Windows","text":"<ul> <li>Activer le venv avant les commandes.</li> <li>Citer les chemins si espaces.</li> <li>Ex\u00e9cuter depuis la racine du repo pour conserver les chemins relatifs.</li> <li>Si execution policy bloque les scripts, utiliser <code>python ...</code> directement.</li> </ul>"}]}
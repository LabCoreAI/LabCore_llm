{"config":{"lang":["en","fr"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"LabCore LLM","text":"<p>This documentation is the operational guide for running LabCore end to end: data preparation, training, inference, and export. English pages in <code>docs/</code> are the source of truth, and French pages in <code>docs/fr/</code> are concise mirrors.</p> <p> </p>"},{"location":"#reference-preset-used-across-docs","title":"Reference Preset Used Across Docs","text":"<p>All examples are standardized on this reference setup:</p> <ul> <li>Dataset: <code>tinyshakespeare</code></li> <li>Tokenizer: <code>char</code></li> <li><code>CONFIG_EXAMPLE = configs/base.toml</code></li> <li>Canonical training override: <code>--max-iters 5000</code></li> <li><code>CHECKPOINT = checkpoints/ckpt_last.pt</code></li> <li><code>META_TXT = data/processed/meta.json</code></li> <li><code>META_BIN = data/meta.json</code></li> </ul> <p>Tip</p> <p>Keep these values unchanged for your first full run. Most failures come from checkpoint and metadata path mismatches.</p>"},{"location":"#quick-install","title":"Quick Install","text":"<pre><code>python -m pip install -e \".[torch,dev]\"\n</code></pre> <p>For Hugging Face export and demo UI:</p> <pre><code>python -m pip install -e \".[torch,hf,demo]\"\n</code></pre>"},{"location":"#quick-start-commands","title":"Quick Start Commands","text":"<pre><code>python scripts/prepare_data.py --dataset tinyshakespeare --tokenizer char --output-format txt --output-dir data/processed\npython train.py --config configs/base.toml --tokenizer char --max-iters 5000\npython generate.py --checkpoint checkpoints/ckpt_last.pt --meta data/processed/meta.json --tokenizer char --prompt \"To be\"\n</code></pre> <p>Expected artifacts:</p> <ul> <li><code>checkpoints/ckpt_last.pt</code></li> <li><code>checkpoints/train_log.json</code></li> <li><code>data/processed/meta.json</code></li> </ul>"},{"location":"#end-to-end-flow","title":"End-to-End Flow","text":"<pre><code>prepare_data.py -&gt; train.py -&gt; generate.py/demo_gradio.py -&gt; export_hf.py -&gt; quantize_gguf.py\n</code></pre>"},{"location":"#documentation-map","title":"Documentation Map","text":""},{"location":"#guides","title":"Guides","text":"<ul> <li>Getting Started: environment setup and a reproducible first run.</li> <li>Data Pipeline: build <code>txt</code> or <code>bin</code> datasets and metadata.</li> <li>Training: run training, checkpointing, and format selection.</li> <li>Inference &amp; Demo: CLI generation and Gradio demo.</li> <li>Fine-Tuning: LoRA instruction tuning workflow.</li> <li>Export &amp; Deployment: HF export and GGUF conversion.</li> </ul>"},{"location":"#reference","title":"Reference","text":"<ul> <li>Configuration: complete TOML key reference.</li> <li>Operations: artifacts, release flow, and operational checks.</li> <li>Troubleshooting: anchored fixes for common failures.</li> <li>Benchmarks: benchmark template and reporting method.</li> </ul>"},{"location":"#development","title":"Development","text":"<ul> <li>Developer Guide: contributor workflow and validation commands.</li> </ul>"},{"location":"#next-related","title":"Next / Related","text":"<ul> <li>Getting Started</li> <li>Data Pipeline</li> <li>Training</li> </ul>"},{"location":"benchmarks/","title":"Benchmarks","text":"<p>Use this page to log reproducible performance results for the same doc preset and path conventions. Prerequisite: run with fixed config, tokenizer, and data format before recording numbers.</p>"},{"location":"benchmarks/#commands","title":"Command(s)","text":"<p>Reference benchmark launch command:</p> <pre><code>python train.py --config configs/base.toml --tokenizer char --max-iters 5000 --device cuda\n</code></pre>"},{"location":"benchmarks/#benchmark-table-template","title":"Benchmark Table Template","text":"Preset Dataset / Tokenizer Device Batch / Seq / Accum Throughput (it/s or tok/s) Max VRAM Notes <code>configs/base.toml</code> <code>tinyshakespeare / char</code> <code>cpu</code> <code>8 / 512 / 1</code> <code>TBD</code> <code>TBD</code> template row"},{"location":"benchmarks/#rtx-4060-reference-entry","title":"RTX 4060 Reference Entry","text":"<p>No validated RTX 4060 measurement is checked into this repository yet. Use this row as a fill-in template for your first stable run:</p> Preset Dataset / Tokenizer Device Batch / Seq / Accum Throughput (it/s or tok/s) Max VRAM Notes <code>configs/base.toml</code> <code>tinyshakespeare / char</code> <code>RTX 4060</code> <code>8 / 512 / 2</code> <code>TBD</code> <code>TBD</code> record driver + torch version"},{"location":"benchmarks/#how-to-add-your-benchmark","title":"How to Add Your Benchmark","text":"<ol> <li>Run at least one warmup phase before measuring.</li> <li>Log exact preset path, overrides, and <code>training.data_format</code>.</li> <li>Record hardware (<code>GPU</code>, driver) and software (<code>torch</code>, CUDA version).</li> <li>Add one table row per run and avoid mixing metrics from different settings.</li> </ol> <p>Tip</p> <p>If numbers look unstable, start with the Stable generation and debug guidance and verify no fallback to CPU.</p>"},{"location":"benchmarks/#related","title":"Related","text":"<ul> <li>Training</li> <li>Configuration Reference</li> <li>Operations</li> </ul>"},{"location":"configuration-reference/","title":"Configuration Reference","text":"<p>Use this page as the single source of truth for <code>train.py</code> TOML keys and defaults. Prerequisite: familiarity with the reference preset (<code>configs/base.toml</code>).</p>"},{"location":"configuration-reference/#canonical-values-used-in-guides","title":"Canonical Values Used in Guides","text":"<ul> <li><code>CONFIG_EXAMPLE = configs/base.toml</code></li> <li><code>CHECKPOINT = checkpoints/ckpt_last.pt</code></li> <li><code>META_TXT = data/processed/meta.json</code></li> <li><code>META_BIN = data/meta.json</code></li> </ul>"},{"location":"configuration-reference/#section-and-key-reference","title":"Section and Key Reference","text":""},{"location":"configuration-reference/#general","title":"<code>[general]</code>","text":"Key Type Typical value Notes <code>run_name</code> string <code>\"tiny_char_baseline\"</code> Informational run label. <code>seed</code> int <code>1337</code> Optional random seed field. <code>tokenizer</code> string <code>\"char\"</code> or <code>\"bpe\"</code> Default is <code>\"char\"</code> if omitted."},{"location":"configuration-reference/#data","title":"<code>[data]</code>","text":"Key Type Typical value Notes <code>dataset</code> string <code>\"tinyshakespeare\"</code> Metadata label. <code>processed_dir</code> string path <code>\"data/processed\"</code> Default is <code>\"data/processed\"</code> if omitted."},{"location":"configuration-reference/#model","title":"<code>[model]</code>","text":"Key Type Typical value Notes <code>vocab_size</code> int <code>65</code> (char) / <code>50257</code> (bpe) Can be inferred from metadata/tokenizer when omitted. <code>block_size</code> int <code>512</code> Sequence length. <code>n_layer</code> int <code>6</code> Transformer depth. <code>n_head</code> int <code>8</code> Attention heads. <code>n_embd</code> int <code>256</code> Embedding size. <code>dropout</code> float <code>0.1</code> Dropout rate. <code>bias</code> bool <code>true</code> Linear layer bias toggle. <code>use_rope</code> bool <code>false</code>/<code>true</code> Default <code>false</code>. <code>use_flash</code> bool <code>false</code>/<code>true</code> Default <code>false</code>."},{"location":"configuration-reference/#optimizer","title":"<code>[optimizer]</code>","text":"Key Type Typical value Notes <code>learning_rate</code> float <code>3e-4</code> Falls back to <code>[training]</code> if missing. <code>weight_decay</code> float <code>0.01</code> Falls back to <code>[training]</code> if missing. <code>beta1</code> float <code>0.9</code> AdamW beta1. <code>beta2</code> float <code>0.95</code> AdamW beta2. <code>grad_clip</code> float <code>1.0</code> Falls back to <code>[training]</code> if missing."},{"location":"configuration-reference/#training","title":"<code>[training]</code>","text":"Key Type Typical value Notes <code>batch_size</code> int <code>8</code> Micro-batch size per step. <code>gradient_accumulation_steps</code> int <code>1</code> to <code>8</code> Effective batch multiplier. <code>max_iters</code> int <code>5000</code> Docs reference value. <code>warmup_iters</code> int <code>0</code> to <code>500</code> LR warmup steps. <code>lr_decay_iters</code> int <code>5000</code> Usually align with <code>max_iters</code>. <code>min_lr</code> float <code>3e-5</code> Cosine LR floor. <code>eval_interval</code> int <code>200</code> Eval/checkpoint cadence. <code>eval_iters</code> int <code>50</code> Validation batches per eval. <code>log_interval</code> int <code>20</code> Console logging cadence. <code>save_interval</code> int <code>200</code> or <code>500</code> Optional extra save cadence. <code>device</code> string <code>\"cuda\"</code> or <code>\"cpu\"</code> CLI <code>--device</code> overrides this. <code>checkpoint_dir</code> string path <code>\"checkpoints\"</code> Produces <code>ckpt_last.pt</code> and <code>train_log.json</code>. <code>data_format</code> string <code>\"txt\"</code> or <code>\"bin\"</code> Default is <code>\"txt\"</code> if omitted."},{"location":"configuration-reference/#generation","title":"<code>[generation]</code>","text":"Key Type Typical value Notes <code>max_new_tokens</code> int <code>200</code> Useful for shared defaults in docs. <code>temperature</code> float <code>0.6</code> to <code>0.9</code> Sampling randomness. <code>top_k</code> int <code>40</code> to <code>50</code> Sampling truncation."},{"location":"configuration-reference/#loader-defaults-applied-automatically","title":"Loader Defaults Applied Automatically","text":"<p><code>load_config</code> guarantees:</p> <ul> <li><code>general.tokenizer = \"char\"</code></li> <li><code>data.processed_dir = \"data/processed\"</code></li> <li><code>model.use_rope = false</code></li> <li><code>model.use_flash = false</code></li> <li><code>training.data_format = \"txt\"</code></li> </ul>"},{"location":"configuration-reference/#example-minimal-small-config","title":"Example: Minimal Small Config","text":"<pre><code>[general]\ntokenizer = \"char\"\n\n[data]\ndataset = \"tinyshakespeare\"\nprocessed_dir = \"data/processed\"\n\n[model]\nblock_size = 256\nn_layer = 4\nn_head = 4\nn_embd = 192\nvocab_size = 65\ndropout = 0.1\nbias = true\nuse_rope = false\nuse_flash = false\n\n[training]\nbatch_size = 8\ngradient_accumulation_steps = 1\nmax_iters = 2000\neval_interval = 200\neval_iters = 50\nlog_interval = 20\ndevice = \"cpu\"\ncheckpoint_dir = \"checkpoints\"\ndata_format = \"txt\"\n</code></pre>"},{"location":"configuration-reference/#example-rtx-4060-starting-point-8gb-validate-locally","title":"Example: RTX 4060 Starting Point (8GB, Validate Locally)","text":"<pre><code>[general]\ntokenizer = \"char\"\n\n[data]\ndataset = \"tinyshakespeare\"\nprocessed_dir = \"data/processed\"\n\n[model]\nblock_size = 512\nn_layer = 6\nn_head = 8\nn_embd = 256\ndropout = 0.1\nbias = true\nuse_rope = false\nuse_flash = false\n\n[optimizer]\nlearning_rate = 3e-4\nweight_decay = 0.01\nbeta1 = 0.9\nbeta2 = 0.95\ngrad_clip = 1.0\n\n[training]\nbatch_size = 8\ngradient_accumulation_steps = 2\nmax_iters = 5000\nwarmup_iters = 200\nlr_decay_iters = 5000\nmin_lr = 3e-5\neval_interval = 200\neval_iters = 50\nlog_interval = 20\nsave_interval = 200\ndevice = \"cuda\"\ncheckpoint_dir = \"checkpoints\"\ndata_format = \"txt\"\n</code></pre> <p>Note</p> <p>The RTX 4060 block above is a starting template, not a guaranteed benchmark profile. Adjust based on your exact VRAM and driver stack.</p>"},{"location":"configuration-reference/#related","title":"Related","text":"<ul> <li>Training</li> <li>Operations</li> <li>Benchmarks</li> </ul>"},{"location":"data-pipeline/","title":"Data Pipeline","text":"<p>Use this page to prepare training data and metadata in a predictable layout. Prerequisite: dependencies installed from Getting Started.</p>"},{"location":"data-pipeline/#commands","title":"Command(s)","text":"<p>Reference <code>txt</code> pipeline (used by <code>configs/base.toml</code>):</p> <pre><code>python scripts/prepare_data.py \\\n  --dataset tinyshakespeare \\\n  --tokenizer char \\\n  --output-format txt \\\n  --raw-dir data/raw \\\n  --output-dir data/processed \\\n  --val-ratio 0.1\n</code></pre> <p>Alternative <code>bin</code> pipeline:</p> <pre><code>python scripts/prepare_data.py \\\n  --dataset tinyshakespeare \\\n  --tokenizer char \\\n  --output-format bin \\\n  --raw-dir data/raw \\\n  --output-dir data/processed \\\n  --val-ratio 0.1\n</code></pre>"},{"location":"data-pipeline/#output-files-artifacts-produced","title":"Output Files / Artifacts Produced","text":"<p><code>txt</code> format (<code>output-dir = data/processed</code>):</p> <ul> <li><code>data/processed/train.txt</code></li> <li><code>data/processed/val.txt</code></li> <li><code>data/processed/corpus.txt</code></li> <li><code>data/processed/train.npy</code></li> <li><code>data/processed/val.npy</code></li> <li><code>data/processed/meta.json</code> (<code>META_TXT</code>)</li> </ul> <p><code>bin</code> format:</p> <ul> <li><code>data/train.bin</code></li> <li><code>data/val.bin</code></li> <li><code>data/meta.json</code> (<code>META_BIN</code>)</li> </ul> <p>Note</p> <p>For <code>--output-format bin</code>, if <code>--output-dir</code> ends with <code>processed</code>, binary files are written to its parent (<code>data/</code>).</p>"},{"location":"data-pipeline/#format-selection","title":"Format Selection","text":"<ul> <li>Use <code>txt</code> when training with <code>training.data_format = \"txt\"</code> and metadata at <code>data/processed/meta.json</code>.</li> <li>Use <code>bin</code> when training with <code>training.data_format = \"bin\"</code> and metadata at <code>data/meta.json</code>.</li> </ul>"},{"location":"data-pipeline/#common-errors","title":"Common Errors","text":"<ul> <li>Missing binary shards: see Binary shards not found.</li> <li>Wrong metadata path: see Meta path mismatch.</li> <li>Char tokenizer vocab issues: see Char vocab missing.</li> </ul>"},{"location":"data-pipeline/#next-related","title":"Next / Related","text":"<ul> <li>Training</li> <li>Inference &amp; Demo</li> <li>Troubleshooting</li> </ul>"},{"location":"developer-guide/","title":"Developer Guide","text":"<p>This page is for contributors working on the codebase itself.</p>"},{"location":"developer-guide/#repository-map","title":"Repository Map","text":"<pre><code>src/labcore_llm/\n  config/      # TOML loader and defaults\n  data/        # dataset abstractions\n  model/       # GPT model implementation\n  tokenizer/   # char + BPE tokenizers\n  trainer/     # training loop, scheduler, checkpointing\n\nscripts/       # data prep, export, quantize, fine-tune helpers\nconfigs/       # TOML presets\ntests/         # unit tests\n</code></pre>"},{"location":"developer-guide/#local-dev-environment","title":"Local Dev Environment","text":"<pre><code>python -m venv .venv\n## PowerShell\n.\\.venv\\Scripts\\Activate.ps1\npython -m pip install --upgrade pip\npython -m pip install -e \".[torch,dev]\"\n</code></pre>"},{"location":"developer-guide/#validation-commands","title":"Validation Commands","text":"<p>Run tests:</p> <pre><code>python -m pytest -q\n</code></pre> <p>Run lint rules aligned with CI:</p> <pre><code>ruff check src scripts tests train.py generate.py demo_gradio.py --select E9,F63,F7,F82\n</code></pre>"},{"location":"developer-guide/#ci-workflows","title":"CI Workflows","text":"<ul> <li><code>.github/workflows/ci.yml</code>: lint + tests</li> <li><code>.github/workflows/docs.yml</code>: MkDocs build and deploy</li> </ul>"},{"location":"developer-guide/#contribution-quality-bar","title":"Contribution Quality Bar","text":"<ul> <li>Keep commits focused and atomic.</li> <li>Update docs for behavior/CLI changes.</li> <li>Add tests for bug fixes and new logic.</li> <li>Do not commit large data/model artifacts.</li> </ul>"},{"location":"developer-guide/#packaging-notes","title":"Packaging Notes","text":"<ul> <li>Project uses <code>src/</code> layout with setuptools.</li> <li>Optional dependency groups are defined in <code>pyproject.toml</code>.</li> <li>Entry scripts are plain Python files, not console-script wrappers.</li> </ul>"},{"location":"export-and-deployment/","title":"Export and Deployment","text":"<p>Use this page to package a local checkpoint for Hugging Face and optional GGUF deployment. Prerequisites: a valid checkpoint (<code>checkpoints/ckpt_last.pt</code>) and matching metadata (<code>data/processed/meta.json</code> or <code>data/meta.json</code>).</p>"},{"location":"export-and-deployment/#hf-export-vs-gguf-export","title":"HF Export vs GGUF Export","text":"<ul> <li>HF export creates standard model artifacts for Hugging Face workflows.</li> <li>GGUF conversion creates quantized files for <code>llama.cpp</code>-style runtimes.</li> </ul>"},{"location":"export-and-deployment/#commands","title":"Command(s)","text":"<p>Export local checkpoint to HF layout:</p> <pre><code>python scripts/export_hf.py \\\n  --checkpoint checkpoints/ckpt_last.pt \\\n  --meta data/processed/meta.json \\\n  --output-dir outputs/hf_export\n</code></pre> <p>Push exported folder to HF Hub:</p> <pre><code>python scripts/export_hf.py \\\n  --checkpoint checkpoints/ckpt_last.pt \\\n  --meta data/processed/meta.json \\\n  --output-dir outputs/hf_export \\\n  --push \\\n  --repo-id GhostPunishR/labcore-llm-50M\n</code></pre> <p>Convert HF export to GGUF and quantize:</p> <pre><code>python scripts/quantize_gguf.py \\\n  --hf-dir outputs/hf_export \\\n  --llama-cpp-dir third_party/llama.cpp \\\n  --output-dir outputs/gguf \\\n  --quant-type Q4_K_M\n</code></pre>"},{"location":"export-and-deployment/#output-files-artifacts-produced","title":"Output Files / Artifacts Produced","text":"<p><code>outputs/hf_export/</code>:</p> <ul> <li><code>model.safetensors</code></li> <li><code>config.json</code></li> <li><code>tokenizer.json</code></li> <li><code>README.md</code></li> </ul> <p><code>outputs/gguf/</code>:</p> <ul> <li><code>labcore-50m-f16.gguf</code></li> <li><code>labcore-50m-q4_k_m.gguf</code> (or <code>q5_k_m</code> / both when <code>--quant-type all</code>)</li> </ul> <p>Warning</p> <p>GGUF conversion requires a valid <code>llama.cpp</code> checkout with conversion script and quantizer binary available.</p>"},{"location":"export-and-deployment/#common-errors","title":"Common Errors","text":"<ul> <li>Metadata mismatch (<code>txt</code> vs <code>bin</code>): see Meta path mismatch.</li> <li>Missing <code>huggingface_hub</code>/<code>safetensors</code>: see Torch not installed.</li> <li>Missing llama.cpp tools: see Windows path and policy issues.</li> </ul>"},{"location":"export-and-deployment/#next-related","title":"Next / Related","text":"<ul> <li>Fine-Tuning</li> <li>Operations</li> <li>Troubleshooting</li> </ul>"},{"location":"fine-tuning/","title":"Fine-Tuning","text":"<p>Use this page for LoRA instruction tuning on HF-compatible CausalLM checkpoints. Prerequisites: HF + finetune dependencies and an accessible base model.</p>"},{"location":"fine-tuning/#commands","title":"Command(s)","text":"<pre><code>python scripts/fine_tune_instruction.py \\\n  --model-id GhostPunishR/labcore-llm-50M \\\n  --dataset yahma/alpaca-cleaned \\\n  --dataset-split train \\\n  --output-dir outputs/lora_instruction \\\n  --config configs/bpe_rope_flash/bpe_50M_rope_flash.toml \\\n  --max-samples 20000 \\\n  --epochs 1\n</code></pre> <p>Dependencies:</p> <pre><code>python -m pip install -e \".[torch,hf,finetune]\"\n</code></pre>"},{"location":"fine-tuning/#output-files-artifacts-produced","title":"Output Files / Artifacts Produced","text":"<ul> <li><code>outputs/lora_instruction/</code> (LoRA adapter + tokenizer files)</li> </ul>"},{"location":"fine-tuning/#dataset-mapping","title":"Dataset Mapping","text":"<p>The script accepts common field aliases:</p> <ul> <li>Instruction: <code>instruction</code>, <code>question</code>, <code>prompt</code></li> <li>Input: <code>input</code>, <code>context</code></li> <li>Output: <code>output</code>, <code>response</code>, <code>answer</code></li> </ul>"},{"location":"fine-tuning/#common-errors","title":"Common Errors","text":"<ul> <li>Missing HF dependencies: see Torch not installed.</li> <li>OOM during fine-tuning: see Out of memory.</li> <li>Wrong base model/tokenizer expectations: verify config and model compatibility before launching.</li> </ul> <p>Note</p> <p>Fine-tuning uses the HF trainer stack and writes to <code>outputs/</code> rather than <code>checkpoints/</code>.</p>"},{"location":"fine-tuning/#next-related","title":"Next / Related","text":"<ul> <li>Configuration Reference</li> <li>Export &amp; Deployment</li> <li>Operations</li> </ul>"},{"location":"getting-started/","title":"Getting Started","text":"<p>Use this page to get a reproducible first run in about 5 minutes and confirm your environment is healthy. Prerequisites: Python <code>3.11+</code>, <code>pip</code>, and optional CUDA GPU.</p>"},{"location":"getting-started/#commands","title":"Command(s)","text":"<p>Install dependencies:</p> <pre><code>python -m pip install -e \".[torch,dev]\"\n</code></pre> <p>Check CUDA visibility:</p> <pre><code>python -c \"import torch; print(torch.cuda.is_available()); print(torch.version.cuda)\"\n</code></pre> <p>Quick Start (reference preset: tinyshakespeare + char + <code>configs/base.toml</code>):</p> <pre><code>python scripts/prepare_data.py --dataset tinyshakespeare --tokenizer char --output-format txt --output-dir data/processed\npython train.py --config configs/base.toml --tokenizer char --max-iters 5000\npython generate.py --checkpoint checkpoints/ckpt_last.pt --meta data/processed/meta.json --tokenizer char --prompt \"To be\"\n</code></pre> <p>Tip</p> <p>For a strict 5-minute smoke test, start the training command, wait for the first eval/checkpoint output, then stop and run generation.</p>"},{"location":"getting-started/#output-files-artifacts-produced","title":"Output Files / Artifacts Produced","text":"<ul> <li><code>data/processed/meta.json</code></li> <li><code>data/processed/train.txt</code>, <code>data/processed/val.txt</code></li> <li><code>checkpoints/ckpt_last.pt</code></li> <li><code>checkpoints/train_log.json</code></li> </ul>"},{"location":"getting-started/#success-checklist","title":"Success Checklist","text":"<ul> <li>Training logs show at least two <code>train_loss</code> lines and the value trends down.</li> <li>A checkpoint exists at <code>checkpoints/ckpt_last.pt</code>.</li> <li><code>generate.py</code> returns non-empty text from your prompt.</li> </ul>"},{"location":"getting-started/#common-errors","title":"Common Errors","text":"<ul> <li><code>ModuleNotFoundError: torch</code>: see Torch not installed.</li> <li>CUDA expected but disabled: see CUDA not detected.</li> <li>Metadata mismatch: see Meta path mismatch.</li> </ul> <p>Warning</p> <p>Keep <code>--checkpoint</code> and <code>--meta</code> aligned with the same run. Mixed files from different runs produce misleading results.</p>"},{"location":"getting-started/#next-related","title":"Next / Related","text":"<ul> <li>Data Pipeline</li> <li>Training</li> <li>Troubleshooting</li> </ul>"},{"location":"inference-and-demo/","title":"Inference and Demo","text":"<p>Use this page to run deterministic CLI generation and the local Gradio demo. Prerequisites: a trained checkpoint (<code>checkpoints/ckpt_last.pt</code>) and matching metadata.</p>"},{"location":"inference-and-demo/#commands","title":"Command(s)","text":"<p>CLI generation (reference paths):</p> <pre><code>python generate.py \\\n  --checkpoint checkpoints/ckpt_last.pt \\\n  --meta data/processed/meta.json \\\n  --tokenizer char \\\n  --prompt \"To be\" \\\n  --max-new-tokens 200 \\\n  --temperature 0.9 \\\n  --top-k 40 \\\n  --device cpu\n</code></pre> <p>Gradio demo from local checkpoint:</p> <pre><code>python demo_gradio.py \\\n  --source local \\\n  --checkpoint checkpoints/ckpt_last.pt \\\n  --meta data/processed/meta.json \\\n  --device cpu \\\n  --port 7860\n</code></pre> <p>Gradio demo from Hugging Face:</p> <pre><code>python demo_gradio.py \\\n  --source hf \\\n  --repo-id GhostPunishR/labcore-llm-50M \\\n  --device cpu \\\n  --port 7860\n</code></pre>"},{"location":"inference-and-demo/#stable-generation-settings-debug-mode","title":"Stable Generation Settings (Debug Mode)","text":"<p>Use conservative sampling when debugging reproducibility:</p> <ul> <li><code>temperature = 0.2</code> to reduce randomness</li> <li><code>top-k = 20</code> (or lower)</li> <li><code>max-new-tokens = 80</code> for quick checks</li> </ul> <p>Tip</p> <p>If output quality suddenly drops, first verify that <code>--meta</code> belongs to the same tokenizer/checkpoint run.</p>"},{"location":"inference-and-demo/#sampling-controls","title":"Sampling Controls","text":"<ul> <li><code>temperature</code>: scales logits before sampling.</li> <li><code>top_k</code>: keeps only the <code>k</code> most likely tokens.</li> <li><code>top_p</code>: nucleus sampling cutoff (<code>1.0</code> disables it).</li> <li><code>repetition_penalty</code>: penalizes already generated tokens (<code>1.0</code> disables it).</li> </ul> <p><code>top_p</code> and <code>repetition_penalty</code> are read from <code>[generation]</code> when using <code>generate.py --config ...</code>.</p> <p>RTX 4060 stable example:</p> <pre><code>[generation]\ntemperature = 0.6\ntop_k = 50\ntop_p = 0.9\nrepetition_penalty = 1.1\n</code></pre>"},{"location":"inference-and-demo/#output-files-artifacts-produced","title":"Output Files / Artifacts Produced","text":"<ul> <li>CLI: generated text in terminal output</li> <li>Demo: generated text in Gradio UI</li> <li>No new model files unless you explicitly export</li> </ul>"},{"location":"inference-and-demo/#common-errors","title":"Common Errors","text":"<ul> <li>Char tokenizer metadata missing: see Char vocab missing.</li> <li>Metadata path mismatch (<code>txt</code> vs <code>bin</code>): see Meta path mismatch.</li> <li>CUDA fallback behavior: see CUDA not detected.</li> </ul>"},{"location":"inference-and-demo/#next-related","title":"Next / Related","text":"<ul> <li>Export &amp; Deployment</li> <li>Fine-Tuning</li> <li>Troubleshooting</li> </ul>"},{"location":"operations/","title":"Operations","text":"<p>Use this page for operational folder hygiene, validation checks, and release/security workflows. Prerequisite: project dependencies installed in your active environment.</p>"},{"location":"operations/#artifact-directories","title":"Artifact Directories","text":"<ul> <li><code>data/</code>: prepared datasets, including <code>data/meta.json</code> for <code>bin</code> runs.</li> <li><code>checkpoints/</code>: training outputs (<code>ckpt_last.pt</code>, <code>train_log.json</code>).</li> <li><code>outputs/</code>: exported artifacts (<code>hf_export/</code>, <code>gguf/</code>, fine-tuning outputs).</li> <li><code>runs/</code>: optional experiment logs or external tracker exports (not auto-created by core scripts).</li> </ul> <p>Note</p> <p>The core training scripts write to <code>checkpoints/</code> and <code>outputs/</code>. Keep those directories versioned in your run notes, but do not commit large artifacts.</p>"},{"location":"operations/#commands","title":"Command(s)","text":"<p>Local quality checks:</p> <pre><code>python -m pytest -q\nruff check src scripts tests train.py generate.py demo_gradio.py --select E9,F63,F7,F82\n</code></pre> <p>Docs build checks:</p> <pre><code>python -m pip install -r docs/requirements.txt\npython -m mkdocs build\n</code></pre>"},{"location":"operations/#output-files-artifacts-produced","title":"Output Files / Artifacts Produced","text":"<ul> <li>Test and lint logs in terminal output</li> <li>Built docs in <code>site/</code> after <code>mkdocs build</code></li> <li>CI pipelines: <code>.github/workflows/ci.yml</code> and <code>.github/workflows/docs.yml</code></li> </ul>"},{"location":"operations/#security-and-release","title":"Security and Release","text":"<p>Security reporting (see <code>SECURITY.md</code>):</p> <ul> <li>Use GitHub Security Advisories for vulnerabilities</li> <li>Include impact, affected components, and reproduction steps</li> </ul> <p>Release flow (see <code>RELEASE.md</code>):</p> <ol> <li>Confirm CI is green.</li> <li>Run local validation commands.</li> <li>Update version and changelog.</li> <li>Tag and publish the release.</li> </ol>"},{"location":"operations/#common-errors","title":"Common Errors","text":"<ul> <li>Missing dependencies: see Torch not installed.</li> <li>Metadata and path confusion: see Meta path mismatch.</li> <li>CUDA expected but unavailable: see CUDA not detected.</li> </ul>"},{"location":"operations/#related","title":"Related","text":"<ul> <li>Troubleshooting</li> <li>Benchmarks</li> <li>Developer Guide</li> </ul>"},{"location":"training/","title":"Training","text":"<p>Use this page to launch, monitor, and checkpoint training runs with consistent settings. Prerequisites: prepared dataset and metadata from Data Pipeline.</p>"},{"location":"training/#commands","title":"Command(s)","text":"<p>Reference training command:</p> <pre><code>python train.py --config configs/base.toml --tokenizer char --max-iters 5000\n</code></pre> <p>Device override examples:</p> <pre><code>python train.py --config configs/base.toml --tokenizer char --max-iters 5000 --device cpu\npython train.py --config configs/base.toml --tokenizer char --max-iters 5000 --device cuda\n</code></pre> <p><code>train.py</code> flags:</p> <ul> <li><code>--config</code>: TOML preset path (<code>CONFIG_EXAMPLE = configs/base.toml</code>)</li> <li><code>--max-iters</code>: runtime override for total iterations</li> <li><code>--device</code>: <code>cpu</code> or <code>cuda</code></li> <li><code>--tokenizer</code>: <code>char</code> or <code>bpe</code></li> </ul>"},{"location":"training/#precision-and-gradient-accumulation","title":"Precision and Gradient Accumulation","text":"<p>Configure these in <code>[training]</code>:</p> <ul> <li><code>grad_accum_steps</code>: gradient accumulation factor (default <code>1</code>)</li> <li><code>precision</code>: <code>fp32</code> (default), <code>fp16</code>, or <code>bf16</code></li> </ul> <p><code>effective_batch_size = batch_size * grad_accum_steps</code></p> <p>Mixed precision is enabled only when <code>device = \"cuda\"</code> and <code>precision != \"fp32\"</code>. On CPU, training falls back to <code>fp32</code>.</p> <p>RTX 4060 example:</p> <pre><code>[training]\nbatch_size = 8\ngrad_accum_steps = 4\nprecision = \"fp16\"\n</code></pre>"},{"location":"training/#data_format-and-metadata-mapping","title":"<code>data_format</code> and Metadata Mapping","text":"Training mode Config value Data artifacts expected Metadata path Text pipeline <code>training.data_format = \"txt\"</code> <code>data/processed/train.txt</code> + <code>data/processed/val.txt</code> (or <code>.npy</code>) <code>data/processed/meta.json</code> (<code>META_TXT</code>) Binary pipeline <code>training.data_format = \"bin\"</code> <code>data/train.bin</code> + <code>data/val.bin</code> <code>data/meta.json</code> (<code>META_BIN</code>) <p>Note</p> <p>For binary mode, if <code>data.processed_dir</code> points to <code>data/processed</code>, <code>train.py</code> automatically checks the parent directory (<code>data/</code>) for <code>train.bin</code> and <code>val.bin</code>.</p>"},{"location":"training/#checkpointing-and-resume-behavior","title":"Checkpointing and Resume Behavior","text":"<p>Produced during training:</p> <ul> <li><code>checkpoints/ckpt_last.pt</code> (<code>CHECKPOINT</code>)</li> <li><code>checkpoints/train_log.json</code></li> </ul> <p>Warning</p> <p>Native resume-from-checkpoint is not implemented in <code>train.py</code> yet. <code>ckpt_last.pt</code> is for inference/export compatibility and state inspection, not automatic continuation.</p>"},{"location":"training/#common-errors","title":"Common Errors","text":"<ul> <li>Binary shards missing: see Binary shards not found.</li> <li>Metadata path mismatch: see Meta path mismatch.</li> <li>Vocab inference failure: see Char vocab missing.</li> <li>CUDA fallback warning: see CUDA not detected.</li> </ul>"},{"location":"training/#next-related","title":"Next / Related","text":"<ul> <li>Inference &amp; Demo</li> <li>Export &amp; Deployment</li> <li>Troubleshooting</li> </ul>"},{"location":"troubleshooting/","title":"Troubleshooting","text":"<p>Use this page for fast diagnosis of common setup, data, and runtime failures. All anchors below are referenced from guide pages.</p>"},{"location":"troubleshooting/#setup","title":"Setup","text":""},{"location":"troubleshooting/#torch-not-installed","title":"Torch not installed","text":"<p>Symptoms: <code>ModuleNotFoundError: torch</code>, or scripts fail on import.</p> <p>Fix:</p> <pre><code>python -m pip install -e \".[torch,dev]\"\n</code></pre> <p>Use the official selector if you need a specific CUDA build: https://pytorch.org/get-started/locally/</p>"},{"location":"troubleshooting/#cuda-not-detected","title":"CUDA not detected","text":"<p>Symptoms: <code>torch.cuda.is_available()</code> returns <code>False</code>, or scripts print CPU fallback warnings.</p> <p>Quick check:</p> <pre><code>python -c \"import torch; print(torch.cuda.is_available()); print(torch.version.cuda)\"\n</code></pre> <p>Checks:</p> <ul> <li>NVIDIA driver installed and up to date</li> <li>Correct PyTorch build for your CUDA runtime</li> <li>Same Python environment used for install and execution</li> </ul>"},{"location":"troubleshooting/#data-and-metadata","title":"Data and Metadata","text":""},{"location":"troubleshooting/#meta-path-mismatch","title":"Meta path mismatch","text":"<p>Symptoms: generation/training behaves incorrectly or cannot load tokenizer metadata.</p> <p>Expected mapping:</p> <ul> <li><code>txt</code> pipeline -&gt; <code>data/processed/meta.json</code></li> <li><code>bin</code> pipeline -&gt; <code>data/meta.json</code></li> </ul>"},{"location":"troubleshooting/#binary-shards-not-found","title":"Binary shards not found","text":"<p>Symptoms: <code>Binary shards not found</code> during <code>training.data_format = \"bin\"</code>.</p> <p>Fix:</p> <pre><code>python scripts/prepare_data.py --dataset tinyshakespeare --tokenizer char --output-format bin --output-dir data/processed\n</code></pre>"},{"location":"troubleshooting/#char-vocab-missing","title":"Char vocab missing","text":"<p>Symptoms: <code>Char tokenizer requires vocab in meta.json</code>.</p> <p>Fix:</p> <pre><code>python scripts/prepare_data.py --dataset tinyshakespeare --tokenizer char --output-format txt --output-dir data/processed\n</code></pre> <p>Then pass the matching <code>--meta data/processed/meta.json</code> to generation/export commands.</p>"},{"location":"troubleshooting/#runtime","title":"Runtime","text":""},{"location":"troubleshooting/#oom-errors","title":"Out of memory","text":"<p>Reduce in this order:</p> <ol> <li><code>training.batch_size</code></li> <li><code>model.block_size</code></li> <li><code>training.gradient_accumulation_steps</code></li> <li>Model size / preset complexity</li> </ol>"},{"location":"troubleshooting/#flashattention-not-available","title":"FlashAttention not available","text":"<p>LabCore falls back automatically:</p> <ul> <li>FlashAttention (preferred, if available)</li> <li>PyTorch SDPA fallback</li> <li>Standard causal attention fallback</li> </ul>"},{"location":"troubleshooting/#windows-path-policy","title":"Windows path and policy issues","text":"<ul> <li>Activate your venv before commands.</li> <li>Run commands from repository root.</li> <li>Quote paths when directories contain spaces.</li> <li>If PowerShell policy blocks scripts, use <code>python ...</code> commands directly.</li> </ul>"},{"location":"fr/","title":"Accueil","text":"<p>Cette page FR resume le flux principal. La documentation EN (<code>docs/</code>) reste la reference complete. Preset de reference: <code>tinyshakespeare</code> + <code>char</code> + <code>configs/base.toml</code>, checkpoint <code>checkpoints/ckpt_last.pt</code>.</p>"},{"location":"fr/#commandes-rapides","title":"Commandes Rapides","text":"<pre><code>python -m pip install -e \".[torch,dev]\"\npython scripts/prepare_data.py --dataset tinyshakespeare --tokenizer char --output-format txt --output-dir data/processed\npython train.py --config configs/base.toml --tokenizer char --max-iters 5000\npython generate.py --checkpoint checkpoints/ckpt_last.pt --meta data/processed/meta.json --tokenizer char --prompt \"To be\"\n</code></pre>"},{"location":"fr/#voir-aussi","title":"Voir aussi","text":"<ul> <li>Getting Started (EN)</li> <li>Training (EN)</li> <li>Export &amp; Deployment (EN)</li> </ul>"},{"location":"fr/benchmarks/","title":"Benchmarks","text":"<p>Utilisez cette page pour suivre des mesures de performance reproductibles selon preset et mat\u00e9riel.</p>"},{"location":"fr/benchmarks/#modele-de-tableau","title":"Mod\u00e8le de Tableau","text":"Preset Device Batch / Seq / Accum D\u00e9bit (it/s ou tok/s) VRAM Max Notes <code>configs/base.toml</code> <code>cpu</code> <code>32 / 128 / 1</code> <code>0.00</code> <code>0.0 GB</code> placeholder"},{"location":"fr/benchmarks/#exemple-minimal","title":"Exemple Minimal","text":"<p>Une ligne par run avec param\u00e8tres exacts :</p> Preset Device Batch / Seq / Accum D\u00e9bit (it/s ou tok/s) VRAM Max Notes <code>configs/bpe_rope_flash/bpe_30M_rope_flash.toml</code> <code>cuda</code> <code>8 / 512 / 8</code> <code>TBD</code> <code>TBD</code> compl\u00e9ter apr\u00e8s run stable"},{"location":"fr/benchmarks/#conseils","title":"Conseils","text":"<ul> <li>Faire quelques it\u00e9rations de warmup avant mesure.</li> <li>Garder dataset, tokenizer et pr\u00e9cision constants entre runs.</li> <li>Noter GPU, driver et version PyTorch dans les notes.</li> </ul>"},{"location":"fr/configuration-reference/","title":"Reference Config","text":"<p><code>train.py</code> lit les fichiers TOML puis applique des valeurs par defaut.</p>"},{"location":"fr/configuration-reference/#sections-supportees","title":"Sections supportees","text":""},{"location":"fr/configuration-reference/#general","title":"<code>[general]</code>","text":"<ul> <li><code>run_name</code></li> <li><code>seed</code></li> <li><code>tokenizer</code> (<code>char</code> ou <code>bpe</code>)</li> </ul>"},{"location":"fr/configuration-reference/#data","title":"<code>[data]</code>","text":"<ul> <li><code>dataset</code></li> <li><code>processed_dir</code></li> </ul>"},{"location":"fr/configuration-reference/#model","title":"<code>[model]</code>","text":"<ul> <li><code>vocab_size</code></li> <li><code>block_size</code></li> <li><code>n_layer</code></li> <li><code>n_head</code></li> <li><code>n_embd</code></li> <li><code>dropout</code></li> <li><code>bias</code></li> <li><code>use_rope</code></li> <li><code>use_flash</code></li> </ul>"},{"location":"fr/configuration-reference/#optimizer","title":"<code>[optimizer]</code>","text":"<ul> <li><code>learning_rate</code></li> <li><code>weight_decay</code></li> <li><code>beta1</code></li> <li><code>beta2</code></li> <li><code>grad_clip</code></li> </ul>"},{"location":"fr/configuration-reference/#training","title":"<code>[training]</code>","text":"<ul> <li><code>batch_size</code></li> <li><code>gradient_accumulation_steps</code></li> <li><code>max_iters</code></li> <li><code>warmup_iters</code></li> <li><code>lr_decay_iters</code></li> <li><code>min_lr</code></li> <li><code>eval_interval</code></li> <li><code>eval_iters</code></li> <li><code>log_interval</code></li> <li><code>save_interval</code></li> <li><code>device</code></li> <li><code>checkpoint_dir</code></li> <li><code>data_format</code> (<code>txt</code> ou <code>bin</code>)</li> </ul>"},{"location":"fr/configuration-reference/#generation","title":"<code>[generation]</code>","text":"<ul> <li><code>max_new_tokens</code></li> <li><code>temperature</code></li> <li><code>top_k</code></li> </ul>"},{"location":"fr/configuration-reference/#defauts-automatiques","title":"Defauts automatiques","text":"<p>Le loader applique:</p> <ul> <li><code>general.tokenizer = \"char\"</code></li> <li><code>data.processed_dir = \"data/processed\"</code></li> <li><code>model.use_rope = false</code></li> <li><code>model.use_flash = false</code></li> <li><code>training.data_format = \"txt\"</code></li> </ul>"},{"location":"fr/configuration-reference/#exemple-minimal","title":"Exemple minimal","text":"<pre><code>[data]\nprocessed_dir = \"data/processed\"\n\n[model]\nblock_size = 128\nn_layer = 6\nn_head = 8\nn_embd = 256\nvocab_size = 65\n\n[training]\nbatch_size = 32\nmax_iters = 1000\ndevice = \"cpu\"\n</code></pre>"},{"location":"fr/data-pipeline/","title":"Pipeline Data","text":"<p>LabCore gere deux formats de sortie:</p> <ul> <li><code>txt</code> pour flux texte + numpy</li> <li><code>bin</code> pour flux binaire memmap</li> </ul>"},{"location":"fr/data-pipeline/#commande-principale","title":"Commande Principale","text":"<pre><code>python scripts/prepare_data.py \\\n  --dataset tinyshakespeare \\\n  --tokenizer char \\\n  --output-format txt \\\n  --raw-dir data/raw \\\n  --output-dir data/processed \\\n  --val-ratio 0.1\n</code></pre>"},{"location":"fr/data-pipeline/#options-cli","title":"Options CLI","text":"<ul> <li><code>--dataset</code>: <code>tinyshakespeare</code> ou <code>wikitext</code></li> <li><code>--tokenizer</code>: <code>char</code> ou <code>bpe</code></li> <li><code>--output-format</code>: <code>txt</code> ou <code>bin</code></li> <li><code>--raw-dir</code>: cache corpus brut</li> <li><code>--output-dir</code>: dossier de sortie</li> <li><code>--val-ratio</code>: ratio validation</li> </ul>"},{"location":"fr/data-pipeline/#arborescence-de-sortie","title":"Arborescence de sortie","text":""},{"location":"fr/data-pipeline/#format-txt","title":"Format <code>txt</code>","text":"<ul> <li><code>train.txt</code></li> <li><code>val.txt</code></li> <li><code>corpus.txt</code></li> <li><code>train.npy</code></li> <li><code>val.npy</code></li> <li><code>meta.json</code></li> </ul>"},{"location":"fr/data-pipeline/#format-bin","title":"Format <code>bin</code>","text":"<p>Si <code>output-dir</code> finit par <code>processed</code>, les binaires partent dans le parent:</p> <ul> <li><code>data/train.bin</code></li> <li><code>data/val.bin</code></li> <li><code>data/meta.json</code></li> </ul>"},{"location":"fr/data-pipeline/#champs-metajson","title":"Champs <code>meta.json</code>","text":"<ul> <li><code>dataset</code></li> <li><code>vocab_size</code></li> <li><code>tokenizer</code></li> <li><code>dtype</code></li> <li><code>output_format</code></li> <li>champs de fichiers texte ou bin selon format</li> </ul>"},{"location":"fr/data-pipeline/#notes-tokenizer","title":"Notes tokenizer","text":"<ul> <li><code>char</code>: vocab appris sur tout le corpus.</li> <li><code>bpe</code>: encodeur GPT-2 via <code>tiktoken</code>.</li> </ul>"},{"location":"fr/data-pipeline/#bonnes-pratiques","title":"Bonnes pratiques","text":"<ul> <li>verifier les compteurs de tokens affiches</li> <li>verifier presence de <code>meta.json</code></li> <li>aligner <code>training.data_format</code> avec les artifacts generes</li> </ul>"},{"location":"fr/data-pipeline/#suite","title":"Suite","text":"<p>Voir Entrainement.</p>"},{"location":"fr/developer-guide/","title":"Guide Dev","text":"<p>Page dediee aux contributeurs code.</p>"},{"location":"fr/developer-guide/#structure-repo","title":"Structure repo","text":"<pre><code>src/labcore_llm/\n  config/\n  data/\n  model/\n  tokenizer/\n  trainer/\n\nscripts/\nconfigs/\ntests/\n</code></pre>"},{"location":"fr/developer-guide/#environment-local","title":"Environment local","text":"<pre><code>python -m venv .venv\n.\\.venv\\Scripts\\Activate.ps1\npython -m pip install --upgrade pip\npython -m pip install -e \".[torch,dev]\"\n</code></pre>"},{"location":"fr/developer-guide/#validation-locale","title":"Validation locale","text":"<p>Tests:</p> <pre><code>python -m pytest -q\n</code></pre> <p>Lint:</p> <pre><code>ruff check src scripts tests train.py generate.py demo_gradio.py --select E9,F63,F7,F82\n</code></pre>"},{"location":"fr/developer-guide/#ci","title":"CI","text":"<ul> <li><code>.github/workflows/ci.yml</code></li> <li><code>.github/workflows/docs.yml</code></li> </ul>"},{"location":"fr/developer-guide/#regles-contribution","title":"Regles contribution","text":"<ul> <li>commits atomiques</li> <li>docs mises a jour en meme temps que le code</li> <li>tests ajoutes quand logique modifiee</li> <li>pas d'artifacts lourds dans git</li> </ul>"},{"location":"fr/export-and-deployment/","title":"Export et Deploiement","text":"<p>Cette page FR resume la sortie HF puis la conversion GGUF. Point de depart: <code>checkpoints/ckpt_last.pt</code> + metadata correspondante.</p>"},{"location":"fr/export-and-deployment/#commandes-rapides","title":"Commandes Rapides","text":"<pre><code>python scripts/export_hf.py --checkpoint checkpoints/ckpt_last.pt --meta data/processed/meta.json --output-dir outputs/hf_export\npython scripts/export_hf.py --checkpoint checkpoints/ckpt_last.pt --meta data/processed/meta.json --output-dir outputs/hf_export --push --repo-id GhostPunishR/labcore-llm-50M\npython scripts/quantize_gguf.py --hf-dir outputs/hf_export --llama-cpp-dir third_party/llama.cpp --output-dir outputs/gguf --quant-type Q4_K_M\n</code></pre>"},{"location":"fr/export-and-deployment/#artifacts","title":"Artifacts","text":"<ul> <li><code>outputs/hf_export/model.safetensors</code></li> <li><code>outputs/hf_export/config.json</code></li> <li><code>outputs/hf_export/tokenizer.json</code></li> <li><code>outputs/gguf/labcore-50m-f16.gguf</code></li> <li><code>outputs/gguf/labcore-50m-q4_k_m.gguf</code></li> </ul>"},{"location":"fr/export-and-deployment/#voir-aussi","title":"Voir aussi","text":"<ul> <li>Export &amp; Deployment (EN)</li> <li>Fine-Tuning (EN)</li> <li>Troubleshooting (EN)</li> </ul>"},{"location":"fr/fine-tuning/","title":"Fine-Tuning","text":"<p>Le projet inclut un script LoRA pour fine-tuning instruction de modeles CausalLM compatibles HF.</p>"},{"location":"fr/fine-tuning/#commande-principale","title":"Commande principale","text":"<pre><code>python scripts/fine_tune_instruction.py \\\n  --model-id GhostPunishR/labcore-llm-50M \\\n  --dataset yahma/alpaca-cleaned \\\n  --dataset-split train \\\n  --output-dir outputs/lora_instruction \\\n  --config configs/bpe_rope_flash/bpe_50M_rope_flash.toml \\\n  --max-samples 20000 \\\n  --epochs 1\n</code></pre>"},{"location":"fr/fine-tuning/#dependances","title":"Dependances","text":"<pre><code>pip install -e \".[torch,hf,finetune]\"\n</code></pre> <p>Ou:</p> <pre><code>pip install -e \".[torch,dev,hf,finetune]\"\n</code></pre>"},{"location":"fr/fine-tuning/#ce-que-fait-le-script","title":"Ce que fait le script","text":"<ul> <li>charge model/tokenizer base</li> <li>detecte modules cibles LoRA</li> <li>normalise les exemples instruction</li> <li>tokenize avec taille sequence fixe</li> <li>entraine via HF <code>Trainer</code></li> <li>sauvegarde adapter + tokenizer</li> </ul>"},{"location":"fr/fine-tuning/#mapping-des-champs-dataset","title":"Mapping des champs dataset","text":"<ul> <li>instruction: <code>instruction</code> / <code>question</code> / <code>prompt</code></li> <li>input: <code>input</code> / <code>context</code></li> <li>output: <code>output</code> / <code>response</code> / <code>answer</code></li> </ul>"},{"location":"fr/fine-tuning/#conseils","title":"Conseils","text":"<ul> <li>commencer avec peu de samples</li> <li>ajuster <code>batch-size</code> et <code>max-seq-len</code> selon VRAM</li> <li>privilegier un dataset propre et homogene</li> </ul>"},{"location":"fr/fine-tuning/#suite","title":"Suite","text":"<p>Voir Reference Config.</p>"},{"location":"fr/getting-started/","title":"Demarrage","text":"<p>Cette page FR donne un demarrage rapide et fiable. Utilisez la page EN pour le detail complet. Prerequis: Python <code>3.11+</code>, <code>pip</code>, et idealement un GPU CUDA.</p>"},{"location":"fr/getting-started/#commandes-rapides","title":"Commandes Rapides","text":"<pre><code>python -m pip install -e \".[torch,dev]\"\npython -c \"import torch; print(torch.cuda.is_available()); print(torch.version.cuda)\"\npython scripts/prepare_data.py --dataset tinyshakespeare --tokenizer char --output-format txt --output-dir data/processed\npython train.py --config configs/base.toml --tokenizer char --max-iters 5000\npython generate.py --checkpoint checkpoints/ckpt_last.pt --meta data/processed/meta.json --tokenizer char --prompt \"To be\"\n</code></pre>"},{"location":"fr/getting-started/#validation-rapide","title":"Validation rapide","text":"<ul> <li><code>checkpoints/ckpt_last.pt</code> existe</li> <li><code>data/processed/meta.json</code> existe</li> <li>la generation retourne du texte non vide</li> </ul>"},{"location":"fr/getting-started/#voir-aussi","title":"Voir aussi","text":"<ul> <li>Getting Started (EN)</li> <li>Data Pipeline (EN)</li> <li>Troubleshooting (EN)</li> </ul>"},{"location":"fr/inference-and-demo/","title":"Inference et Demo","text":"<p>Cette page FR couvre la generation CLI et la demo Gradio avec les chemins standard. Utilisez les memes metadata et checkpoint que pour l'entrainement.</p>"},{"location":"fr/inference-and-demo/#commandes-rapides","title":"Commandes Rapides","text":"<pre><code>python generate.py --checkpoint checkpoints/ckpt_last.pt --meta data/processed/meta.json --tokenizer char --prompt \"To be\"\npython demo_gradio.py --source local --checkpoint checkpoints/ckpt_last.pt --meta data/processed/meta.json --device cpu --port 7860\npython demo_gradio.py --source hf --repo-id GhostPunishR/labcore-llm-50M --device cpu --port 7860\n</code></pre>"},{"location":"fr/inference-and-demo/#reglages-stables-debug","title":"Reglages stables (debug)","text":"<ul> <li><code>temperature = 0.2</code></li> <li><code>top-k = 20</code></li> <li><code>max-new-tokens = 80</code></li> </ul>"},{"location":"fr/inference-and-demo/#voir-aussi","title":"Voir aussi","text":"<ul> <li>Inference &amp; Demo (EN)</li> <li>Export &amp; Deployment (EN)</li> <li>Troubleshooting (EN)</li> </ul>"},{"location":"fr/operations/","title":"Operations","text":"<p>Page centralisant release, securite et depannage.</p>"},{"location":"fr/operations/#securite","title":"Securite","text":"<p>Ne pas publier les vuln dans les issues publiques. Suivre <code>SECURITY.md</code>:</p> <ul> <li>GitHub Security Advisories en priorite</li> <li>details impact + composants + reproduction</li> </ul>"},{"location":"fr/operations/#process-release","title":"Process release","text":"<p>Reference: <code>RELEASE.md</code></p> <ol> <li>CI verte</li> <li>tests locaux ok</li> <li>bump version dans <code>pyproject.toml</code></li> <li>update <code>CHANGELOG.md</code></li> <li>tag release</li> <li>publier GitHub Release</li> </ol>"},{"location":"fr/operations/#support","title":"Support","text":"<p>Reference: <code>SUPPORT.md</code></p> <ul> <li>usage: discussion/issue label <code>support</code></li> <li>bug: template bug</li> <li>feature: template feature request</li> </ul>"},{"location":"fr/operations/#qualite-et-validation","title":"Qualite et validation","text":"<p>Checks locaux principaux:</p> <pre><code>python -m pytest -q\nruff check src scripts tests train.py generate.py demo_gradio.py --select E9,F63,F7,F82\n</code></pre> <p>Workflows CI:</p> <ul> <li><code>.github/workflows/ci.yml</code> (lint + tests)</li> <li><code>.github/workflows/docs.yml</code> (build/deploy docs)</li> </ul>"},{"location":"fr/operations/#guidance-hardware","title":"Guidance hardware","text":"<ul> <li>CPU-only: suffisant pour smoke tests et runs courts.</li> <li>GPU CUDA recommande pour un entrainement pratique.</li> <li>Pour ~8 GB VRAM, commencer avec les familles 50M puis ajuster batch/sequence.</li> </ul>"},{"location":"fr/operations/#depannage-rapide","title":"Depannage rapide","text":""},{"location":"fr/operations/#no-module-named-torch","title":"<code>No module named 'torch'</code>","text":"<pre><code>pip install -e \".[torch,dev]\"\n</code></pre>"},{"location":"fr/operations/#char-tokenizer-requires-vocab-in-metajson","title":"<code>Char tokenizer requires vocab in meta.json</code>","text":"<p>Regenerer metadata avec tokenizer char.</p>"},{"location":"fr/operations/#binary-shards-not-found","title":"<code>Binary shards not found</code>","text":"<pre><code>python scripts/prepare_data.py --dataset wikitext --tokenizer bpe --output-format bin --output-dir data\n</code></pre>"},{"location":"fr/operations/#warning-cuda-fallback","title":"Warning CUDA fallback","text":"<p>Les scripts basculent auto CPU si CUDA indisponible.</p>"},{"location":"fr/operations/#operations-docs","title":"Operations docs","text":"<p>Build local:</p> <pre><code>pip install -r docs/requirements.txt\npython -m mkdocs serve\n</code></pre> <p>Deploiement:</p> <ul> <li>push sur <code>master</code>/<code>main</code></li> <li>workflow docs construit <code>site/</code> puis deploy sur <code>gh-pages</code></li> </ul>"},{"location":"fr/operations/#license","title":"License","text":"<p>Le projet est sous GPL-3.0. Voir LICENSE.</p>"},{"location":"fr/operations/#disclaimer","title":"Disclaimer","text":"<p>Projet destine a l'education et la recherche. Non optimise pour un deploiement production a grande echelle.</p>"},{"location":"fr/training/","title":"Entrainement","text":"<p>Cette page FR resume le lancement d'entrainement et les chemins a respecter. Le preset de reference est <code>configs/base.toml</code> avec <code>--max-iters 5000</code>.</p>"},{"location":"fr/training/#commandes-rapides","title":"Commandes Rapides","text":"<pre><code>python train.py --config configs/base.toml --tokenizer char --max-iters 5000\npython train.py --config configs/base.toml --tokenizer char --max-iters 5000 --device cpu\npython train.py --config configs/base.toml --tokenizer char --max-iters 5000 --device cuda\n</code></pre>"},{"location":"fr/training/#artifacts","title":"Artifacts","text":"<ul> <li>checkpoint: <code>checkpoints/ckpt_last.pt</code></li> <li>log: <code>checkpoints/train_log.json</code></li> <li>metadata <code>txt</code>: <code>data/processed/meta.json</code></li> <li>metadata <code>bin</code>: <code>data/meta.json</code></li> </ul>"},{"location":"fr/training/#voir-aussi","title":"Voir aussi","text":"<ul> <li>Training (EN)</li> <li>Inference &amp; Demo (EN)</li> <li>Troubleshooting (EN)</li> </ul>"},{"location":"fr/troubleshooting/","title":"D\u00e9pannage","text":"<p>Probl\u00e8mes fr\u00e9quents de setup/runtime avec correctifs rapides.</p>"},{"location":"fr/troubleshooting/#installation-pytorch-cpucuda","title":"Installation PyTorch (CPU/CUDA)","text":"<p>Utilisez le s\u00e9lecteur officiel selon OS + version CUDA :</p> <p>https://pytorch.org/get-started/locally/</p> <p>Commande type :</p> <pre><code>pip install -e \".[torch]\"\n</code></pre>"},{"location":"fr/troubleshooting/#cuda-non-detecte-torchcudais_available-false","title":"CUDA Non D\u00e9tect\u00e9 (<code>torch.cuda.is_available() == False</code>)","text":"<p>V\u00e9rifiez :</p> <ul> <li>Build PyTorch compatible avec votre runtime/driver CUDA.</li> <li>Driver NVIDIA install\u00e9 et \u00e0 jour.</li> <li>M\u00eame venv/Python utilis\u00e9 pour installer et ex\u00e9cuter PyTorch.</li> </ul> <p>V\u00e9rification rapide :</p> <pre><code>python -c \"import torch; print(torch.__version__, torch.cuda.is_available())\"\n</code></pre>"},{"location":"fr/troubleshooting/#oom-quoi-reduire-en-premier","title":"OOM: Quoi R\u00e9duire en Premier","text":"<p>En cas d'erreur m\u00e9moire, r\u00e9duire dans cet ordre :</p> <ol> <li><code>training.batch_size</code></li> <li><code>model.block_size</code> (longueur de s\u00e9quence)</li> <li><code>training.gradient_accumulation_steps</code></li> <li>Strat\u00e9gie de pr\u00e9cision (si vous ajoutez mixed precision)</li> </ol> <p>Si n\u00e9cessaire, utiliser un preset de taille plus petite.</p>"},{"location":"fr/troubleshooting/#emplacement-de-metajson","title":"Emplacement de <code>meta.json</code>","text":"<p>L'emplacement d\u00e9pend du format de sortie :</p> <ul> <li>Pipeline <code>txt</code>: g\u00e9n\u00e9ralement <code>data/processed/meta.json</code></li> <li>Pipeline <code>bin</code>: g\u00e9n\u00e9ralement <code>data/meta.json</code></li> </ul> <p>Si <code>train.py</code> ne trouve pas la metadata, v\u00e9rifier <code>processed_dir</code> dans le TOML et le dossier de sortie r\u00e9el de <code>prepare_data.py</code>.</p>"},{"location":"fr/troubleshooting/#flashattention-non-disponible","title":"FlashAttention Non Disponible","text":"<p>LabCore applique un fallback automatiquement :</p> <ul> <li>Priorit\u00e9: kernel FlashAttention (si dispo)</li> <li>Fallback: PyTorch SDPA</li> <li>Dernier chemin: attention causale standard</li> </ul> <p>Vous pouvez garder <code>use_flash = true</code>, le fallback g\u00e8re les environnements non support\u00e9s.</p>"},{"location":"fr/troubleshooting/#notes-windows","title":"Notes Windows","text":"<ul> <li>Activer le venv avant les commandes.</li> <li>Citer les chemins si espaces.</li> <li>Ex\u00e9cuter depuis la racine du repo pour conserver les chemins relatifs.</li> <li>Si execution policy bloque les scripts, utiliser <code>python ...</code> directement.</li> </ul>"}]}
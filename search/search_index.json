{"config":{"lang":["en","fr"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"LabCore LLM","text":"<p>This documentation is the operational guide for running LabCore end to end: data preparation, training, inference, and export. English pages in <code>docs/</code> are the source of truth, and French pages in <code>docs/fr/</code> are concise mirrors.</p> <p> </p>"},{"location":"#reference-preset-used-across-docs","title":"Reference Preset Used Across Docs","text":"<p>All examples are standardized on this reference setup:</p> <ul> <li>Dataset: <code>tinyshakespeare</code></li> <li>Tokenizer: <code>char</code></li> <li><code>CONFIG_EXAMPLE = configs/base.toml</code></li> <li>Canonical training override: <code>--max-iters 5000</code></li> <li><code>CHECKPOINT = checkpoints/ckpt_last.pt</code></li> <li><code>META_TXT = data/processed/meta.json</code></li> <li><code>META_BIN = data/meta.json</code></li> </ul> <p>Tip</p> <p>Keep these values unchanged for your first full run. Most failures come from checkpoint and metadata path mismatches.</p>"},{"location":"#quick-install","title":"Quick Install","text":"<pre><code>python -m pip install -e \".[torch,dev]\"\n</code></pre> <p>For Hugging Face export and demo UI:</p> <pre><code>python -m pip install -e \".[torch,hf,demo]\"\n</code></pre>"},{"location":"#quick-start-commands","title":"Quick Start Commands","text":"<pre><code>python scripts/prepare_data.py --dataset tinyshakespeare --tokenizer char --output-format txt --output-dir data/processed\npython train.py --config configs/base.toml --tokenizer char --max-iters 5000\npython generate.py --checkpoint checkpoints/ckpt_last.pt --meta data/processed/meta.json --tokenizer char --prompt \"To be\"\n</code></pre> <p>Expected artifacts:</p> <ul> <li><code>checkpoints/ckpt_last.pt</code></li> <li><code>checkpoints/train_log.json</code></li> <li><code>data/processed/meta.json</code></li> </ul>"},{"location":"#end-to-end-flow","title":"End-to-End Flow","text":"<pre><code>prepare_data.py -&gt; train.py -&gt; generate.py/demo_gradio.py -&gt; export_hf.py -&gt; quantize_gguf.py\n</code></pre>"},{"location":"#documentation-map","title":"Documentation Map","text":""},{"location":"#guides","title":"Guides","text":"<ul> <li>Getting Started: environment setup and a reproducible first run.</li> <li>Data Pipeline: build <code>txt</code> or <code>bin</code> datasets and metadata.</li> <li>Training: run training, checkpointing, and format selection.</li> <li>Inference &amp; Demo: CLI generation and Gradio demo.</li> <li>Fine-Tuning: LoRA instruction tuning workflow.</li> <li>Export &amp; Deployment: HF export and GGUF conversion.</li> </ul>"},{"location":"#reference","title":"Reference","text":"<ul> <li>Configuration: complete TOML key reference.</li> <li>Operations: artifacts, release flow, and operational checks.</li> <li>Troubleshooting: anchored fixes for common failures.</li> <li>Benchmarks: benchmark template and reporting method.</li> </ul>"},{"location":"#development","title":"Development","text":"<ul> <li>Developer Guide: contributor workflow and validation commands.</li> </ul>"},{"location":"#next-related","title":"Next / Related","text":"<ul> <li>Getting Started</li> <li>Data Pipeline</li> <li>Training</li> </ul>"},{"location":"benchmarks/","title":"Benchmarks","text":"<p>Use the inference benchmark script to measure reproducible <code>tokens/s</code> and peak VRAM. The script supports both local checkpoints and Hugging Face repos and can export JSON + Markdown outputs.</p>"},{"location":"benchmarks/#inference-benchmark-script","title":"Inference Benchmark Script","text":"<pre><code>python scripts/benchmark_infer.py --help\n</code></pre> <p>Default runtime is intentionally short (warmup + 3 measured runs).</p>"},{"location":"benchmarks/#local-example","title":"Local Example","text":"<pre><code>python scripts/benchmark_infer.py \\\n  --source local \\\n  --checkpoint checkpoints/ckpt_last.pt \\\n  --meta data/processed/meta.json \\\n  --tokenizer char \\\n  --config configs/base.toml \\\n  --device cpu \\\n  --json-out outputs/bench_infer.json \\\n  --md-out outputs/bench_infer.md\n</code></pre>"},{"location":"benchmarks/#hugging-face-example","title":"Hugging Face Example","text":"<pre><code>python scripts/benchmark_infer.py \\\n  --source hf \\\n  --repo-id LabCoreAI/&lt;id&gt; \\\n  --config configs/base.toml \\\n  --device cuda \\\n  --json-out outputs/bench_infer_hf.json \\\n  --md-out outputs/bench_infer_hf.md\n</code></pre>"},{"location":"benchmarks/#what-is-measured","title":"What Is Measured","text":"<ul> <li>Warmup generation (<code>--warmup-tokens</code>, not counted in final throughput).</li> <li>Measured generation (<code>--gen-tokens</code>) repeated <code>--iters</code> times.</li> <li>Throughput summary: <code>mean</code>, <code>min</code>, <code>max</code> tokens/sec.</li> <li>Peak VRAM (<code>torch.cuda.max_memory_allocated</code>) when CUDA is used.</li> </ul> <p>Reproducibility settings are read from <code>[generation]</code> when <code>--config</code> is provided:</p> <ul> <li><code>seed</code></li> <li><code>deterministic</code></li> <li>sampling settings (<code>temperature</code>, <code>top_k</code>, <code>top_p</code>, <code>repetition_penalty</code>)</li> <li><code>use_kv_cache</code> (unless overridden by CLI flag)</li> </ul>"},{"location":"benchmarks/#json-output-schema-summary","title":"JSON Output Schema (Summary)","text":"<pre><code>{\n  \"timestamp\": \"...\",\n  \"commit\": \"...\",\n  \"platform\": {\"os\": \"...\", \"python\": \"...\"},\n  \"torch\": {\"version\": \"...\", \"cuda\": \"...\"},\n  \"device\": {\"type\": \"cpu|cuda\", \"name\": \"...\"},\n  \"model\": {\"source\": \"local|hf\", \"params_m\": 0.0, \"block_size\": 0, \"n_layer\": 0, \"n_head\": 0, \"n_embd\": 0},\n  \"generation\": {\"prompt\": \"...\", \"gen_tokens\": 256, \"temperature\": 0.9, \"top_k\": 40, \"top_p\": 1.0, \"repetition_penalty\": 1.0, \"use_kv_cache\": true},\n  \"results\": {\"iters\": 3, \"tokens_per_sec\": {\"mean\": 0.0, \"min\": 0.0, \"max\": 0.0}, \"vram_peak_mib\": null}\n}\n</code></pre>"},{"location":"benchmarks/#community-results","title":"Community Results","text":"<p>Paste the generated Markdown row (from <code>--md-out</code> or terminal output) into this table. Attach the JSON output in the PR description if available.</p> Device Source Model size (params M) KV-cache gen_tokens mean tok/s peak VRAM MiB your result local/hf 0.000 on/off 256 0.00 N/A or value"},{"location":"configuration-reference/","title":"Configuration Reference","text":"<p>Use this page as the single source of truth for <code>train.py</code> TOML keys and defaults. Prerequisite: familiarity with the reference preset (<code>configs/base.toml</code>).</p>"},{"location":"configuration-reference/#canonical-values-used-in-guides","title":"Canonical Values Used in Guides","text":"<ul> <li><code>CONFIG_EXAMPLE = configs/base.toml</code></li> <li><code>CHECKPOINT = checkpoints/ckpt_last.pt</code></li> <li><code>META_TXT = data/processed/meta.json</code></li> <li><code>META_BIN = data/meta.json</code></li> </ul>"},{"location":"configuration-reference/#section-and-key-reference","title":"Section and Key Reference","text":""},{"location":"configuration-reference/#general","title":"<code>[general]</code>","text":"Key Type Typical value Notes <code>run_name</code> string <code>\"tiny_char_baseline\"</code> Informational run label. <code>seed</code> int <code>1337</code> Optional random seed field. <code>tokenizer</code> string <code>\"char\"</code> or <code>\"bpe\"</code> Default is <code>\"char\"</code> if omitted."},{"location":"configuration-reference/#data","title":"<code>[data]</code>","text":"Key Type Typical value Notes <code>dataset</code> string <code>\"tinyshakespeare\"</code> Metadata label. <code>processed_dir</code> string path <code>\"data/processed\"</code> Default is <code>\"data/processed\"</code> if omitted."},{"location":"configuration-reference/#model","title":"<code>[model]</code>","text":"Key Type Typical value Notes <code>vocab_size</code> int <code>65</code> (char) / <code>50257</code> (bpe) Can be inferred from metadata/tokenizer when omitted. <code>block_size</code> int <code>512</code> Sequence length. <code>n_layer</code> int <code>6</code> Transformer depth. <code>n_head</code> int <code>8</code> Attention heads. <code>n_embd</code> int <code>256</code> Embedding size. <code>dropout</code> float <code>0.1</code> Dropout rate. <code>bias</code> bool <code>true</code> Linear layer bias toggle. <code>use_rope</code> bool <code>false</code>/<code>true</code> Default <code>false</code>. <code>use_flash</code> bool <code>false</code>/<code>true</code> Default <code>false</code>."},{"location":"configuration-reference/#optimizer","title":"<code>[optimizer]</code>","text":"Key Type Typical value Notes <code>learning_rate</code> float <code>3e-4</code> Falls back to <code>[training]</code> if missing. <code>weight_decay</code> float <code>0.01</code> Falls back to <code>[training]</code> if missing. <code>beta1</code> float <code>0.9</code> AdamW beta1. <code>beta2</code> float <code>0.95</code> AdamW beta2. <code>grad_clip</code> float <code>1.0</code> Falls back to <code>[training]</code> if missing."},{"location":"configuration-reference/#training","title":"<code>[training]</code>","text":"Key Type Typical value Notes <code>batch_size</code> int <code>8</code> Micro-batch size per step. <code>gradient_accumulation_steps</code> int <code>1</code> to <code>8</code> Effective batch multiplier. <code>max_iters</code> int <code>5000</code> Docs reference value. <code>warmup_iters</code> int <code>0</code> to <code>500</code> LR warmup steps. <code>lr_decay_iters</code> int <code>5000</code> Usually align with <code>max_iters</code>. <code>min_lr</code> float <code>3e-5</code> Cosine LR floor. <code>eval_interval</code> int <code>200</code> Eval/checkpoint cadence. <code>eval_iters</code> int <code>50</code> Validation batches per eval. <code>log_interval</code> int <code>20</code> Console logging cadence. <code>save_interval</code> int <code>200</code> or <code>500</code> Optional extra save cadence. <code>device</code> string <code>\"cuda\"</code> or <code>\"cpu\"</code> CLI <code>--device</code> overrides this. <code>checkpoint_dir</code> string path <code>\"checkpoints\"</code> Produces <code>ckpt_last.pt</code> and <code>train_log.json</code>. <code>data_format</code> string <code>\"txt\"</code> or <code>\"bin\"</code> Default is <code>\"txt\"</code> if omitted."},{"location":"configuration-reference/#generation","title":"<code>[generation]</code>","text":"Key Type Typical value Notes <code>max_new_tokens</code> int <code>200</code> Useful for shared defaults in docs. <code>temperature</code> float <code>0.6</code> to <code>0.9</code> Sampling randomness. <code>top_k</code> int <code>40</code> to <code>50</code> Sampling truncation."},{"location":"configuration-reference/#loader-defaults-applied-automatically","title":"Loader Defaults Applied Automatically","text":"<p><code>load_config</code> guarantees:</p> <ul> <li><code>general.tokenizer = \"char\"</code></li> <li><code>data.processed_dir = \"data/processed\"</code></li> <li><code>model.use_rope = false</code></li> <li><code>model.use_flash = false</code></li> <li><code>training.data_format = \"txt\"</code></li> </ul>"},{"location":"configuration-reference/#example-minimal-small-config","title":"Example: Minimal Small Config","text":"<pre><code>[general]\ntokenizer = \"char\"\n\n[data]\ndataset = \"tinyshakespeare\"\nprocessed_dir = \"data/processed\"\n\n[model]\nblock_size = 256\nn_layer = 4\nn_head = 4\nn_embd = 192\nvocab_size = 65\ndropout = 0.1\nbias = true\nuse_rope = false\nuse_flash = false\n\n[training]\nbatch_size = 8\ngradient_accumulation_steps = 1\nmax_iters = 2000\neval_interval = 200\neval_iters = 50\nlog_interval = 20\ndevice = \"cpu\"\ncheckpoint_dir = \"checkpoints\"\ndata_format = \"txt\"\n</code></pre>"},{"location":"configuration-reference/#example-rtx-4060-starting-point-8gb-validate-locally","title":"Example: RTX 4060 Starting Point (8GB, Validate Locally)","text":"<pre><code>[general]\ntokenizer = \"char\"\n\n[data]\ndataset = \"tinyshakespeare\"\nprocessed_dir = \"data/processed\"\n\n[model]\nblock_size = 512\nn_layer = 6\nn_head = 8\nn_embd = 256\ndropout = 0.1\nbias = true\nuse_rope = false\nuse_flash = false\n\n[optimizer]\nlearning_rate = 3e-4\nweight_decay = 0.01\nbeta1 = 0.9\nbeta2 = 0.95\ngrad_clip = 1.0\n\n[training]\nbatch_size = 8\ngradient_accumulation_steps = 2\nmax_iters = 5000\nwarmup_iters = 200\nlr_decay_iters = 5000\nmin_lr = 3e-5\neval_interval = 200\neval_iters = 50\nlog_interval = 20\nsave_interval = 200\ndevice = \"cuda\"\ncheckpoint_dir = \"checkpoints\"\ndata_format = \"txt\"\n</code></pre> <p>Note</p> <p>The RTX 4060 block above is a starting template, not a guaranteed benchmark profile. Adjust based on your exact VRAM and driver stack.</p>"},{"location":"configuration-reference/#related","title":"Related","text":"<ul> <li>Training</li> <li>Operations</li> <li>Benchmarks</li> </ul>"},{"location":"data-pipeline/","title":"Data Pipeline","text":"<p>Use this page to prepare training data and metadata in a predictable layout. Prerequisite: dependencies installed from Getting Started.</p>"},{"location":"data-pipeline/#commands","title":"Command(s)","text":"<p>Reference <code>txt</code> pipeline (used by <code>configs/base.toml</code>):</p> <pre><code>python scripts/prepare_data.py \\\n  --dataset tinyshakespeare \\\n  --tokenizer char \\\n  --output-format txt \\\n  --raw-dir data/raw \\\n  --output-dir data/processed \\\n  --val-ratio 0.1\n</code></pre> <p>Alternative <code>bin</code> pipeline:</p> <pre><code>python scripts/prepare_data.py \\\n  --dataset tinyshakespeare \\\n  --tokenizer char \\\n  --output-format bin \\\n  --raw-dir data/raw \\\n  --output-dir data/processed \\\n  --val-ratio 0.1\n</code></pre>"},{"location":"data-pipeline/#output-files-artifacts-produced","title":"Output Files / Artifacts Produced","text":"<p><code>txt</code> format (<code>output-dir = data/processed</code>):</p> <ul> <li><code>data/processed/train.txt</code></li> <li><code>data/processed/val.txt</code></li> <li><code>data/processed/corpus.txt</code></li> <li><code>data/processed/train.npy</code></li> <li><code>data/processed/val.npy</code></li> <li><code>data/processed/meta.json</code> (<code>META_TXT</code>)</li> </ul> <p><code>bin</code> format:</p> <ul> <li><code>data/train.bin</code></li> <li><code>data/val.bin</code></li> <li><code>data/meta.json</code> (<code>META_BIN</code>)</li> </ul> <p>Note</p> <p>For <code>--output-format bin</code>, if <code>--output-dir</code> ends with <code>processed</code>, binary files are written to its parent (<code>data/</code>).</p>"},{"location":"data-pipeline/#format-selection","title":"Format Selection","text":"<ul> <li>Use <code>txt</code> when training with <code>training.data_format = \"txt\"</code> and metadata at <code>data/processed/meta.json</code>.</li> <li>Use <code>bin</code> when training with <code>training.data_format = \"bin\"</code> and metadata at <code>data/meta.json</code>.</li> </ul>"},{"location":"data-pipeline/#common-errors","title":"Common Errors","text":"<ul> <li>Missing binary shards: see Binary shards not found.</li> <li>Wrong metadata path: see Meta path mismatch.</li> <li>Char tokenizer vocab issues: see Char vocab missing.</li> </ul>"},{"location":"data-pipeline/#next-related","title":"Next / Related","text":"<ul> <li>Training</li> <li>Inference &amp; Demo</li> <li>Troubleshooting</li> </ul>"},{"location":"developer-guide/","title":"Developer Guide","text":"<p>This page is for contributors working on the codebase itself.</p>"},{"location":"developer-guide/#repository-map","title":"Repository Map","text":"<pre><code>src/labcore_llm/\n  config/      # TOML loader and defaults\n  data/        # dataset abstractions\n  model/       # GPT model implementation\n  tokenizer/   # char + BPE tokenizers\n  trainer/     # training loop, scheduler, checkpointing\n\nscripts/       # data prep, export, quantize, fine-tune helpers\nconfigs/       # TOML presets\ntests/         # unit tests\n</code></pre>"},{"location":"developer-guide/#local-dev-environment","title":"Local Dev Environment","text":"<pre><code>python -m venv .venv\n## PowerShell\n.\\.venv\\Scripts\\Activate.ps1\npython -m pip install --upgrade pip\npython -m pip install -e \".[torch,dev]\"\n</code></pre>"},{"location":"developer-guide/#validation-commands","title":"Validation Commands","text":"<p>Run tests:</p> <pre><code>python -m pytest -q\n</code></pre> <p>Run lint rules aligned with CI:</p> <pre><code>ruff check src scripts tests train.py generate.py demo_gradio.py --select E9,F63,F7,F82\n</code></pre>"},{"location":"developer-guide/#ci-workflows","title":"CI Workflows","text":"<ul> <li><code>.github/workflows/ci.yml</code>: lint + tests</li> <li><code>.github/workflows/docs.yml</code>: MkDocs build and deploy</li> </ul>"},{"location":"developer-guide/#contribution-quality-bar","title":"Contribution Quality Bar","text":"<ul> <li>Keep commits focused and atomic.</li> <li>Update docs for behavior/CLI changes.</li> <li>Add tests for bug fixes and new logic.</li> <li>Do not commit large data/model artifacts.</li> </ul>"},{"location":"developer-guide/#packaging-notes","title":"Packaging Notes","text":"<ul> <li>Project uses <code>src/</code> layout with setuptools.</li> <li>Optional dependency groups are defined in <code>pyproject.toml</code>.</li> <li>Entry scripts are plain Python files, not console-script wrappers.</li> </ul>"},{"location":"export-and-deployment/","title":"Export and Deployment","text":"<p>Use this page to package a local checkpoint for Hugging Face and optional GGUF deployment. Prerequisites: a valid checkpoint (<code>checkpoints/ckpt_last.pt</code>) and matching metadata (<code>data/processed/meta.json</code> or <code>data/meta.json</code>).</p>"},{"location":"export-and-deployment/#hf-export-vs-gguf-export","title":"HF Export vs GGUF Export","text":"<ul> <li>HF export creates standard model artifacts for Hugging Face workflows.</li> <li>GGUF conversion creates quantized files for <code>llama.cpp</code>-style runtimes.</li> </ul>"},{"location":"export-and-deployment/#commands","title":"Command(s)","text":"<p>Export local checkpoint to HF layout:</p> <pre><code>python scripts/export_hf.py \\\n  --checkpoint checkpoints/ckpt_last.pt \\\n  --meta data/processed/meta.json \\\n  --output-dir outputs/hf_export\n</code></pre> <p>Push exported folder to HF Hub:</p> <pre><code>python scripts/export_hf.py \\\n  --checkpoint checkpoints/ckpt_last.pt \\\n  --meta data/processed/meta.json \\\n  --output-dir outputs/hf_export \\\n  --push \\\n  --repo-id GhostPunishR/labcore-llm-50M\n</code></pre> <p>Convert HF export to GGUF and quantize:</p> <pre><code>python scripts/quantize_gguf.py \\\n  --hf-dir outputs/hf_export \\\n  --llama-cpp-dir third_party/llama.cpp \\\n  --output-dir outputs/gguf \\\n  --quant-type Q4_K_M\n</code></pre>"},{"location":"export-and-deployment/#output-files-artifacts-produced","title":"Output Files / Artifacts Produced","text":"<p><code>outputs/hf_export/</code>:</p> <ul> <li><code>model.safetensors</code></li> <li><code>config.json</code></li> <li><code>tokenizer.json</code></li> <li><code>README.md</code></li> </ul> <p><code>outputs/gguf/</code>:</p> <ul> <li><code>labcore-50m-f16.gguf</code></li> <li><code>labcore-50m-q4_k_m.gguf</code> (or <code>q5_k_m</code> / both when <code>--quant-type all</code>)</li> </ul> <p>Warning</p> <p>GGUF conversion requires a valid <code>llama.cpp</code> checkout with conversion script and quantizer binary available.</p>"},{"location":"export-and-deployment/#common-errors","title":"Common Errors","text":"<ul> <li>Metadata mismatch (<code>txt</code> vs <code>bin</code>): see Meta path mismatch.</li> <li>Missing <code>huggingface_hub</code>/<code>safetensors</code>: see Torch not installed.</li> <li>Missing llama.cpp tools: see Windows path and policy issues.</li> </ul>"},{"location":"export-and-deployment/#next-related","title":"Next / Related","text":"<ul> <li>Fine-Tuning</li> <li>Operations</li> <li>Troubleshooting</li> </ul>"},{"location":"fine-tuning/","title":"Fine-Tuning","text":"<p>Use this page for LoRA instruction tuning on HF-compatible CausalLM checkpoints. Prerequisites: HF + finetune dependencies and an accessible base model.</p>"},{"location":"fine-tuning/#commands","title":"Command(s)","text":"<pre><code>python scripts/fine_tune_instruction.py \\\n  --model-id GhostPunishR/labcore-llm-50M \\\n  --dataset yahma/alpaca-cleaned \\\n  --dataset-split train \\\n  --output-dir outputs/lora_instruction \\\n  --config configs/bpe_rope_flash/bpe_50M_rope_flash.toml \\\n  --max-samples 20000 \\\n  --epochs 1\n</code></pre> <p>Dependencies:</p> <pre><code>python -m pip install -e \".[torch,hf,finetune]\"\n</code></pre>"},{"location":"fine-tuning/#output-files-artifacts-produced","title":"Output Files / Artifacts Produced","text":"<ul> <li><code>outputs/lora_instruction/</code> (LoRA adapter + tokenizer files)</li> </ul>"},{"location":"fine-tuning/#dataset-mapping","title":"Dataset Mapping","text":"<p>The script accepts common field aliases:</p> <ul> <li>Instruction: <code>instruction</code>, <code>question</code>, <code>prompt</code></li> <li>Input: <code>input</code>, <code>context</code></li> <li>Output: <code>output</code>, <code>response</code>, <code>answer</code></li> </ul>"},{"location":"fine-tuning/#common-errors","title":"Common Errors","text":"<ul> <li>Missing HF dependencies: see Torch not installed.</li> <li>OOM during fine-tuning: see Out of memory.</li> <li>Wrong base model/tokenizer expectations: verify config and model compatibility before launching.</li> </ul> <p>Note</p> <p>Fine-tuning uses the HF trainer stack and writes to <code>outputs/</code> rather than <code>checkpoints/</code>.</p>"},{"location":"fine-tuning/#next-related","title":"Next / Related","text":"<ul> <li>Configuration Reference</li> <li>Export &amp; Deployment</li> <li>Operations</li> </ul>"},{"location":"getting-started/","title":"Getting Started","text":"<p>Use this page to get a reproducible first run in about 5 minutes and confirm your environment is healthy. Prerequisites: Python <code>3.11+</code>, <code>pip</code>, and optional CUDA GPU.</p>"},{"location":"getting-started/#commands","title":"Command(s)","text":"<p>Install dependencies:</p> <pre><code>python -m pip install -e \".[torch,dev]\"\n</code></pre> <p>Check CUDA visibility:</p> <pre><code>python -c \"import torch; print(torch.cuda.is_available()); print(torch.version.cuda)\"\n</code></pre> <p>Quick Start (reference preset: tinyshakespeare + char + <code>configs/base.toml</code>):</p> <pre><code>python scripts/prepare_data.py --dataset tinyshakespeare --tokenizer char --output-format txt --output-dir data/processed\npython train.py --config configs/base.toml --tokenizer char --max-iters 5000\npython generate.py --checkpoint checkpoints/ckpt_last.pt --meta data/processed/meta.json --tokenizer char --prompt \"To be\"\n</code></pre> <p>Tip</p> <p>For a strict 5-minute smoke test, start the training command, wait for the first eval/checkpoint output, then stop and run generation.</p>"},{"location":"getting-started/#output-files-artifacts-produced","title":"Output Files / Artifacts Produced","text":"<ul> <li><code>data/processed/meta.json</code></li> <li><code>data/processed/train.txt</code>, <code>data/processed/val.txt</code></li> <li><code>checkpoints/ckpt_last.pt</code></li> <li><code>checkpoints/train_log.json</code></li> </ul>"},{"location":"getting-started/#success-checklist","title":"Success Checklist","text":"<ul> <li>Training logs show at least two <code>train_loss</code> lines and the value trends down.</li> <li>A checkpoint exists at <code>checkpoints/ckpt_last.pt</code>.</li> <li><code>generate.py</code> returns non-empty text from your prompt.</li> </ul>"},{"location":"getting-started/#common-errors","title":"Common Errors","text":"<ul> <li><code>ModuleNotFoundError: torch</code>: see Torch not installed.</li> <li>CUDA expected but disabled: see CUDA not detected.</li> <li>Metadata mismatch: see Meta path mismatch.</li> </ul> <p>Warning</p> <p>Keep <code>--checkpoint</code> and <code>--meta</code> aligned with the same run. Mixed files from different runs produce misleading results.</p>"},{"location":"getting-started/#next-related","title":"Next / Related","text":"<ul> <li>Data Pipeline</li> <li>Training</li> <li>Troubleshooting</li> </ul>"},{"location":"inference-and-demo/","title":"Inference and Demo","text":"<p>Use this page to run deterministic CLI generation and the local Gradio demo. Prerequisites: a trained checkpoint (<code>checkpoints/ckpt_last.pt</code>) and matching metadata.</p>"},{"location":"inference-and-demo/#commands","title":"Command(s)","text":"<p>CLI generation (reference paths):</p> <pre><code>python generate.py \\\n  --checkpoint checkpoints/ckpt_last.pt \\\n  --meta data/processed/meta.json \\\n  --tokenizer char \\\n  --prompt \"To be\" \\\n  --max-new-tokens 200 \\\n  --temperature 0.9 \\\n  --top-k 40 \\\n  --device cpu\n</code></pre> <p>Gradio demo from local checkpoint:</p> <pre><code>python demo_gradio.py \\\n  --source local \\\n  --checkpoint checkpoints/ckpt_last.pt \\\n  --meta data/processed/meta.json \\\n  --device cpu \\\n  --port 7860\n</code></pre> <p>Gradio demo from Hugging Face:</p> <pre><code>python demo_gradio.py \\\n  --source hf \\\n  --repo-id GhostPunishR/labcore-llm-50M \\\n  --device cpu \\\n  --port 7860\n</code></pre>"},{"location":"inference-and-demo/#stable-generation-settings-debug-mode","title":"Stable Generation Settings (Debug Mode)","text":"<p>Use conservative sampling when debugging reproducibility:</p> <ul> <li><code>temperature = 0.2</code> to reduce randomness</li> <li><code>top-k = 20</code> (or lower)</li> <li><code>max-new-tokens = 80</code> for quick checks</li> </ul> <p>Tip</p> <p>If output quality suddenly drops, first verify that <code>--meta</code> belongs to the same tokenizer/checkpoint run.</p>"},{"location":"inference-and-demo/#sampling-controls","title":"Sampling Controls","text":"<ul> <li><code>temperature</code>: scales logits before sampling.</li> <li><code>top_k</code>: keeps only the <code>k</code> most likely tokens.</li> <li><code>top_p</code>: nucleus sampling cutoff (<code>1.0</code> disables it).</li> <li><code>repetition_penalty</code>: penalizes already generated tokens (<code>1.0</code> disables it).</li> <li><code>use_kv_cache</code>: enables KV cache during decoding.</li> <li><code>stream</code>: enables token-by-token output streaming.</li> </ul> <p><code>top_p</code> and <code>repetition_penalty</code> are read from <code>[generation]</code> when using <code>generate.py --config ...</code>.</p> <p>RTX 4060 stable example:</p> <pre><code>[generation]\ntemperature = 0.6\ntop_k = 50\ntop_p = 0.9\nrepetition_penalty = 1.1\nuse_kv_cache = true\nstream = true\nsystem_prompt = \"You are LabCore LLM.\"\nmax_history_turns = 6\n</code></pre>"},{"location":"inference-and-demo/#reproducible-generation","title":"Reproducible Generation","text":"<ul> <li><code>seed</code>: fixes Python/NumPy/Torch RNGs for repeatable sampling.</li> <li><code>deterministic</code>: enables deterministic Torch algorithms (<code>warn_only=True</code>) and deterministic cuDNN settings.</li> </ul> <pre><code>[generation]\ntemperature = 0.6\ntop_k = 50\ntop_p = 0.9\nrepetition_penalty = 1.1\nuse_kv_cache = true\nstream = true\nsystem_prompt = \"You are LabCore LLM.\"\nmax_history_turns = 6\nseed = 1337\ndeterministic = true\n</code></pre>"},{"location":"inference-and-demo/#kv-cache-streaming-and-chat","title":"KV Cache, Streaming, and Chat","text":"<ul> <li>KV cache speeds up generation by reusing past attention keys/values instead of recomputing the whole context.</li> <li>Streaming updates the demo output incrementally as each token is sampled.</li> <li>Multi-turn chat builds prompts with simple text markers:</li> <li><code>&lt;|system|&gt;</code></li> <li><code>&lt;|user|&gt;</code></li> <li><code>&lt;|assistant|&gt;</code></li> <li><code>max_history_turns</code> keeps only the latest turns to bound prompt length.</li> </ul> <pre><code>[generation]\nuse_kv_cache = true\nstream = true\nsystem_prompt = \"You are LabCore LLM.\"\nmax_history_turns = 6\n</code></pre>"},{"location":"inference-and-demo/#output-files-artifacts-produced","title":"Output Files / Artifacts Produced","text":"<ul> <li>CLI: generated text in terminal output</li> <li>Demo: generated text in Gradio UI</li> <li>No new model files unless you explicitly export</li> </ul>"},{"location":"inference-and-demo/#common-errors","title":"Common Errors","text":"<ul> <li>Char tokenizer metadata missing: see Char vocab missing.</li> <li>Metadata path mismatch (<code>txt</code> vs <code>bin</code>): see Meta path mismatch.</li> <li>CUDA fallback behavior: see CUDA not detected.</li> </ul>"},{"location":"inference-and-demo/#next-related","title":"Next / Related","text":"<ul> <li>Export &amp; Deployment</li> <li>Fine-Tuning</li> <li>Troubleshooting</li> </ul>"},{"location":"operations/","title":"Operations","text":"<p>Use this page for operational folder hygiene, validation checks, and release/security workflows. Prerequisite: project dependencies installed in your active environment.</p>"},{"location":"operations/#artifact-directories","title":"Artifact Directories","text":"<ul> <li><code>data/</code>: prepared datasets, including <code>data/meta.json</code> for <code>bin</code> runs.</li> <li><code>checkpoints/</code>: training outputs (<code>ckpt_last.pt</code>, <code>train_log.json</code>).</li> <li><code>outputs/</code>: exported artifacts (<code>hf_export/</code>, <code>gguf/</code>, fine-tuning outputs).</li> <li><code>runs/</code>: optional experiment logs or external tracker exports (not auto-created by core scripts).</li> </ul> <p>Note</p> <p>The core training scripts write to <code>checkpoints/</code> and <code>outputs/</code>. Keep those directories versioned in your run notes, but do not commit large artifacts.</p>"},{"location":"operations/#commands","title":"Command(s)","text":"<p>Local quality checks:</p> <pre><code>python -m pytest -q\nruff check src scripts tests train.py generate.py demo_gradio.py --select E9,F63,F7,F82\n</code></pre> <p>Docs build checks:</p> <pre><code>python -m pip install -r docs/requirements.txt\npython -m mkdocs build\n</code></pre>"},{"location":"operations/#output-files-artifacts-produced","title":"Output Files / Artifacts Produced","text":"<ul> <li>Test and lint logs in terminal output</li> <li>Built docs in <code>site/</code> after <code>mkdocs build</code></li> <li>CI pipelines: <code>.github/workflows/ci.yml</code> and <code>.github/workflows/docs.yml</code></li> </ul>"},{"location":"operations/#security-and-release","title":"Security and Release","text":"<p>Security reporting (see <code>SECURITY.md</code>):</p> <ul> <li>Use GitHub Security Advisories for vulnerabilities</li> <li>Include impact, affected components, and reproduction steps</li> </ul> <p>Release flow (see <code>RELEASE.md</code>):</p> <ol> <li>Confirm CI is green.</li> <li>Run local validation commands.</li> <li>Update version and changelog.</li> <li>Tag and publish the release.</li> </ol>"},{"location":"operations/#common-errors","title":"Common Errors","text":"<ul> <li>Missing dependencies: see Torch not installed.</li> <li>Metadata and path confusion: see Meta path mismatch.</li> <li>CUDA expected but unavailable: see CUDA not detected.</li> </ul>"},{"location":"operations/#related","title":"Related","text":"<ul> <li>Troubleshooting</li> <li>Benchmarks</li> <li>Developer Guide</li> </ul>"},{"location":"training/","title":"Training","text":"<p>Use this page to launch, monitor, and checkpoint training runs with consistent settings. Prerequisites: prepared dataset and metadata from Data Pipeline.</p>"},{"location":"training/#commands","title":"Command(s)","text":"<p>Reference training command:</p> <pre><code>python train.py --config configs/base.toml --tokenizer char --max-iters 5000\n</code></pre> <p>Device override examples:</p> <pre><code>python train.py --config configs/base.toml --tokenizer char --max-iters 5000 --device cpu\npython train.py --config configs/base.toml --tokenizer char --max-iters 5000 --device cuda\n</code></pre> <p><code>train.py</code> flags:</p> <ul> <li><code>--config</code>: TOML preset path (<code>CONFIG_EXAMPLE = configs/base.toml</code>)</li> <li><code>--max-iters</code>: runtime override for total iterations</li> <li><code>--device</code>: <code>cpu</code> or <code>cuda</code></li> <li><code>--tokenizer</code>: <code>char</code> or <code>bpe</code></li> </ul>"},{"location":"training/#precision-and-gradient-accumulation","title":"Precision and Gradient Accumulation","text":"<p>Configure these in <code>[training]</code>:</p> <ul> <li><code>grad_accum_steps</code>: gradient accumulation factor (default <code>1</code>)</li> <li><code>precision</code>: <code>fp32</code> (default), <code>fp16</code>, or <code>bf16</code></li> </ul> <p><code>effective_batch_size = batch_size * grad_accum_steps</code></p> <p>Mixed precision is enabled only when <code>device = \"cuda\"</code> and <code>precision != \"fp32\"</code>. On CPU, training falls back to <code>fp32</code>.</p> <p>RTX 4060 example:</p> <pre><code>[training]\nbatch_size = 8\ngrad_accum_steps = 4\nprecision = \"fp16\"\n</code></pre>"},{"location":"training/#best-checkpoint-early-stopping","title":"Best Checkpoint &amp; Early Stopping","text":"<ul> <li><code>save_best</code> saves <code>checkpoints/ckpt_best.pt</code> whenever validation loss improves by at least <code>early_stopping_min_delta</code>.</li> <li><code>early_stopping</code> is disabled by default.</li> <li><code>early_stopping_patience</code> counts evaluation rounds without sufficient validation improvement.</li> <li><code>early_stopping_min_delta</code> defines the minimum improvement threshold on <code>val_loss</code>.</li> </ul> <pre><code>[training]\nearly_stopping = true\nearly_stopping_patience = 3\nearly_stopping_min_delta = 0.001\nsave_best = true\n</code></pre>"},{"location":"training/#data_format-and-metadata-mapping","title":"<code>data_format</code> and Metadata Mapping","text":"Training mode Config value Data artifacts expected Metadata path Text pipeline <code>training.data_format = \"txt\"</code> <code>data/processed/train.txt</code> + <code>data/processed/val.txt</code> (or <code>.npy</code>) <code>data/processed/meta.json</code> (<code>META_TXT</code>) Binary pipeline <code>training.data_format = \"bin\"</code> <code>data/train.bin</code> + <code>data/val.bin</code> <code>data/meta.json</code> (<code>META_BIN</code>) <p>Note</p> <p>For binary mode, if <code>data.processed_dir</code> points to <code>data/processed</code>, <code>train.py</code> automatically checks the parent directory (<code>data/</code>) for <code>train.bin</code> and <code>val.bin</code>.</p>"},{"location":"training/#checkpointing-and-resume-behavior","title":"Checkpointing and Resume Behavior","text":"<p>Produced during training:</p> <ul> <li><code>checkpoints/ckpt_last.pt</code> (<code>CHECKPOINT</code>)</li> <li><code>checkpoints/ckpt_best.pt</code> (when <code>save_best = true</code>)</li> <li><code>checkpoints/train_log.json</code></li> </ul> <p>Warning</p> <p>Native resume-from-checkpoint is not implemented in <code>train.py</code> yet. <code>ckpt_last.pt</code> is for inference/export compatibility and state inspection, not automatic continuation.</p>"},{"location":"training/#common-errors","title":"Common Errors","text":"<ul> <li>Binary shards missing: see Binary shards not found.</li> <li>Metadata path mismatch: see Meta path mismatch.</li> <li>Vocab inference failure: see Char vocab missing.</li> <li>CUDA fallback warning: see CUDA not detected.</li> </ul>"},{"location":"training/#next-related","title":"Next / Related","text":"<ul> <li>Inference &amp; Demo</li> <li>Export &amp; Deployment</li> <li>Troubleshooting</li> </ul>"},{"location":"troubleshooting/","title":"Troubleshooting","text":"<p>Use this page for fast diagnosis of common setup, data, and runtime failures. All anchors below are referenced from guide pages.</p>"},{"location":"troubleshooting/#setup","title":"Setup","text":""},{"location":"troubleshooting/#torch-not-installed","title":"Torch not installed","text":"<p>Symptoms: <code>ModuleNotFoundError: torch</code>, or scripts fail on import.</p> <p>Fix:</p> <pre><code>python -m pip install -e \".[torch,dev]\"\n</code></pre> <p>Use the official selector if you need a specific CUDA build: https://pytorch.org/get-started/locally/</p>"},{"location":"troubleshooting/#cuda-not-detected","title":"CUDA not detected","text":"<p>Symptoms: <code>torch.cuda.is_available()</code> returns <code>False</code>, or scripts print CPU fallback warnings.</p> <p>Quick check:</p> <pre><code>python -c \"import torch; print(torch.cuda.is_available()); print(torch.version.cuda)\"\n</code></pre> <p>Checks:</p> <ul> <li>NVIDIA driver installed and up to date</li> <li>Correct PyTorch build for your CUDA runtime</li> <li>Same Python environment used for install and execution</li> </ul>"},{"location":"troubleshooting/#data-and-metadata","title":"Data and Metadata","text":""},{"location":"troubleshooting/#meta-path-mismatch","title":"Meta path mismatch","text":"<p>Symptoms: generation/training behaves incorrectly or cannot load tokenizer metadata.</p> <p>Expected mapping:</p> <ul> <li><code>txt</code> pipeline -&gt; <code>data/processed/meta.json</code></li> <li><code>bin</code> pipeline -&gt; <code>data/meta.json</code></li> </ul>"},{"location":"troubleshooting/#binary-shards-not-found","title":"Binary shards not found","text":"<p>Symptoms: <code>Binary shards not found</code> during <code>training.data_format = \"bin\"</code>.</p> <p>Fix:</p> <pre><code>python scripts/prepare_data.py --dataset tinyshakespeare --tokenizer char --output-format bin --output-dir data/processed\n</code></pre>"},{"location":"troubleshooting/#char-vocab-missing","title":"Char vocab missing","text":"<p>Symptoms: <code>Char tokenizer requires vocab in meta.json</code>.</p> <p>Fix:</p> <pre><code>python scripts/prepare_data.py --dataset tinyshakespeare --tokenizer char --output-format txt --output-dir data/processed\n</code></pre> <p>Then pass the matching <code>--meta data/processed/meta.json</code> to generation/export commands.</p>"},{"location":"troubleshooting/#runtime","title":"Runtime","text":""},{"location":"troubleshooting/#oom-errors","title":"Out of memory","text":"<p>Reduce in this order:</p> <ol> <li><code>training.batch_size</code></li> <li><code>model.block_size</code></li> <li><code>training.gradient_accumulation_steps</code></li> <li>Model size / preset complexity</li> </ol>"},{"location":"troubleshooting/#flashattention-not-available","title":"FlashAttention not available","text":"<p>LabCore falls back automatically:</p> <ul> <li>FlashAttention (preferred, if available)</li> <li>PyTorch SDPA fallback</li> <li>Standard causal attention fallback</li> </ul>"},{"location":"troubleshooting/#windows-path-policy","title":"Windows path and policy issues","text":"<ul> <li>Activate your venv before commands.</li> <li>Run commands from repository root.</li> <li>Quote paths when directories contain spaces.</li> <li>If PowerShell policy blocks scripts, use <code>python ...</code> commands directly.</li> </ul>"},{"location":"fr/","title":"LabCore LLM","text":"<p>Cette documentation est le guide operationnel pour executer LabCore de bout en bout: preparation des donnees, entrainement, inference et export. Les pages EN dans <code>docs/</code> restent la source de verite, et les pages FR dans <code>docs/fr/</code> sont des miroirs complets.</p> <p> </p>"},{"location":"fr/#preset-de-reference-utilise-dans-la-documentation","title":"Preset de reference utilise dans la documentation","text":"<p>Tous les exemples utilisent cette base:</p> <ul> <li>Dataset: <code>tinyshakespeare</code></li> <li>Tokenizer: <code>char</code></li> <li><code>CONFIG_EXAMPLE = configs/base.toml</code></li> <li>Override entrainement standard: <code>--max-iters 5000</code></li> <li><code>CHECKPOINT = checkpoints/ckpt_last.pt</code></li> <li><code>META_TXT = data/processed/meta.json</code></li> <li><code>META_BIN = data/meta.json</code></li> </ul> <p>Tip</p> <p>Gardez ces valeurs pour votre premier run complet. La plupart des erreurs viennent d'un mauvais alignement checkpoint/metadata.</p>"},{"location":"fr/#installation-rapide","title":"Installation rapide","text":"<pre><code>python -m pip install -e \".[torch,dev]\"\n</code></pre> <p>Pour export Hugging Face et interface Gradio:</p> <pre><code>python -m pip install -e \".[torch,hf,demo]\"\n</code></pre>"},{"location":"fr/#commandes-de-demarrage-rapide","title":"Commandes de demarrage rapide","text":"<pre><code>python scripts/prepare_data.py --dataset tinyshakespeare --tokenizer char --output-format txt --output-dir data/processed\npython train.py --config configs/base.toml --tokenizer char --max-iters 5000\npython generate.py --checkpoint checkpoints/ckpt_last.pt --meta data/processed/meta.json --tokenizer char --prompt \"To be\"\n</code></pre> <p>Artifacts attendus:</p> <ul> <li><code>checkpoints/ckpt_last.pt</code></li> <li><code>checkpoints/train_log.json</code></li> <li><code>data/processed/meta.json</code></li> </ul>"},{"location":"fr/#flux-de-bout-en-bout","title":"Flux de bout en bout","text":"<pre><code>prepare_data.py -&gt; train.py -&gt; generate.py/demo_gradio.py -&gt; export_hf.py -&gt; quantize_gguf.py\n</code></pre>"},{"location":"fr/#plan-de-documentation","title":"Plan de documentation","text":""},{"location":"fr/#guides","title":"Guides","text":"<ul> <li>Getting Started: setup environnement et premier run reproductible.</li> <li>Data Pipeline: creation des donnees <code>txt</code> ou <code>bin</code> et metadata.</li> <li>Training: entrainement, checkpointing et mapping des formats.</li> <li>Inference &amp; Demo: generation CLI et demo Gradio.</li> <li>Fine-Tuning: workflow LoRA instruction tuning.</li> <li>Export &amp; Deployment: export HF et conversion GGUF.</li> </ul>"},{"location":"fr/#reference","title":"Reference","text":"<ul> <li>Configuration: reference complete des cles TOML.</li> <li>Operations: hygiene operationnelle, release et checks.</li> <li>Troubleshooting: correctifs rapides avec ancres.</li> <li>Benchmarks: benchmark inference (<code>tok/s</code>, VRAM peak) et reporting.</li> </ul>"},{"location":"fr/#developpement","title":"Developpement","text":"<ul> <li>Developer Guide: workflow contributeur et commandes de validation.</li> </ul>"},{"location":"fr/benchmarks/","title":"Benchmarks","text":"<p>Utilisez le script de benchmark inference pour mesurer de maniere reproductible les <code>tokens/s</code> et le pic VRAM. Le script supporte les checkpoints locaux et les repos Hugging Face, avec export JSON + Markdown.</p>"},{"location":"fr/benchmarks/#inference-benchmark-script","title":"Inference Benchmark Script","text":"<pre><code>python scripts/benchmark_infer.py --help\n</code></pre> <p>Le runtime par defaut reste court (warmup + 3 runs mesures).</p>"},{"location":"fr/benchmarks/#what-is-measured","title":"What Is Measured","text":"<ul> <li>Generation de warmup (<code>--warmup-tokens</code>, non comptee dans le throughput final).</li> <li>Generation mesuree (<code>--gen-tokens</code>) repetee <code>--iters</code> fois.</li> <li>Resume throughput: <code>mean</code>, <code>min</code>, <code>max</code> tokens/sec.</li> <li>VRAM peak (<code>torch.cuda.max_memory_allocated</code>) en execution CUDA.</li> </ul> <p>Les reglages de reproductibilite sont lus dans <code>[generation]</code> quand <code>--config</code> est fourni:</p> <ul> <li><code>seed</code></li> <li><code>deterministic</code></li> <li>reglages sampling (<code>temperature</code>, <code>top_k</code>, <code>top_p</code>, <code>repetition_penalty</code>)</li> <li><code>use_kv_cache</code> (sauf override via flags CLI)</li> </ul>"},{"location":"fr/benchmarks/#community-results","title":"Community Results","text":"<p>Collez la ligne Markdown generee (via <code>--md-out</code> ou sortie terminal) dans ce tableau. Ajoutez le JSON dans la description de PR si disponible.</p> Device Source Model size (params M) KV-cache gen_tokens mean tok/s peak VRAM MiB your result local/hf 0.000 on/off 256 0.00 N/A or value"},{"location":"fr/configuration-reference/","title":"Configuration Reference","text":"<p>Utilisez cette page comme reference unique des cles TOML de <code>train.py</code> et des defaults. Prerequis: connaitre le preset de reference (<code>configs/base.toml</code>).</p>"},{"location":"fr/configuration-reference/#general","title":"<code>[general]</code>","text":"Key Type Valeur typique Notes <code>run_name</code> string <code>\"tiny_char_baseline\"</code> Label informatif de run. <code>seed</code> int <code>1337</code> Champ seed optionnel. <code>tokenizer</code> string <code>\"char\"</code> ou <code>\"bpe\"</code> Default <code>\"char\"</code> si omis."},{"location":"fr/configuration-reference/#data","title":"<code>[data]</code>","text":"Key Type Valeur typique Notes <code>dataset</code> string <code>\"tinyshakespeare\"</code> Label metadata. <code>processed_dir</code> string path <code>\"data/processed\"</code> Default <code>\"data/processed\"</code> si omis."},{"location":"fr/configuration-reference/#model","title":"<code>[model]</code>","text":"Key Type Valeur typique Notes <code>vocab_size</code> int <code>65</code> (char) / <code>50257</code> (bpe) Peut etre infere via metadata/tokenizer si omis. <code>block_size</code> int <code>512</code> Longueur de sequence. <code>n_layer</code> int <code>6</code> Profondeur Transformer. <code>n_head</code> int <code>8</code> Nombre de tetes attention. <code>n_embd</code> int <code>256</code> Taille embeddings. <code>dropout</code> float <code>0.1</code> Taux dropout. <code>bias</code> bool <code>true</code> Active/desactive le bias lineaire. <code>use_rope</code> bool <code>false</code>/<code>true</code> Default <code>false</code>. <code>use_flash</code> bool <code>false</code>/<code>true</code> Default <code>false</code>."},{"location":"fr/configuration-reference/#optimizer","title":"<code>[optimizer]</code>","text":"Key Type Valeur typique Notes <code>learning_rate</code> float <code>3e-4</code> Fallback sur <code>[training]</code> si absent. <code>weight_decay</code> float <code>0.01</code> Fallback sur <code>[training]</code> si absent. <code>beta1</code> float <code>0.9</code> AdamW beta1. <code>beta2</code> float <code>0.95</code> AdamW beta2. <code>grad_clip</code> float <code>1.0</code> Fallback sur <code>[training]</code> si absent."},{"location":"fr/configuration-reference/#training","title":"<code>[training]</code>","text":"Key Type Valeur typique Notes <code>batch_size</code> int <code>8</code> Micro-batch size par step. <code>gradient_accumulation_steps</code> int <code>1</code> a <code>8</code> Multiplicateur batch effectif. <code>max_iters</code> int <code>5000</code> Valeur de reference docs. <code>warmup_iters</code> int <code>0</code> a <code>500</code> Steps de warmup LR. <code>lr_decay_iters</code> int <code>5000</code> Souvent aligne sur <code>max_iters</code>. <code>min_lr</code> float <code>3e-5</code> Floor cosine LR. <code>eval_interval</code> int <code>200</code> Cadence eval/checkpoint. <code>eval_iters</code> int <code>50</code> Batches validation par eval. <code>log_interval</code> int <code>20</code> Cadence logs console. <code>save_interval</code> int <code>200</code> ou <code>500</code> Cadence de sauvegarde supplementaire. <code>device</code> string <code>\"cuda\"</code> ou <code>\"cpu\"</code> Override par CLI <code>--device</code>. <code>checkpoint_dir</code> string path <code>\"checkpoints\"</code> Produit <code>ckpt_last.pt</code> et <code>train_log.json</code>. <code>data_format</code> string <code>\"txt\"</code> ou <code>\"bin\"</code> Default <code>\"txt\"</code> si omis. <code>early_stopping</code> bool <code>false</code> Active l'arret anticipe sur loss de validation. <code>early_stopping_patience</code> int <code>5</code> Nombre d'evaluations sans amelioration avant stop. <code>early_stopping_min_delta</code> float <code>0.0</code> Gain minimal de loss de validation pour compter une amelioration. <code>save_best</code> bool <code>true</code> Sauvegarde <code>ckpt_best.pt</code> sur nouvelle meilleure loss de validation."},{"location":"fr/configuration-reference/#generation","title":"<code>[generation]</code>","text":"Key Type Valeur typique Notes <code>max_new_tokens</code> int <code>200</code> Default pratique pour scripts docs. <code>temperature</code> float <code>0.6</code> a <code>0.9</code> Randomness sampling. <code>top_k</code> int <code>40</code> a <code>50</code> Troncature sampling. <code>top_p</code> float <code>0.9</code> a <code>1.0</code> Nucleus sampling cutoff. <code>repetition_penalty</code> float <code>1.0</code> a <code>1.2</code> Penalite tokens repetes. <code>use_kv_cache</code> bool <code>true</code> Active KV-cache en inference. <code>stream</code> bool <code>true</code> Active streaming token-by-token en demo. <code>system_prompt</code> string <code>\"\"</code> Prompt system du chat minimal. <code>max_history_turns</code> int <code>6</code> Nombre max de tours d'historique chat. <code>seed</code> int or null <code>1337</code> Seed de reproductibilite. <code>deterministic</code> bool <code>false</code> Active algorithmes deterministes PyTorch."},{"location":"fr/configuration-reference/#loader-defaults-applied-automatically","title":"Loader Defaults Applied Automatically","text":"<p><code>load_config</code> garantit:</p> <ul> <li><code>general.tokenizer = \"char\"</code></li> <li><code>data.processed_dir = \"data/processed\"</code></li> <li><code>model.use_rope = false</code></li> <li><code>model.use_flash = false</code></li> <li><code>training.data_format = \"txt\"</code></li> <li><code>training.early_stopping = false</code></li> <li><code>training.early_stopping_patience = 5</code></li> <li><code>training.early_stopping_min_delta = 0.0</code></li> <li><code>training.save_best = true</code></li> <li><code>generation.top_p = 1.0</code></li> <li><code>generation.repetition_penalty = 1.0</code></li> <li><code>generation.use_kv_cache = true</code></li> <li><code>generation.stream = true</code></li> <li><code>generation.system_prompt = \"\"</code></li> <li><code>generation.max_history_turns = 6</code></li> <li><code>generation.seed = null</code></li> <li><code>generation.deterministic = false</code></li> </ul>"},{"location":"fr/configuration-reference/#example-rtx-4060-starting-point-8gb-validate-locally","title":"Example: RTX 4060 Starting Point (8GB, Validate Locally)","text":"<pre><code>[general]\ntokenizer = \"char\"\n\n[data]\ndataset = \"tinyshakespeare\"\nprocessed_dir = \"data/processed\"\n\n[model]\nblock_size = 512\nn_layer = 6\nn_head = 8\nn_embd = 256\ndropout = 0.1\nbias = true\nuse_rope = false\nuse_flash = false\n\n[optimizer]\nlearning_rate = 3e-4\nweight_decay = 0.01\nbeta1 = 0.9\nbeta2 = 0.95\ngrad_clip = 1.0\n\n[training]\nbatch_size = 8\ngradient_accumulation_steps = 2\nmax_iters = 5000\nwarmup_iters = 200\nlr_decay_iters = 5000\nmin_lr = 3e-5\neval_interval = 200\neval_iters = 50\nlog_interval = 20\nsave_interval = 200\ndevice = \"cuda\"\ncheckpoint_dir = \"checkpoints\"\ndata_format = \"txt\"\n</code></pre> <p>Note</p> <p>Le bloc RTX 4060 est un point de depart, pas un profil benchmark garanti. Ajustez selon votre VRAM et stack driver.</p>"},{"location":"fr/data-pipeline/","title":"Data Pipeline","text":"<p>Utilisez cette page pour preparer les donnees et metadata avec une structure de sortie previsible. Prerequis: dependances installees depuis Getting Started.</p>"},{"location":"fr/data-pipeline/#commands","title":"Command(s)","text":"<p>Pipeline <code>txt</code> de reference (utilise par <code>configs/base.toml</code>):</p> <pre><code>python scripts/prepare_data.py \\\n  --dataset tinyshakespeare \\\n  --tokenizer char \\\n  --output-format txt \\\n  --raw-dir data/raw \\\n  --output-dir data/processed \\\n  --val-ratio 0.1\n</code></pre> <p>Pipeline <code>bin</code> alternatif:</p> <pre><code>python scripts/prepare_data.py \\\n  --dataset tinyshakespeare \\\n  --tokenizer char \\\n  --output-format bin \\\n  --raw-dir data/raw \\\n  --output-dir data/processed \\\n  --val-ratio 0.1\n</code></pre>"},{"location":"fr/data-pipeline/#output-files-artifacts-produced","title":"Output Files / Artifacts Produced","text":"<p>Format <code>txt</code> (<code>output-dir = data/processed</code>):</p> <ul> <li><code>data/processed/train.txt</code></li> <li><code>data/processed/val.txt</code></li> <li><code>data/processed/corpus.txt</code></li> <li><code>data/processed/train.npy</code></li> <li><code>data/processed/val.npy</code></li> <li><code>data/processed/meta.json</code> (<code>META_TXT</code>)</li> </ul> <p>Format <code>bin</code>:</p> <ul> <li><code>data/train.bin</code></li> <li><code>data/val.bin</code></li> <li><code>data/meta.json</code> (<code>META_BIN</code>)</li> </ul> <p>Note</p> <p>Avec <code>--output-format bin</code>, si <code>--output-dir</code> se termine par <code>processed</code>, les fichiers binaires sont ecrits dans le parent (<code>data/</code>).</p>"},{"location":"fr/data-pipeline/#format-selection","title":"Format Selection","text":"<ul> <li>Utilisez <code>txt</code> avec <code>training.data_format = \"txt\"</code> et metadata <code>data/processed/meta.json</code>.</li> <li>Utilisez <code>bin</code> avec <code>training.data_format = \"bin\"</code> et metadata <code>data/meta.json</code>.</li> </ul>"},{"location":"fr/data-pipeline/#common-errors","title":"Common Errors","text":"<ul> <li>Binaries manquants: voir Binary shards not found.</li> <li>Mauvais chemin metadata: voir Meta path mismatch.</li> <li>Probleme de vocab char: voir Char vocab missing.</li> </ul>"},{"location":"fr/developer-guide/","title":"Developer Guide","text":"<p>Cette page cible les contributeurs qui travaillent sur le code du projet.</p>"},{"location":"fr/developer-guide/#repository-map","title":"Repository Map","text":"<pre><code>src/labcore_llm/\n  config/      # loader TOML et defaults\n  data/        # abstractions dataset\n  model/       # implementation GPT\n  tokenizer/   # tokenizers char + BPE\n  trainer/     # boucle training, scheduler, checkpointing\n\nscripts/       # helpers data prep, export, quantize, fine-tune\nconfigs/       # presets TOML\ntests/         # tests unitaires\n</code></pre>"},{"location":"fr/developer-guide/#validation-commands","title":"Validation Commands","text":"<p>Lancer les tests:</p> <pre><code>python -m pytest -q\n</code></pre> <p>Lancer le lint aligne sur la CI:</p> <pre><code>ruff check src scripts tests train.py generate.py demo_gradio.py --select E9,F63,F7,F82\n</code></pre>"},{"location":"fr/developer-guide/#ci-workflows","title":"CI Workflows","text":"<ul> <li><code>.github/workflows/ci.yml</code>: lint + tests</li> <li><code>.github/workflows/docs.yml</code>: build et deploiement MkDocs</li> </ul>"},{"location":"fr/developer-guide/#contribution-quality-bar","title":"Contribution Quality Bar","text":"<ul> <li>Gardez les commits focalises et atomiques.</li> <li>Mettez a jour la doc lors des changements de comportement/CLI.</li> <li>Ajoutez des tests pour corrections de bugs et nouvelles logiques.</li> <li>Ne commitez pas de gros artifacts data/model.</li> </ul>"},{"location":"fr/developer-guide/#packaging-notes","title":"Packaging Notes","text":"<ul> <li>Le projet utilise un layout <code>src/</code> avec setuptools.</li> <li>Les groupes de dependances optionnelles sont dans <code>pyproject.toml</code>.</li> <li>Les scripts d'entree sont des fichiers Python, pas des wrappers console-script.</li> </ul>"},{"location":"fr/export-and-deployment/","title":"Export and Deployment","text":"<p>Utilisez cette page pour packager un checkpoint local vers Hugging Face puis, optionnellement, vers GGUF. Prerequis: checkpoint valide (<code>checkpoints/ckpt_last.pt</code>) et metadata correspondante (<code>data/processed/meta.json</code> ou <code>data/meta.json</code>).</p>"},{"location":"fr/export-and-deployment/#hf-export-vs-gguf-export","title":"HF Export vs GGUF Export","text":"<ul> <li>L'export HF cree des artifacts standards pour les workflows Hugging Face.</li> <li>La conversion GGUF cree des fichiers quantifies pour runtimes <code>llama.cpp</code>.</li> </ul>"},{"location":"fr/export-and-deployment/#commands","title":"Command(s)","text":"<p>Exporter un checkpoint local au format HF:</p> <pre><code>python scripts/export_hf.py \\\n  --checkpoint checkpoints/ckpt_last.pt \\\n  --meta data/processed/meta.json \\\n  --output-dir outputs/hf_export\n</code></pre> <p>Pousser le dossier exporte vers HF Hub:</p> <pre><code>python scripts/export_hf.py \\\n  --checkpoint checkpoints/ckpt_last.pt \\\n  --meta data/processed/meta.json \\\n  --output-dir outputs/hf_export \\\n  --push \\\n  --repo-id GhostPunishR/labcore-llm-50M\n</code></pre> <p>Convertir l'export HF en GGUF et quantifier:</p> <pre><code>python scripts/quantize_gguf.py \\\n  --hf-dir outputs/hf_export \\\n  --llama-cpp-dir third_party/llama.cpp \\\n  --output-dir outputs/gguf \\\n  --quant-type Q4_K_M\n</code></pre>"},{"location":"fr/export-and-deployment/#output-files-artifacts-produced","title":"Output Files / Artifacts Produced","text":"<p><code>outputs/hf_export/</code>:</p> <ul> <li><code>model.safetensors</code></li> <li><code>config.json</code></li> <li><code>tokenizer.json</code></li> <li><code>README.md</code></li> </ul> <p><code>outputs/gguf/</code>:</p> <ul> <li><code>labcore-50m-f16.gguf</code></li> <li><code>labcore-50m-q4_k_m.gguf</code> (ou <code>q5_k_m</code> / les deux avec <code>--quant-type all</code>)</li> </ul> <p>Warning</p> <p>La conversion GGUF exige un checkout <code>llama.cpp</code> valide avec script de conversion et binaire de quantization disponibles.</p>"},{"location":"fr/export-and-deployment/#common-errors","title":"Common Errors","text":"<ul> <li>Mauvais metadata mapping (<code>txt</code> vs <code>bin</code>): voir Meta path mismatch.</li> <li>Dependances <code>huggingface_hub</code>/<code>safetensors</code> manquantes: voir Torch not installed.</li> <li>Outils llama.cpp manquants: voir Windows path and policy issues.</li> </ul>"},{"location":"fr/fine-tuning/","title":"Fine-Tuning","text":"<p>Utilisez cette page pour du LoRA instruction tuning sur checkpoints CausalLM compatibles HF. Prerequis: dependances HF + finetune et modele de base accessible.</p>"},{"location":"fr/fine-tuning/#commands","title":"Command(s)","text":"<pre><code>python scripts/fine_tune_instruction.py \\\n  --model-id GhostPunishR/labcore-llm-50M \\\n  --dataset yahma/alpaca-cleaned \\\n  --dataset-split train \\\n  --output-dir outputs/lora_instruction \\\n  --config configs/bpe_rope_flash/bpe_50M_rope_flash.toml \\\n  --max-samples 20000 \\\n  --epochs 1\n</code></pre> <p>Dependances:</p> <pre><code>python -m pip install -e \".[torch,hf,finetune]\"\n</code></pre>"},{"location":"fr/fine-tuning/#output-files-artifacts-produced","title":"Output Files / Artifacts Produced","text":"<ul> <li><code>outputs/lora_instruction/</code> (adapter LoRA + fichiers tokenizer)</li> </ul>"},{"location":"fr/fine-tuning/#dataset-mapping","title":"Dataset Mapping","text":"<p>Le script accepte plusieurs alias de champs:</p> <ul> <li>Instruction: <code>instruction</code>, <code>question</code>, <code>prompt</code></li> <li>Input: <code>input</code>, <code>context</code></li> <li>Output: <code>output</code>, <code>response</code>, <code>answer</code></li> </ul>"},{"location":"fr/fine-tuning/#common-errors","title":"Common Errors","text":"<ul> <li>Dependances HF manquantes: voir Torch not installed.</li> <li>OOM en fine-tuning: voir Out of memory.</li> <li>Incompatibilite modele/tokenizer de base: verifier config et compatibilite modele avant lancement.</li> </ul> <p>Note</p> <p>Le fine-tuning utilise la stack HF Trainer et ecrit dans <code>outputs/</code> plutot que <code>checkpoints/</code>.</p>"},{"location":"fr/getting-started/","title":"Getting Started","text":"<p>Utilisez cette page pour un premier run reproductible en environ 5 minutes et verifier que l'environnement est sain. Prerequis: Python <code>3.11+</code>, <code>pip</code>, et GPU CUDA optionnel.</p>"},{"location":"fr/getting-started/#commands","title":"Command(s)","text":"<p>Installer les dependances:</p> <pre><code>python -m pip install -e \".[torch,dev]\"\n</code></pre> <p>Verifier CUDA:</p> <pre><code>python -c \"import torch; print(torch.cuda.is_available()); print(torch.version.cuda)\"\n</code></pre> <p>Demarrage rapide (preset de reference: tinyshakespeare + char + <code>configs/base.toml</code>):</p> <pre><code>python scripts/prepare_data.py --dataset tinyshakespeare --tokenizer char --output-format txt --output-dir data/processed\npython train.py --config configs/base.toml --tokenizer char --max-iters 5000\npython generate.py --checkpoint checkpoints/ckpt_last.pt --meta data/processed/meta.json --tokenizer char --prompt \"To be\"\n</code></pre> <p>Tip</p> <p>Pour un smoke test strict de 5 minutes, lancez l'entrainement, attendez le premier eval/checkpoint, puis stoppez et lancez la generation.</p>"},{"location":"fr/getting-started/#success-checklist","title":"Success Checklist","text":"<ul> <li>Les logs affichent au moins deux lignes <code>train_loss</code> et une tendance a la baisse.</li> <li>Un checkpoint existe dans <code>checkpoints/ckpt_last.pt</code>.</li> <li><code>generate.py</code> retourne un texte non vide.</li> </ul>"},{"location":"fr/getting-started/#common-errors","title":"Common Errors","text":"<ul> <li><code>ModuleNotFoundError: torch</code>: voir Torch not installed.</li> <li>CUDA attendu mais indisponible: voir CUDA not detected.</li> <li>Mauvais metadata path: voir Meta path mismatch.</li> </ul> <p>Warning</p> <p>Gardez <code>--checkpoint</code> et <code>--meta</code> alignes sur le meme run. Melanger des fichiers de runs differents donne des resultats trompeurs.</p>"},{"location":"fr/inference-and-demo/","title":"Inference and Demo","text":"<p>Utilisez cette page pour lancer une generation CLI deterministe et la demo Gradio locale. Prerequis: checkpoint entraine (<code>checkpoints/ckpt_last.pt</code>) et metadata correspondante.</p>"},{"location":"fr/inference-and-demo/#commands","title":"Command(s)","text":"<p>Generation CLI (chemins de reference):</p> <pre><code>python generate.py \\\n  --checkpoint checkpoints/ckpt_last.pt \\\n  --meta data/processed/meta.json \\\n  --tokenizer char \\\n  --prompt \"To be\" \\\n  --max-new-tokens 200 \\\n  --temperature 0.9 \\\n  --top-k 40 \\\n  --device cpu\n</code></pre> <p>Demo Gradio depuis checkpoint local:</p> <pre><code>python demo_gradio.py \\\n  --source local \\\n  --checkpoint checkpoints/ckpt_last.pt \\\n  --meta data/processed/meta.json \\\n  --device cpu \\\n  --port 7860\n</code></pre> <p>Demo Gradio depuis Hugging Face:</p> <pre><code>python demo_gradio.py \\\n  --source hf \\\n  --repo-id GhostPunishR/labcore-llm-50M \\\n  --device cpu \\\n  --port 7860\n</code></pre>"},{"location":"fr/inference-and-demo/#stable-generation-settings-debug-mode","title":"Stable Generation Settings (Debug Mode)","text":"<p>Utilisez un sampling conservateur pour deboguer la reproductibilite:</p> <ul> <li><code>temperature = 0.2</code> pour reduire l'aleatoire</li> <li><code>top-k = 20</code> (ou moins)</li> <li><code>max-new-tokens = 80</code> pour des checks rapides</li> </ul> <p>Tip</p> <p>Si la qualite chute brutalement, verifiez d'abord que <code>--meta</code> correspond au meme run tokenizer/checkpoint.</p>"},{"location":"fr/inference-and-demo/#sampling-controls","title":"Sampling Controls","text":"<ul> <li><code>temperature</code>: met a l'echelle les logits avant sampling.</li> <li><code>top_k</code>: conserve uniquement les <code>k</code> tokens les plus probables.</li> <li><code>top_p</code>: cutoff nucleus sampling (<code>1.0</code> le desactive).</li> <li><code>repetition_penalty</code>: penalise les tokens deja generes (<code>1.0</code> le desactive).</li> <li><code>use_kv_cache</code>: active le KV-cache pendant le decodage.</li> <li><code>stream</code>: active le streaming token-by-token.</li> </ul> <p><code>top_p</code> et <code>repetition_penalty</code> sont lus depuis <code>[generation]</code> avec <code>generate.py --config ...</code>.</p> <p>Exemple stable RTX 4060:</p> <pre><code>[generation]\ntemperature = 0.6\ntop_k = 50\ntop_p = 0.9\nrepetition_penalty = 1.1\nuse_kv_cache = true\nstream = true\nsystem_prompt = \"You are LabCore LLM.\"\nmax_history_turns = 6\n</code></pre>"},{"location":"fr/inference-and-demo/#reproducible-generation","title":"Reproducible Generation","text":"<ul> <li><code>seed</code>: fige les RNG Python/NumPy/Torch pour un sampling reproductible.</li> <li><code>deterministic</code>: active les algorithmes deterministes PyTorch (<code>warn_only=True</code>) et les reglages cuDNN deterministes.</li> </ul> <pre><code>[generation]\ntemperature = 0.6\ntop_k = 50\ntop_p = 0.9\nrepetition_penalty = 1.1\nuse_kv_cache = true\nstream = true\nsystem_prompt = \"You are LabCore LLM.\"\nmax_history_turns = 6\nseed = 1337\ndeterministic = true\n</code></pre>"},{"location":"fr/inference-and-demo/#kv-cache-streaming-and-chat","title":"KV Cache, Streaming, and Chat","text":"<ul> <li>Le KV-cache accelere la generation en reutilisant les key/value attention passees au lieu de recalculer tout le contexte.</li> <li>Le streaming met a jour la sortie demo incrementale token par token.</li> <li>Le chat multi-tour construit un prompt avec des marqueurs texte simples:</li> <li><code>&lt;|system|&gt;</code></li> <li><code>&lt;|user|&gt;</code></li> <li><code>&lt;|assistant|&gt;</code></li> <li><code>max_history_turns</code> conserve seulement les tours les plus recents pour borner la longueur du prompt.</li> </ul> <pre><code>[generation]\nuse_kv_cache = true\nstream = true\nsystem_prompt = \"You are LabCore LLM.\"\nmax_history_turns = 6\n</code></pre>"},{"location":"fr/inference-and-demo/#output-files-artifacts-produced","title":"Output Files / Artifacts Produced","text":"<ul> <li>CLI: texte genere dans le terminal</li> <li>Demo: texte genere dans l'UI Gradio</li> <li>Aucun nouveau fichier modele sauf export explicite</li> </ul>"},{"location":"fr/inference-and-demo/#common-errors","title":"Common Errors","text":"<ul> <li>Metadata tokenizer char manquante: voir Char vocab missing.</li> <li>Mauvais mapping metadata (<code>txt</code> vs <code>bin</code>): voir Meta path mismatch.</li> <li>Fallback CUDA: voir CUDA not detected.</li> </ul>"},{"location":"fr/operations/","title":"Operations","text":"<p>Utilisez cette page pour l'hygiene des dossiers operationnels, les checks de validation et les workflows release/securite. Prerequis: dependances du projet installees dans l'environnement actif.</p>"},{"location":"fr/operations/#artifact-directories","title":"Artifact Directories","text":"<ul> <li><code>data/</code>: datasets prepares, incluant <code>data/meta.json</code> pour runs <code>bin</code>.</li> <li><code>checkpoints/</code>: sorties entrainement (<code>ckpt_last.pt</code>, <code>train_log.json</code>, <code>ckpt_best.pt</code> si active).</li> <li><code>outputs/</code>: artifacts exportes (<code>hf_export/</code>, <code>gguf/</code>, sorties fine-tuning).</li> <li><code>runs/</code>: logs d'experience optionnels ou exports trackers externes (pas auto-cree par les scripts core).</li> </ul> <p>Note</p> <p>Les scripts core ecrivent surtout dans <code>checkpoints/</code> et <code>outputs/</code>. Gardez ces dossiers dans vos notes de run, mais ne committez pas de gros artifacts.</p>"},{"location":"fr/operations/#commands","title":"Command(s)","text":"<p>Checks qualite locaux:</p> <pre><code>python -m pytest -q\nruff check src scripts tests train.py generate.py demo_gradio.py --select E9,F63,F7,F82\n</code></pre> <p>Checks build docs:</p> <pre><code>python -m pip install -r docs/requirements.txt\npython -m mkdocs build\n</code></pre>"},{"location":"fr/operations/#output-files-artifacts-produced","title":"Output Files / Artifacts Produced","text":"<ul> <li>Logs tests/lint dans le terminal</li> <li>Site docs dans <code>site/</code> apres <code>mkdocs build</code></li> <li>Pipelines CI: <code>.github/workflows/ci.yml</code> et <code>.github/workflows/docs.yml</code></li> </ul>"},{"location":"fr/operations/#security-and-release","title":"Security and Release","text":"<p>Signalement securite (voir <code>SECURITY.md</code>):</p> <ul> <li>Utiliser GitHub Security Advisories pour les vulnerabilites</li> <li>Inclure impact, composants affectes et etapes de reproduction</li> </ul> <p>Flux release (voir <code>RELEASE.md</code>):</p> <ol> <li>Verifier que la CI est verte.</li> <li>Lancer les validations locales.</li> <li>Mettre a jour la version et le changelog.</li> <li>Tagger et publier la release.</li> </ol>"},{"location":"fr/operations/#common-errors","title":"Common Errors","text":"<ul> <li>Dependances manquantes: voir Torch not installed.</li> <li>Confusion metadata/path: voir Meta path mismatch.</li> <li>CUDA attendu mais indisponible: voir CUDA not detected.</li> </ul>"},{"location":"fr/training/","title":"Training","text":"<p>Utilisez cette page pour lancer, monitorer et checkpoint un entrainement avec des reglages coherents. Prerequis: dataset et metadata prepares depuis Data Pipeline.</p>"},{"location":"fr/training/#commands","title":"Command(s)","text":"<p>Commande de reference:</p> <pre><code>python train.py --config configs/base.toml --tokenizer char --max-iters 5000\n</code></pre> <p>Exemples d'override device:</p> <pre><code>python train.py --config configs/base.toml --tokenizer char --max-iters 5000 --device cpu\npython train.py --config configs/base.toml --tokenizer char --max-iters 5000 --device cuda\n</code></pre> <p>Flags <code>train.py</code>:</p> <ul> <li><code>--config</code>: chemin preset TOML (<code>CONFIG_EXAMPLE = configs/base.toml</code>)</li> <li><code>--max-iters</code>: override runtime du nombre total d'iterations</li> <li><code>--device</code>: <code>cpu</code> ou <code>cuda</code></li> <li><code>--tokenizer</code>: <code>char</code> ou <code>bpe</code></li> </ul>"},{"location":"fr/training/#precision-and-gradient-accumulation","title":"Precision and Gradient Accumulation","text":"<p>Configurez ces valeurs dans <code>[training]</code>:</p> <ul> <li><code>grad_accum_steps</code>: facteur d'accumulation de gradient (default <code>1</code>)</li> <li><code>precision</code>: <code>fp32</code> (default), <code>fp16</code> ou <code>bf16</code></li> </ul> <p><code>effective_batch_size = batch_size * grad_accum_steps</code></p> <p>La mixed precision est activee seulement si <code>device = \"cuda\"</code> et <code>precision != \"fp32\"</code>. Sur CPU, l'entrainement repasse en <code>fp32</code>.</p> <p>Exemple RTX 4060:</p> <pre><code>[training]\nbatch_size = 8\ngrad_accum_steps = 4\nprecision = \"fp16\"\n</code></pre>"},{"location":"fr/training/#best-checkpoint-early-stopping","title":"Best Checkpoint &amp; Early Stopping","text":"<ul> <li><code>save_best</code> sauvegarde <code>checkpoints/ckpt_best.pt</code> quand la loss de validation s'ameliore d'au moins <code>early_stopping_min_delta</code>.</li> <li><code>early_stopping</code> est desactive par defaut.</li> <li><code>early_stopping_patience</code> compte les evaluations sans amelioration suffisante.</li> <li><code>early_stopping_min_delta</code> definit le seuil minimal d'amelioration de <code>val_loss</code>.</li> </ul> <pre><code>[training]\nearly_stopping = true\nearly_stopping_patience = 3\nearly_stopping_min_delta = 0.001\nsave_best = true\n</code></pre>"},{"location":"fr/training/#data_format-and-metadata-mapping","title":"<code>data_format</code> and Metadata Mapping","text":"Mode entrainement Valeur config Artifacts attendus Chemin metadata Pipeline texte <code>training.data_format = \"txt\"</code> <code>data/processed/train.txt</code> + <code>data/processed/val.txt</code> (ou <code>.npy</code>) <code>data/processed/meta.json</code> (<code>META_TXT</code>) Pipeline binaire <code>training.data_format = \"bin\"</code> <code>data/train.bin</code> + <code>data/val.bin</code> <code>data/meta.json</code> (<code>META_BIN</code>) <p>Note</p> <p>En mode binaire, si <code>data.processed_dir</code> pointe vers <code>data/processed</code>, <code>train.py</code> verifie automatiquement le parent (<code>data/</code>) pour <code>train.bin</code> et <code>val.bin</code>.</p>"},{"location":"fr/training/#checkpointing-and-resume-behavior","title":"Checkpointing and Resume Behavior","text":"<p>Fichiers produits pendant l'entrainement:</p> <ul> <li><code>checkpoints/ckpt_last.pt</code> (<code>CHECKPOINT</code>)</li> <li><code>checkpoints/ckpt_best.pt</code> (si <code>save_best = true</code>)</li> <li><code>checkpoints/train_log.json</code></li> </ul> <p>Warning</p> <p>La reprise native depuis checkpoint n'est pas encore implementee dans <code>train.py</code>. <code>ckpt_last.pt</code> sert surtout a l'inference/export et a l'inspection d'etat.</p>"},{"location":"fr/training/#common-errors","title":"Common Errors","text":"<ul> <li>Binaries manquants: voir Binary shards not found.</li> <li>Mauvais chemin metadata: voir Meta path mismatch.</li> <li>Echec d'inference vocab: voir Char vocab missing.</li> <li>Warning CUDA fallback: voir CUDA not detected.</li> </ul>"},{"location":"fr/troubleshooting/","title":"Troubleshooting","text":"<p>Utilisez cette page pour diagnostiquer rapidement les erreurs frequentes de setup, donnees et runtime. Toutes les ancres ci-dessous sont referencees depuis les pages guides.</p>"},{"location":"fr/troubleshooting/#torch-not-installed","title":"Torch not installed","text":"<p>Symptomes: <code>ModuleNotFoundError: torch</code>, ou echec des scripts a l'import.</p> <p>Correctif:</p> <pre><code>python -m pip install -e \".[torch,dev]\"\n</code></pre> <p>Utilisez le selecteur officiel si vous avez besoin d'un build CUDA specifique: https://pytorch.org/get-started/locally/</p>"},{"location":"fr/troubleshooting/#cuda-not-detected","title":"CUDA not detected","text":"<p>Symptomes: <code>torch.cuda.is_available()</code> retourne <code>False</code>, ou fallback CPU dans les scripts.</p> <p>Verification rapide:</p> <pre><code>python -c \"import torch; print(torch.cuda.is_available()); print(torch.version.cuda)\"\n</code></pre> <p>Points a verifier:</p> <ul> <li>Driver NVIDIA installe et a jour</li> <li>Build PyTorch compatible avec votre runtime CUDA</li> <li>Meme environnement Python utilise pour l'installation et l'execution</li> </ul>"},{"location":"fr/troubleshooting/#meta-path-mismatch","title":"Meta path mismatch","text":"<p>Symptomes: generation/entrainement incorrects ou impossibles a charger via metadata tokenizer.</p> <p>Mapping attendu:</p> <ul> <li>Pipeline <code>txt</code> -&gt; <code>data/processed/meta.json</code></li> <li>Pipeline <code>bin</code> -&gt; <code>data/meta.json</code></li> </ul>"},{"location":"fr/troubleshooting/#binary-shards-not-found","title":"Binary shards not found","text":"<p>Symptomes: <code>Binary shards not found</code> avec <code>training.data_format = \"bin\"</code>.</p> <p>Correctif:</p> <pre><code>python scripts/prepare_data.py --dataset tinyshakespeare --tokenizer char --output-format bin --output-dir data/processed\n</code></pre>"},{"location":"fr/troubleshooting/#char-vocab-missing","title":"Char vocab missing","text":"<p>Symptomes: <code>Char tokenizer requires vocab in meta.json</code>.</p> <p>Correctif:</p> <pre><code>python scripts/prepare_data.py --dataset tinyshakespeare --tokenizer char --output-format txt --output-dir data/processed\n</code></pre> <p>Puis passez le <code>--meta data/processed/meta.json</code> correspondant aux commandes generation/export.</p>"},{"location":"fr/troubleshooting/#oom-errors","title":"Out of memory","text":"<p>Reduire dans cet ordre:</p> <ol> <li><code>training.batch_size</code></li> <li><code>model.block_size</code></li> <li><code>training.gradient_accumulation_steps</code></li> <li>Taille modele / complexite preset</li> </ol>"},{"location":"fr/troubleshooting/#flashattention-not-available","title":"FlashAttention not available","text":"<p>LabCore applique des fallbacks automatiques:</p> <ul> <li>FlashAttention (priorite, si disponible)</li> <li>Fallback SDPA PyTorch</li> <li>Fallback attention causale standard</li> </ul>"},{"location":"fr/troubleshooting/#windows-path-policy","title":"Windows path and policy issues","text":"<ul> <li>Activez le venv avant les commandes.</li> <li>Lancez les commandes depuis la racine du repo.</li> <li>Citez les chemins si des espaces sont presents.</li> <li>Si la policy PowerShell bloque les scripts, utilisez les commandes <code>python ...</code> directement.</li> </ul>"}]}
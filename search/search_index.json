{"config":{"lang":["en","fr"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"LabCore LLM","text":"<p>This documentation is the operational guide for running LabCore end to end: data preparation, training, inference, and export. English pages in <code>docs/</code> are the source of truth, and French pages in <code>docs/fr/</code> are concise mirrors.</p> <p> </p>"},{"location":"#reference-preset-used-across-docs","title":"Reference Preset Used Across Docs","text":"<p>All examples are standardized on this reference setup:</p> <ul> <li>Dataset: <code>tinyshakespeare</code></li> <li>Tokenizer: <code>char</code></li> <li><code>CONFIG_EXAMPLE = configs/base.toml</code></li> <li>Canonical training override: <code>--max-iters 5000</code></li> <li><code>CHECKPOINT = checkpoints/ckpt_last.pt</code></li> <li><code>META_TXT = data/processed/meta.json</code></li> <li><code>META_BIN = data/meta.json</code></li> </ul> <p>Tip</p> <p>Keep these values unchanged for your first full run. Most failures come from checkpoint and metadata path mismatches.</p>"},{"location":"#quick-install","title":"Quick Install","text":"<pre><code>python -m pip install -e \".[torch,dev]\"\n</code></pre> <p>For Hugging Face export and demo UI:</p> <pre><code>python -m pip install -e \".[torch,hf,demo]\"\n</code></pre>"},{"location":"#quick-start-commands","title":"Quick Start Commands","text":"<pre><code>python scripts/prepare_data.py --dataset tinyshakespeare --tokenizer char --output-format txt --output-dir data/processed\npython train.py --config configs/base.toml --tokenizer char --max-iters 5000\npython generate.py --checkpoint checkpoints/ckpt_last.pt --meta data/processed/meta.json --tokenizer char --prompt \"To be\"\n</code></pre> <p>Expected artifacts:</p> <ul> <li><code>checkpoints/ckpt_last.pt</code></li> <li><code>checkpoints/train_log.json</code></li> <li><code>data/processed/meta.json</code></li> </ul>"},{"location":"#end-to-end-flow","title":"End-to-End Flow","text":"<pre><code>prepare_data.py -&gt; train.py -&gt; generate.py/demo_gradio.py -&gt; export_hf.py -&gt; quantize_gguf.py\n</code></pre>"},{"location":"#documentation-map","title":"Documentation Map","text":""},{"location":"#guides","title":"Guides","text":"<ul> <li>Getting Started: environment setup and a reproducible first run.</li> <li>Data Pipeline: build <code>txt</code> or <code>bin</code> datasets and metadata.</li> <li>Training: run training, checkpointing, and format selection.</li> <li>Inference &amp; Demo: CLI generation and Gradio demo.</li> <li>Fine-Tuning: LoRA instruction tuning workflow.</li> <li>Export &amp; Deployment: HF export and GGUF conversion.</li> </ul>"},{"location":"#reference","title":"Reference","text":"<ul> <li>Configuration: complete TOML key reference.</li> <li>Operations: artifacts, release flow, and operational checks.</li> <li>Troubleshooting: anchored fixes for common failures.</li> <li>Benchmarks: benchmark template and reporting method.</li> </ul>"},{"location":"#development","title":"Development","text":"<ul> <li>Developer Guide: contributor workflow and validation commands.</li> </ul>"},{"location":"#next-related","title":"Next / Related","text":"<ul> <li>Getting Started</li> <li>Data Pipeline</li> <li>Training</li> </ul>"},{"location":"benchmarks/","title":"Benchmarks","text":"<p>Use the inference benchmark script to measure reproducible <code>tokens/s</code> and peak VRAM. The script supports both local checkpoints and Hugging Face repos and can export JSON + Markdown outputs.</p>"},{"location":"benchmarks/#inference-benchmark-script","title":"Inference Benchmark Script","text":"<pre><code>python scripts/benchmark_infer.py --help\n</code></pre> <p>Default runtime is intentionally short (warmup + 3 measured runs).</p>"},{"location":"benchmarks/#local-example","title":"Local Example","text":"<pre><code>python scripts/benchmark_infer.py \\\n  --source local \\\n  --checkpoint checkpoints/ckpt_last.pt \\\n  --meta data/processed/meta.json \\\n  --tokenizer char \\\n  --config configs/base.toml \\\n  --device cpu \\\n  --json-out outputs/bench_infer.json \\\n  --md-out outputs/bench_infer.md\n</code></pre>"},{"location":"benchmarks/#hugging-face-example","title":"Hugging Face Example","text":"<pre><code>python scripts/benchmark_infer.py \\\n  --source hf \\\n  --repo-id LabCoreAI/&lt;id&gt; \\\n  --config configs/base.toml \\\n  --device cuda \\\n  --json-out outputs/bench_infer_hf.json \\\n  --md-out outputs/bench_infer_hf.md\n</code></pre>"},{"location":"benchmarks/#what-is-measured","title":"What Is Measured","text":"<ul> <li>Warmup generation (<code>--warmup-tokens</code>, not counted in final throughput).</li> <li>Measured generation (<code>--gen-tokens</code>) repeated <code>--iters</code> times.</li> <li>Throughput summary: <code>mean</code>, <code>min</code>, <code>max</code> tokens/sec.</li> <li>Peak VRAM (<code>torch.cuda.max_memory_allocated</code>) when CUDA is used.</li> </ul> <p>Reproducibility settings are read from <code>[generation]</code> when <code>--config</code> is provided:</p> <ul> <li><code>seed</code></li> <li><code>deterministic</code></li> <li>sampling settings (<code>temperature</code>, <code>top_k</code>, <code>top_p</code>, <code>repetition_penalty</code>)</li> <li><code>use_kv_cache</code> (unless overridden by CLI flag)</li> </ul>"},{"location":"benchmarks/#json-output-schema-summary","title":"JSON Output Schema (Summary)","text":"<pre><code>{\n  \"timestamp\": \"...\",\n  \"commit\": \"...\",\n  \"platform\": {\"os\": \"...\", \"python\": \"...\"},\n  \"torch\": {\"version\": \"...\", \"cuda\": \"...\"},\n  \"device\": {\"type\": \"cpu|cuda\", \"name\": \"...\"},\n  \"model\": {\"source\": \"local|hf\", \"params_m\": 0.0, \"block_size\": 0, \"n_layer\": 0, \"n_head\": 0, \"n_embd\": 0},\n  \"generation\": {\"prompt\": \"...\", \"gen_tokens\": 256, \"temperature\": 0.9, \"top_k\": 40, \"top_p\": 1.0, \"repetition_penalty\": 1.0, \"use_kv_cache\": true},\n  \"results\": {\"iters\": 3, \"tokens_per_sec\": {\"mean\": 0.0, \"min\": 0.0, \"max\": 0.0}, \"vram_peak_mib\": null}\n}\n</code></pre>"},{"location":"benchmarks/#community-results","title":"Community Results","text":"<p>Paste the generated Markdown row (from <code>--md-out</code> or terminal output) into this table. Attach the JSON output in the PR description if available.</p> Device Source Model size (params M) KV-cache gen_tokens mean tok/s peak VRAM MiB your result local/hf 0.000 on/off 256 0.00 N/A or value"},{"location":"configuration-reference/","title":"Configuration Reference","text":"<p>Use this page as the single source of truth for <code>train.py</code> TOML keys and defaults. Prerequisite: familiarity with the reference preset (<code>configs/base.toml</code>).</p>"},{"location":"configuration-reference/#canonical-values-used-in-guides","title":"Canonical Values Used in Guides","text":"<ul> <li><code>CONFIG_EXAMPLE = configs/base.toml</code></li> <li><code>CHECKPOINT = checkpoints/ckpt_last.pt</code></li> <li><code>META_TXT = data/processed/meta.json</code></li> <li><code>META_BIN = data/meta.json</code></li> </ul>"},{"location":"configuration-reference/#section-and-key-reference","title":"Section and Key Reference","text":""},{"location":"configuration-reference/#general","title":"<code>[general]</code>","text":"Key Type Typical value Notes <code>run_name</code> string <code>\"tiny_char_baseline\"</code> Informational run label. <code>seed</code> int <code>1337</code> Optional random seed field. <code>tokenizer</code> string <code>\"char\"</code> or <code>\"bpe\"</code> Default is <code>\"char\"</code> if omitted."},{"location":"configuration-reference/#data","title":"<code>[data]</code>","text":"Key Type Typical value Notes <code>dataset</code> string <code>\"tinyshakespeare\"</code> Metadata label. <code>processed_dir</code> string path <code>\"data/processed\"</code> Default is <code>\"data/processed\"</code> if omitted."},{"location":"configuration-reference/#model","title":"<code>[model]</code>","text":"Key Type Typical value Notes <code>vocab_size</code> int <code>65</code> (char) / <code>50257</code> (bpe) Can be inferred from metadata/tokenizer when omitted. <code>block_size</code> int <code>512</code> Sequence length. <code>n_layer</code> int <code>6</code> Transformer depth. <code>n_head</code> int <code>8</code> Attention heads. <code>n_embd</code> int <code>256</code> Embedding size. <code>dropout</code> float <code>0.1</code> Dropout rate. <code>bias</code> bool <code>true</code> Linear layer bias toggle. <code>use_rope</code> bool <code>false</code>/<code>true</code> Default <code>false</code>. <code>use_flash</code> bool <code>false</code>/<code>true</code> Default <code>false</code>."},{"location":"configuration-reference/#optimizer","title":"<code>[optimizer]</code>","text":"Key Type Typical value Notes <code>learning_rate</code> float <code>3e-4</code> Falls back to <code>[training]</code> if missing. <code>weight_decay</code> float <code>0.01</code> Falls back to <code>[training]</code> if missing. <code>beta1</code> float <code>0.9</code> AdamW beta1. <code>beta2</code> float <code>0.95</code> AdamW beta2. <code>grad_clip</code> float <code>1.0</code> Falls back to <code>[training]</code> if missing."},{"location":"configuration-reference/#training","title":"<code>[training]</code>","text":"Key Type Typical value Notes <code>batch_size</code> int <code>8</code> Micro-batch size per step. <code>gradient_accumulation_steps</code> int <code>1</code> to <code>8</code> Effective batch multiplier. <code>max_iters</code> int <code>5000</code> Docs reference value. <code>warmup_iters</code> int <code>0</code> to <code>500</code> LR warmup steps. <code>lr_decay_iters</code> int <code>5000</code> Usually align with <code>max_iters</code>. <code>min_lr</code> float <code>3e-5</code> Cosine LR floor. <code>eval_interval</code> int <code>200</code> Eval/checkpoint cadence. <code>eval_iters</code> int <code>50</code> Validation batches per eval. <code>log_interval</code> int <code>20</code> Console logging cadence. <code>save_interval</code> int <code>200</code> or <code>500</code> Optional extra save cadence. <code>device</code> string <code>\"cuda\"</code> or <code>\"cpu\"</code> CLI <code>--device</code> overrides this. <code>checkpoint_dir</code> string path <code>\"checkpoints\"</code> Produces <code>ckpt_last.pt</code> and <code>train_log.json</code>. <code>data_format</code> string <code>\"txt\"</code> or <code>\"bin\"</code> Default is <code>\"txt\"</code> if omitted."},{"location":"configuration-reference/#generation","title":"<code>[generation]</code>","text":"Key Type Typical value Notes <code>max_new_tokens</code> int <code>200</code> Useful for shared defaults in docs. <code>temperature</code> float <code>0.6</code> to <code>0.9</code> Sampling randomness. <code>top_k</code> int <code>40</code> to <code>50</code> Sampling truncation."},{"location":"configuration-reference/#loader-defaults-applied-automatically","title":"Loader Defaults Applied Automatically","text":"<p><code>load_config</code> guarantees:</p> <ul> <li><code>general.tokenizer = \"char\"</code></li> <li><code>data.processed_dir = \"data/processed\"</code></li> <li><code>model.use_rope = false</code></li> <li><code>model.use_flash = false</code></li> <li><code>training.data_format = \"txt\"</code></li> </ul>"},{"location":"configuration-reference/#example-minimal-small-config","title":"Example: Minimal Small Config","text":"<pre><code>[general]\ntokenizer = \"char\"\n\n[data]\ndataset = \"tinyshakespeare\"\nprocessed_dir = \"data/processed\"\n\n[model]\nblock_size = 256\nn_layer = 4\nn_head = 4\nn_embd = 192\nvocab_size = 65\ndropout = 0.1\nbias = true\nuse_rope = false\nuse_flash = false\n\n[training]\nbatch_size = 8\ngradient_accumulation_steps = 1\nmax_iters = 2000\neval_interval = 200\neval_iters = 50\nlog_interval = 20\ndevice = \"cpu\"\ncheckpoint_dir = \"checkpoints\"\ndata_format = \"txt\"\n</code></pre>"},{"location":"configuration-reference/#example-rtx-4060-starting-point-8gb-validate-locally","title":"Example: RTX 4060 Starting Point (8GB, Validate Locally)","text":"<pre><code>[general]\ntokenizer = \"char\"\n\n[data]\ndataset = \"tinyshakespeare\"\nprocessed_dir = \"data/processed\"\n\n[model]\nblock_size = 512\nn_layer = 6\nn_head = 8\nn_embd = 256\ndropout = 0.1\nbias = true\nuse_rope = false\nuse_flash = false\n\n[optimizer]\nlearning_rate = 3e-4\nweight_decay = 0.01\nbeta1 = 0.9\nbeta2 = 0.95\ngrad_clip = 1.0\n\n[training]\nbatch_size = 8\ngradient_accumulation_steps = 2\nmax_iters = 5000\nwarmup_iters = 200\nlr_decay_iters = 5000\nmin_lr = 3e-5\neval_interval = 200\neval_iters = 50\nlog_interval = 20\nsave_interval = 200\ndevice = \"cuda\"\ncheckpoint_dir = \"checkpoints\"\ndata_format = \"txt\"\n</code></pre> <p>Note</p> <p>The RTX 4060 block above is a starting template, not a guaranteed benchmark profile. Adjust based on your exact VRAM and driver stack.</p>"},{"location":"configuration-reference/#related","title":"Related","text":"<ul> <li>Training</li> <li>Operations</li> <li>Benchmarks</li> </ul>"},{"location":"data-pipeline/","title":"Data Pipeline","text":"<p>Use this page to prepare training data and metadata in a predictable layout. Prerequisite: dependencies installed from Getting Started.</p>"},{"location":"data-pipeline/#commands","title":"Command(s)","text":"<p>Reference <code>txt</code> pipeline (used by <code>configs/base.toml</code>):</p> <pre><code>python scripts/prepare_data.py \\\n  --dataset tinyshakespeare \\\n  --tokenizer char \\\n  --output-format txt \\\n  --raw-dir data/raw \\\n  --output-dir data/processed \\\n  --val-ratio 0.1\n</code></pre> <p>Alternative <code>bin</code> pipeline:</p> <pre><code>python scripts/prepare_data.py \\\n  --dataset tinyshakespeare \\\n  --tokenizer char \\\n  --output-format bin \\\n  --raw-dir data/raw \\\n  --output-dir data/processed \\\n  --val-ratio 0.1\n</code></pre>"},{"location":"data-pipeline/#output-files-artifacts-produced","title":"Output Files / Artifacts Produced","text":"<p><code>txt</code> format (<code>output-dir = data/processed</code>):</p> <ul> <li><code>data/processed/train.txt</code></li> <li><code>data/processed/val.txt</code></li> <li><code>data/processed/corpus.txt</code></li> <li><code>data/processed/train.npy</code></li> <li><code>data/processed/val.npy</code></li> <li><code>data/processed/meta.json</code> (<code>META_TXT</code>)</li> </ul> <p><code>bin</code> format:</p> <ul> <li><code>data/train.bin</code></li> <li><code>data/val.bin</code></li> <li><code>data/meta.json</code> (<code>META_BIN</code>)</li> </ul> <p>Note</p> <p>For <code>--output-format bin</code>, if <code>--output-dir</code> ends with <code>processed</code>, binary files are written to its parent (<code>data/</code>).</p>"},{"location":"data-pipeline/#format-selection","title":"Format Selection","text":"<ul> <li>Use <code>txt</code> when training with <code>training.data_format = \"txt\"</code> and metadata at <code>data/processed/meta.json</code>.</li> <li>Use <code>bin</code> when training with <code>training.data_format = \"bin\"</code> and metadata at <code>data/meta.json</code>.</li> </ul>"},{"location":"data-pipeline/#common-errors","title":"Common Errors","text":"<ul> <li>Missing binary shards: see Binary shards not found.</li> <li>Wrong metadata path: see Meta path mismatch.</li> <li>Char tokenizer vocab issues: see Char vocab missing.</li> </ul>"},{"location":"data-pipeline/#next-related","title":"Next / Related","text":"<ul> <li>Training</li> <li>Inference &amp; Demo</li> <li>Troubleshooting</li> </ul>"},{"location":"developer-guide/","title":"Developer Guide","text":"<p>This page is for contributors working on the codebase itself.</p>"},{"location":"developer-guide/#repository-map","title":"Repository Map","text":"<pre><code>src/labcore_llm/\n  config/      # TOML loader and defaults\n  data/        # dataset abstractions\n  model/       # GPT model implementation\n  tokenizer/   # char + BPE tokenizers\n  trainer/     # training loop, scheduler, checkpointing\n\nscripts/       # data prep, export, quantize, fine-tune helpers\nconfigs/       # TOML presets\ntests/         # unit tests\n</code></pre>"},{"location":"developer-guide/#local-dev-environment","title":"Local Dev Environment","text":"<pre><code>python -m venv .venv\n## PowerShell\n.\\.venv\\Scripts\\Activate.ps1\npython -m pip install --upgrade pip\npython -m pip install -e \".[torch,dev]\"\n</code></pre>"},{"location":"developer-guide/#validation-commands","title":"Validation Commands","text":"<p>Run tests:</p> <pre><code>python -m pytest -q\n</code></pre> <p>Run lint rules aligned with CI:</p> <pre><code>ruff check src scripts tests train.py generate.py demo_gradio.py --select E9,F63,F7,F82\n</code></pre>"},{"location":"developer-guide/#ci-workflows","title":"CI Workflows","text":"<ul> <li><code>.github/workflows/ci.yml</code>: lint + tests</li> <li><code>.github/workflows/docs.yml</code>: MkDocs build and deploy</li> </ul>"},{"location":"developer-guide/#contribution-quality-bar","title":"Contribution Quality Bar","text":"<ul> <li>Keep commits focused and atomic.</li> <li>Update docs for behavior/CLI changes.</li> <li>Add tests for bug fixes and new logic.</li> <li>Do not commit large data/model artifacts.</li> </ul>"},{"location":"developer-guide/#packaging-notes","title":"Packaging Notes","text":"<ul> <li>Project uses <code>src/</code> layout with setuptools.</li> <li>Optional dependency groups are defined in <code>pyproject.toml</code>.</li> <li>Entry scripts are plain Python files, not console-script wrappers.</li> </ul>"},{"location":"export-and-deployment/","title":"Export and Deployment","text":"<p>Use this page to package a local checkpoint for Hugging Face and optional GGUF deployment. Prerequisites: a valid checkpoint (<code>checkpoints/ckpt_last.pt</code>) and matching metadata (<code>data/processed/meta.json</code> or <code>data/meta.json</code>).</p>"},{"location":"export-and-deployment/#hf-export-vs-gguf-export","title":"HF Export vs GGUF Export","text":"<ul> <li>HF export creates standard model artifacts for Hugging Face workflows.</li> <li>GGUF conversion creates quantized files for <code>llama.cpp</code>-style runtimes.</li> </ul>"},{"location":"export-and-deployment/#commands","title":"Command(s)","text":"<p>Export local checkpoint to HF layout:</p> <pre><code>python scripts/export_hf.py \\\n  --checkpoint checkpoints/ckpt_last.pt \\\n  --meta data/processed/meta.json \\\n  --output-dir outputs/hf_export\n</code></pre> <p>Push exported folder to HF Hub:</p> <pre><code>python scripts/export_hf.py \\\n  --checkpoint checkpoints/ckpt_last.pt \\\n  --meta data/processed/meta.json \\\n  --output-dir outputs/hf_export \\\n  --push \\\n  --repo-id GhostPunishR/labcore-llm-50M\n</code></pre> <p>Convert HF export to GGUF and quantize:</p> <pre><code>python scripts/quantize_gguf.py \\\n  --hf-dir outputs/hf_export \\\n  --llama-cpp-dir third_party/llama.cpp \\\n  --output-dir outputs/gguf \\\n  --quant-type Q4_K_M\n</code></pre>"},{"location":"export-and-deployment/#output-files-artifacts-produced","title":"Output Files / Artifacts Produced","text":"<p><code>outputs/hf_export/</code>:</p> <ul> <li><code>model.safetensors</code></li> <li><code>config.json</code></li> <li><code>tokenizer.json</code></li> <li><code>README.md</code></li> </ul> <p><code>outputs/gguf/</code>:</p> <ul> <li><code>labcore-50m-f16.gguf</code></li> <li><code>labcore-50m-q4_k_m.gguf</code> (or <code>q5_k_m</code> / both when <code>--quant-type all</code>)</li> </ul> <p>Warning</p> <p>GGUF conversion requires a valid <code>llama.cpp</code> checkout with conversion script and quantizer binary available.</p>"},{"location":"export-and-deployment/#common-errors","title":"Common Errors","text":"<ul> <li>Metadata mismatch (<code>txt</code> vs <code>bin</code>): see Meta path mismatch.</li> <li>Missing <code>huggingface_hub</code>/<code>safetensors</code>: see Torch not installed.</li> <li>Missing llama.cpp tools: see Windows path and policy issues.</li> </ul>"},{"location":"export-and-deployment/#next-related","title":"Next / Related","text":"<ul> <li>Fine-Tuning</li> <li>Operations</li> <li>Troubleshooting</li> </ul>"},{"location":"fine-tuning/","title":"Fine-Tuning","text":"<p>Use this page for LoRA instruction tuning on HF-compatible CausalLM checkpoints. Prerequisites: HF + finetune dependencies and an accessible base model.</p>"},{"location":"fine-tuning/#commands","title":"Command(s)","text":"<pre><code>python scripts/fine_tune_instruction.py \\\n  --model-id GhostPunishR/labcore-llm-50M \\\n  --dataset yahma/alpaca-cleaned \\\n  --dataset-split train \\\n  --output-dir outputs/lora_instruction \\\n  --config configs/bpe_rope_flash/bpe_50M_rope_flash.toml \\\n  --max-samples 20000 \\\n  --epochs 1\n</code></pre> <p>Dependencies:</p> <pre><code>python -m pip install -e \".[torch,hf,finetune]\"\n</code></pre>"},{"location":"fine-tuning/#output-files-artifacts-produced","title":"Output Files / Artifacts Produced","text":"<ul> <li><code>outputs/lora_instruction/</code> (LoRA adapter + tokenizer files)</li> </ul>"},{"location":"fine-tuning/#dataset-mapping","title":"Dataset Mapping","text":"<p>The script accepts common field aliases:</p> <ul> <li>Instruction: <code>instruction</code>, <code>question</code>, <code>prompt</code></li> <li>Input: <code>input</code>, <code>context</code></li> <li>Output: <code>output</code>, <code>response</code>, <code>answer</code></li> </ul>"},{"location":"fine-tuning/#common-errors","title":"Common Errors","text":"<ul> <li>Missing HF dependencies: see Torch not installed.</li> <li>OOM during fine-tuning: see Out of memory.</li> <li>Wrong base model/tokenizer expectations: verify config and model compatibility before launching.</li> </ul> <p>Note</p> <p>Fine-tuning uses the HF trainer stack and writes to <code>outputs/</code> rather than <code>checkpoints/</code>.</p>"},{"location":"fine-tuning/#next-related","title":"Next / Related","text":"<ul> <li>Configuration Reference</li> <li>Export &amp; Deployment</li> <li>Operations</li> </ul>"},{"location":"getting-started/","title":"Getting Started","text":"<p>Use this page to get a reproducible first run in about 5 minutes and confirm your environment is healthy. Prerequisites: Python <code>3.11+</code>, <code>pip</code>, and optional CUDA GPU.</p>"},{"location":"getting-started/#commands","title":"Command(s)","text":"<p>Install dependencies:</p> <pre><code>python -m pip install -e \".[torch,dev]\"\n</code></pre> <p>Check CUDA visibility:</p> <pre><code>python -c \"import torch; print(torch.cuda.is_available()); print(torch.version.cuda)\"\n</code></pre> <p>Quick Start (reference preset: tinyshakespeare + char + <code>configs/base.toml</code>):</p> <pre><code>python scripts/prepare_data.py --dataset tinyshakespeare --tokenizer char --output-format txt --output-dir data/processed\npython train.py --config configs/base.toml --tokenizer char --max-iters 5000\npython generate.py --checkpoint checkpoints/ckpt_last.pt --meta data/processed/meta.json --tokenizer char --prompt \"To be\"\n</code></pre> <p>Tip</p> <p>For a strict 5-minute smoke test, start the training command, wait for the first eval/checkpoint output, then stop and run generation.</p>"},{"location":"getting-started/#output-files-artifacts-produced","title":"Output Files / Artifacts Produced","text":"<ul> <li><code>data/processed/meta.json</code></li> <li><code>data/processed/train.txt</code>, <code>data/processed/val.txt</code></li> <li><code>checkpoints/ckpt_last.pt</code></li> <li><code>checkpoints/train_log.json</code></li> </ul>"},{"location":"getting-started/#success-checklist","title":"Success Checklist","text":"<ul> <li>Training logs show at least two <code>train_loss</code> lines and the value trends down.</li> <li>A checkpoint exists at <code>checkpoints/ckpt_last.pt</code>.</li> <li><code>generate.py</code> returns non-empty text from your prompt.</li> </ul>"},{"location":"getting-started/#common-errors","title":"Common Errors","text":"<ul> <li><code>ModuleNotFoundError: torch</code>: see Torch not installed.</li> <li>CUDA expected but disabled: see CUDA not detected.</li> <li>Metadata mismatch: see Meta path mismatch.</li> </ul> <p>Warning</p> <p>Keep <code>--checkpoint</code> and <code>--meta</code> aligned with the same run. Mixed files from different runs produce misleading results.</p>"},{"location":"getting-started/#next-related","title":"Next / Related","text":"<ul> <li>Data Pipeline</li> <li>Training</li> <li>Troubleshooting</li> </ul>"},{"location":"inference-and-demo/","title":"Inference and Demo","text":"<p>Use this page to run deterministic CLI generation and the local Gradio demo. Prerequisites: a trained checkpoint (<code>checkpoints/ckpt_last.pt</code>) and matching metadata.</p>"},{"location":"inference-and-demo/#commands","title":"Command(s)","text":"<p>CLI generation (reference paths):</p> <pre><code>python generate.py \\\n  --checkpoint checkpoints/ckpt_last.pt \\\n  --meta data/processed/meta.json \\\n  --tokenizer char \\\n  --prompt \"To be\" \\\n  --max-new-tokens 200 \\\n  --temperature 0.9 \\\n  --top-k 40 \\\n  --device cpu\n</code></pre> <p>Gradio demo from local checkpoint:</p> <pre><code>python demo_gradio.py \\\n  --source local \\\n  --checkpoint checkpoints/ckpt_last.pt \\\n  --meta data/processed/meta.json \\\n  --device cpu \\\n  --port 7860\n</code></pre> <p>Gradio demo from Hugging Face:</p> <pre><code>python demo_gradio.py \\\n  --source hf \\\n  --repo-id GhostPunishR/labcore-llm-50M \\\n  --device cpu \\\n  --port 7860\n</code></pre>"},{"location":"inference-and-demo/#stable-generation-settings-debug-mode","title":"Stable Generation Settings (Debug Mode)","text":"<p>Use conservative sampling when debugging reproducibility:</p> <ul> <li><code>temperature = 0.2</code> to reduce randomness</li> <li><code>top-k = 20</code> (or lower)</li> <li><code>max-new-tokens = 80</code> for quick checks</li> </ul> <p>Tip</p> <p>If output quality suddenly drops, first verify that <code>--meta</code> belongs to the same tokenizer/checkpoint run.</p>"},{"location":"inference-and-demo/#sampling-controls","title":"Sampling Controls","text":"<ul> <li><code>temperature</code>: scales logits before sampling.</li> <li><code>top_k</code>: keeps only the <code>k</code> most likely tokens.</li> <li><code>top_p</code>: nucleus sampling cutoff (<code>1.0</code> disables it).</li> <li><code>repetition_penalty</code>: penalizes already generated tokens (<code>1.0</code> disables it).</li> <li><code>use_kv_cache</code>: enables KV cache during decoding.</li> <li><code>stream</code>: enables token-by-token output streaming.</li> </ul> <p><code>top_p</code> and <code>repetition_penalty</code> are read from <code>[generation]</code> when using <code>generate.py --config ...</code>.</p> <p>RTX 4060 stable example:</p> <pre><code>[generation]\ntemperature = 0.6\ntop_k = 50\ntop_p = 0.9\nrepetition_penalty = 1.1\nuse_kv_cache = true\nstream = true\nsystem_prompt = \"You are LabCore LLM.\"\nmax_history_turns = 6\n</code></pre>"},{"location":"inference-and-demo/#reproducible-generation","title":"Reproducible Generation","text":"<ul> <li><code>seed</code>: fixes Python/NumPy/Torch RNGs for repeatable sampling.</li> <li><code>deterministic</code>: enables deterministic Torch algorithms (<code>warn_only=True</code>) and deterministic cuDNN settings.</li> </ul> <pre><code>[generation]\ntemperature = 0.6\ntop_k = 50\ntop_p = 0.9\nrepetition_penalty = 1.1\nuse_kv_cache = true\nstream = true\nsystem_prompt = \"You are LabCore LLM.\"\nmax_history_turns = 6\nseed = 1337\ndeterministic = true\n</code></pre>"},{"location":"inference-and-demo/#kv-cache-streaming-and-chat","title":"KV Cache, Streaming, and Chat","text":"<ul> <li>KV cache speeds up generation by reusing past attention keys/values instead of recomputing the whole context.</li> <li>Streaming updates the demo output incrementally as each token is sampled.</li> <li>Multi-turn chat builds prompts with simple text markers:</li> <li><code>&lt;|system|&gt;</code></li> <li><code>&lt;|user|&gt;</code></li> <li><code>&lt;|assistant|&gt;</code></li> <li><code>max_history_turns</code> keeps only the latest turns to bound prompt length.</li> </ul> <pre><code>[generation]\nuse_kv_cache = true\nstream = true\nsystem_prompt = \"You are LabCore LLM.\"\nmax_history_turns = 6\n</code></pre>"},{"location":"inference-and-demo/#output-files-artifacts-produced","title":"Output Files / Artifacts Produced","text":"<ul> <li>CLI: generated text in terminal output</li> <li>Demo: generated text in Gradio UI</li> <li>No new model files unless you explicitly export</li> </ul>"},{"location":"inference-and-demo/#common-errors","title":"Common Errors","text":"<ul> <li>Char tokenizer metadata missing: see Char vocab missing.</li> <li>Metadata path mismatch (<code>txt</code> vs <code>bin</code>): see Meta path mismatch.</li> <li>CUDA fallback behavior: see CUDA not detected.</li> </ul>"},{"location":"inference-and-demo/#next-related","title":"Next / Related","text":"<ul> <li>Export &amp; Deployment</li> <li>Fine-Tuning</li> <li>Troubleshooting</li> </ul>"},{"location":"operations/","title":"Operations","text":"<p>Use this page for operational folder hygiene, validation checks, and release/security workflows. Prerequisite: project dependencies installed in your active environment.</p>"},{"location":"operations/#artifact-directories","title":"Artifact Directories","text":"<ul> <li><code>data/</code>: prepared datasets, including <code>data/meta.json</code> for <code>bin</code> runs.</li> <li><code>checkpoints/</code>: training outputs (<code>ckpt_last.pt</code>, <code>train_log.json</code>).</li> <li><code>outputs/</code>: exported artifacts (<code>hf_export/</code>, <code>gguf/</code>, fine-tuning outputs).</li> <li><code>runs/</code>: optional experiment logs or external tracker exports (not auto-created by core scripts).</li> </ul> <p>Note</p> <p>The core training scripts write to <code>checkpoints/</code> and <code>outputs/</code>. Keep those directories versioned in your run notes, but do not commit large artifacts.</p>"},{"location":"operations/#commands","title":"Command(s)","text":"<p>Local quality checks:</p> <pre><code>python -m pytest -q\nruff check src scripts tests train.py generate.py demo_gradio.py --select E9,F63,F7,F82\n</code></pre> <p>Docs build checks:</p> <pre><code>python -m pip install -r docs/requirements.txt\npython -m mkdocs build\n</code></pre>"},{"location":"operations/#output-files-artifacts-produced","title":"Output Files / Artifacts Produced","text":"<ul> <li>Test and lint logs in terminal output</li> <li>Built docs in <code>site/</code> after <code>mkdocs build</code></li> <li>CI pipelines: <code>.github/workflows/ci.yml</code> and <code>.github/workflows/docs.yml</code></li> </ul>"},{"location":"operations/#security-and-release","title":"Security and Release","text":"<p>Security reporting (see <code>SECURITY.md</code>):</p> <ul> <li>Use GitHub Security Advisories for vulnerabilities</li> <li>Include impact, affected components, and reproduction steps</li> </ul> <p>Release flow (see <code>RELEASE.md</code>):</p> <ol> <li>Confirm CI is green.</li> <li>Run local validation commands.</li> <li>Update version and changelog.</li> <li>Tag and publish the release.</li> </ol>"},{"location":"operations/#common-errors","title":"Common Errors","text":"<ul> <li>Missing dependencies: see Torch not installed.</li> <li>Metadata and path confusion: see Meta path mismatch.</li> <li>CUDA expected but unavailable: see CUDA not detected.</li> </ul>"},{"location":"operations/#related","title":"Related","text":"<ul> <li>Troubleshooting</li> <li>Benchmarks</li> <li>Developer Guide</li> </ul>"},{"location":"training/","title":"Training","text":"<p>Use this page to launch, monitor, and checkpoint training runs with consistent settings. Prerequisites: prepared dataset and metadata from Data Pipeline.</p>"},{"location":"training/#commands","title":"Command(s)","text":"<p>Reference training command:</p> <pre><code>python train.py --config configs/base.toml --tokenizer char --max-iters 5000\n</code></pre> <p>Device override examples:</p> <pre><code>python train.py --config configs/base.toml --tokenizer char --max-iters 5000 --device cpu\npython train.py --config configs/base.toml --tokenizer char --max-iters 5000 --device cuda\n</code></pre> <p><code>train.py</code> flags:</p> <ul> <li><code>--config</code>: TOML preset path (<code>CONFIG_EXAMPLE = configs/base.toml</code>)</li> <li><code>--max-iters</code>: runtime override for total iterations</li> <li><code>--device</code>: <code>cpu</code> or <code>cuda</code></li> <li><code>--tokenizer</code>: <code>char</code> or <code>bpe</code></li> </ul>"},{"location":"training/#precision-and-gradient-accumulation","title":"Precision and Gradient Accumulation","text":"<p>Configure these in <code>[training]</code>:</p> <ul> <li><code>grad_accum_steps</code>: gradient accumulation factor (default <code>1</code>)</li> <li><code>precision</code>: <code>fp32</code> (default), <code>fp16</code>, or <code>bf16</code></li> </ul> <p><code>effective_batch_size = batch_size * grad_accum_steps</code></p> <p>Mixed precision is enabled only when <code>device = \"cuda\"</code> and <code>precision != \"fp32\"</code>. On CPU, training falls back to <code>fp32</code>.</p> <p>RTX 4060 example:</p> <pre><code>[training]\nbatch_size = 8\ngrad_accum_steps = 4\nprecision = \"fp16\"\n</code></pre>"},{"location":"training/#best-checkpoint-early-stopping","title":"Best Checkpoint &amp; Early Stopping","text":"<ul> <li><code>save_best</code> saves <code>checkpoints/ckpt_best.pt</code> whenever validation loss improves by at least <code>early_stopping_min_delta</code>.</li> <li><code>early_stopping</code> is disabled by default.</li> <li><code>early_stopping_patience</code> counts evaluation rounds without sufficient validation improvement.</li> <li><code>early_stopping_min_delta</code> defines the minimum improvement threshold on <code>val_loss</code>.</li> </ul> <pre><code>[training]\nearly_stopping = true\nearly_stopping_patience = 3\nearly_stopping_min_delta = 0.001\nsave_best = true\n</code></pre>"},{"location":"training/#data_format-and-metadata-mapping","title":"<code>data_format</code> and Metadata Mapping","text":"Training mode Config value Data artifacts expected Metadata path Text pipeline <code>training.data_format = \"txt\"</code> <code>data/processed/train.txt</code> + <code>data/processed/val.txt</code> (or <code>.npy</code>) <code>data/processed/meta.json</code> (<code>META_TXT</code>) Binary pipeline <code>training.data_format = \"bin\"</code> <code>data/train.bin</code> + <code>data/val.bin</code> <code>data/meta.json</code> (<code>META_BIN</code>) <p>Note</p> <p>For binary mode, if <code>data.processed_dir</code> points to <code>data/processed</code>, <code>train.py</code> automatically checks the parent directory (<code>data/</code>) for <code>train.bin</code> and <code>val.bin</code>.</p>"},{"location":"training/#checkpointing-and-resume-behavior","title":"Checkpointing and Resume Behavior","text":"<p>Produced during training:</p> <ul> <li><code>checkpoints/ckpt_last.pt</code> (<code>CHECKPOINT</code>)</li> <li><code>checkpoints/ckpt_best.pt</code> (when <code>save_best = true</code>)</li> <li><code>checkpoints/train_log.json</code></li> </ul> <p>Warning</p> <p>Native resume-from-checkpoint is not implemented in <code>train.py</code> yet. <code>ckpt_last.pt</code> is for inference/export compatibility and state inspection, not automatic continuation.</p>"},{"location":"training/#common-errors","title":"Common Errors","text":"<ul> <li>Binary shards missing: see Binary shards not found.</li> <li>Metadata path mismatch: see Meta path mismatch.</li> <li>Vocab inference failure: see Char vocab missing.</li> <li>CUDA fallback warning: see CUDA not detected.</li> </ul>"},{"location":"training/#next-related","title":"Next / Related","text":"<ul> <li>Inference &amp; Demo</li> <li>Export &amp; Deployment</li> <li>Troubleshooting</li> </ul>"},{"location":"troubleshooting/","title":"Troubleshooting","text":"<p>Use this page for fast diagnosis of common setup, data, and runtime failures. All anchors below are referenced from guide pages.</p>"},{"location":"troubleshooting/#setup","title":"Setup","text":""},{"location":"troubleshooting/#torch-not-installed","title":"Torch not installed","text":"<p>Symptoms: <code>ModuleNotFoundError: torch</code>, or scripts fail on import.</p> <p>Fix:</p> <pre><code>python -m pip install -e \".[torch,dev]\"\n</code></pre> <p>Use the official selector if you need a specific CUDA build: https://pytorch.org/get-started/locally/</p>"},{"location":"troubleshooting/#cuda-not-detected","title":"CUDA not detected","text":"<p>Symptoms: <code>torch.cuda.is_available()</code> returns <code>False</code>, or scripts print CPU fallback warnings.</p> <p>Quick check:</p> <pre><code>python -c \"import torch; print(torch.cuda.is_available()); print(torch.version.cuda)\"\n</code></pre> <p>Checks:</p> <ul> <li>NVIDIA driver installed and up to date</li> <li>Correct PyTorch build for your CUDA runtime</li> <li>Same Python environment used for install and execution</li> </ul>"},{"location":"troubleshooting/#data-and-metadata","title":"Data and Metadata","text":""},{"location":"troubleshooting/#meta-path-mismatch","title":"Meta path mismatch","text":"<p>Symptoms: generation/training behaves incorrectly or cannot load tokenizer metadata.</p> <p>Expected mapping:</p> <ul> <li><code>txt</code> pipeline -&gt; <code>data/processed/meta.json</code></li> <li><code>bin</code> pipeline -&gt; <code>data/meta.json</code></li> </ul>"},{"location":"troubleshooting/#binary-shards-not-found","title":"Binary shards not found","text":"<p>Symptoms: <code>Binary shards not found</code> during <code>training.data_format = \"bin\"</code>.</p> <p>Fix:</p> <pre><code>python scripts/prepare_data.py --dataset tinyshakespeare --tokenizer char --output-format bin --output-dir data/processed\n</code></pre>"},{"location":"troubleshooting/#char-vocab-missing","title":"Char vocab missing","text":"<p>Symptoms: <code>Char tokenizer requires vocab in meta.json</code>.</p> <p>Fix:</p> <pre><code>python scripts/prepare_data.py --dataset tinyshakespeare --tokenizer char --output-format txt --output-dir data/processed\n</code></pre> <p>Then pass the matching <code>--meta data/processed/meta.json</code> to generation/export commands.</p>"},{"location":"troubleshooting/#runtime","title":"Runtime","text":""},{"location":"troubleshooting/#oom-errors","title":"Out of memory","text":"<p>Reduce in this order:</p> <ol> <li><code>training.batch_size</code></li> <li><code>model.block_size</code></li> <li><code>training.gradient_accumulation_steps</code></li> <li>Model size / preset complexity</li> </ol>"},{"location":"troubleshooting/#flashattention-not-available","title":"FlashAttention not available","text":"<p>LabCore falls back automatically:</p> <ul> <li>FlashAttention (preferred, if available)</li> <li>PyTorch SDPA fallback</li> <li>Standard causal attention fallback</li> </ul>"},{"location":"troubleshooting/#windows-path-policy","title":"Windows path and policy issues","text":"<ul> <li>Activate your venv before commands.</li> <li>Run commands from repository root.</li> <li>Quote paths when directories contain spaces.</li> <li>If PowerShell policy blocks scripts, use <code>python ...</code> commands directly.</li> </ul>"},{"location":"fr/","title":"LabCore LLM","text":"<p>Cette documentation est le guide op\u00e9rationnel pour ex\u00e9cuter LabCore de bout en bout: pr\u00e9paration des donn\u00e9es, entra\u00eenement, inf\u00e9rence et export. Les pages EN dans <code>docs/</code> restent la source de v\u00e9rit\u00e9, et les pages FR dans <code>docs/fr/</code> sont des miroirs complets.</p> <p> </p>"},{"location":"fr/#preset-de-reference-utilise-dans-la-documentation","title":"Preset de r\u00e9f\u00e9rence utilis\u00e9 dans la documentation","text":"<p>Tous les exemples utilisent cette base:</p> <ul> <li>Dataset: <code>tinyshakespeare</code></li> <li>Tokenizer: <code>char</code></li> <li><code>CONFIG_EXAMPLE = configs/base.toml</code></li> <li>Override d'entra\u00eenement standard: <code>--max-iters 5000</code></li> <li><code>CHECKPOINT = checkpoints/ckpt_last.pt</code></li> <li><code>META_TXT = data/processed/meta.json</code></li> <li><code>META_BIN = data/meta.json</code></li> </ul> <p>Tip</p> <p>Gardez ces valeurs pour votre premier run complet. La plupart des erreurs viennent d'un mauvais alignement checkpoint/m\u00e9tadonn\u00e9es.</p>"},{"location":"fr/#installation-rapide","title":"Installation rapide","text":"<pre><code>python -m pip install -e \".[torch,dev]\"\n</code></pre> <p>Pour l'export Hugging Face et l'interface Gradio:</p> <pre><code>python -m pip install -e \".[torch,hf,demo]\"\n</code></pre>"},{"location":"fr/#commandes-de-demarrage-rapide","title":"Commandes de d\u00e9marrage rapide","text":"<pre><code>python scripts/prepare_data.py --dataset tinyshakespeare --tokenizer char --output-format txt --output-dir data/processed\npython train.py --config configs/base.toml --tokenizer char --max-iters 5000\npython generate.py --checkpoint checkpoints/ckpt_last.pt --meta data/processed/meta.json --tokenizer char --prompt \"To be\"\n</code></pre> <p>Artefacts attendus:</p> <ul> <li><code>checkpoints/ckpt_last.pt</code></li> <li><code>checkpoints/train_log.json</code></li> <li><code>data/processed/meta.json</code></li> </ul>"},{"location":"fr/#flux-de-bout-en-bout","title":"Flux de bout en bout","text":"<pre><code>prepare_data.py -&gt; train.py -&gt; generate.py/demo_gradio.py -&gt; export_hf.py -&gt; quantize_gguf.py\n</code></pre>"},{"location":"fr/#plan-de-documentation","title":"Plan de documentation","text":""},{"location":"fr/#guides","title":"Guides","text":"<ul> <li>D\u00e9marrage: setup de l'environnement et premier run reproductible.</li> <li>Pipeline de donn\u00e9es: cr\u00e9ation des donn\u00e9es <code>txt</code> ou <code>bin</code> et des m\u00e9tadonn\u00e9es.</li> <li>Entra\u00eenement: entra\u00eenement, checkpointing et mapping des formats.</li> <li>Inf\u00e9rence et d\u00e9mo: g\u00e9n\u00e9ration CLI et d\u00e9mo Gradio.</li> <li>Ajustement fin: workflow LoRA instruction tuning.</li> <li>Export et d\u00e9ploiement: export HF et conversion GGUF.</li> </ul>"},{"location":"fr/#reference","title":"R\u00e9f\u00e9rence","text":"<ul> <li>Configuration: r\u00e9f\u00e9rence compl\u00e8te des cl\u00e9s TOML.</li> <li>Op\u00e9rations: hygi\u00e8ne op\u00e9rationnelle, release et checks.</li> <li>D\u00e9pannage: correctifs rapides avec ancres.</li> <li>Performances: benchmark d'inf\u00e9rence (<code>tok/s</code>, pic VRAM) et reporting.</li> </ul>"},{"location":"fr/#developpement","title":"D\u00e9veloppement","text":"<ul> <li>Guide d\u00e9veloppeur: workflow contributeur et commandes de validation.</li> </ul>"},{"location":"fr/#suite-liens","title":"Suite / liens","text":"<ul> <li>D\u00e9marrage</li> <li>Pipeline de donn\u00e9es</li> <li>Entra\u00eenement</li> </ul>"},{"location":"fr/benchmarks/","title":"Performances","text":"<p>Utilisez le script de benchmark d'inf\u00e9rence pour mesurer de mani\u00e8re reproductible les <code>tokens/s</code> et le pic de VRAM. Le script prend en charge les checkpoints locaux et les d\u00e9p\u00f4ts Hugging Face, avec export JSON et Markdown.</p>"},{"location":"fr/benchmarks/#script-de-benchmark-dinference","title":"Script de benchmark d'inf\u00e9rence","text":"<pre><code>python scripts/benchmark_infer.py --help\n</code></pre> <p>Le runtime par d\u00e9faut reste court (warmup + 3 ex\u00e9cutions mesur\u00e9es).</p>"},{"location":"fr/benchmarks/#exemple-local","title":"Exemple local","text":"<pre><code>python scripts/benchmark_infer.py \\\n  --source local \\\n  --checkpoint checkpoints/ckpt_last.pt \\\n  --meta data/processed/meta.json \\\n  --tokenizer char \\\n  --config configs/base.toml \\\n  --device cpu \\\n  --json-out outputs/bench_infer.json \\\n  --md-out outputs/bench_infer.md\n</code></pre>"},{"location":"fr/benchmarks/#exemple-hugging-face","title":"Exemple Hugging Face","text":"<pre><code>python scripts/benchmark_infer.py \\\n  --source hf \\\n  --repo-id LabCoreAI/&lt;id&gt; \\\n  --config configs/base.toml \\\n  --device cuda \\\n  --json-out outputs/bench_infer_hf.json \\\n  --md-out outputs/bench_infer_hf.md\n</code></pre>"},{"location":"fr/benchmarks/#mesures-effectuees","title":"Mesures effectu\u00e9es","text":"<ul> <li>G\u00e9n\u00e9ration de warmup (<code>--warmup-tokens</code>, non compt\u00e9e dans le d\u00e9bit final).</li> <li>G\u00e9n\u00e9ration mesur\u00e9e (<code>--gen-tokens</code>) r\u00e9p\u00e9t\u00e9e <code>--iters</code> fois.</li> <li>R\u00e9sum\u00e9 du d\u00e9bit: <code>mean</code>, <code>min</code>, <code>max</code> tokens/sec.</li> <li>Pic de VRAM (<code>torch.cuda.max_memory_allocated</code>) en ex\u00e9cution CUDA.</li> </ul> <p>Les r\u00e9glages de reproductibilit\u00e9 sont lus dans <code>[generation]</code> quand <code>--config</code> est fourni:</p> <ul> <li><code>seed</code> (graine d'initialisation)</li> <li><code>deterministic</code></li> <li>r\u00e9glages de sampling (<code>temperature</code>, <code>top_k</code>, <code>top_p</code>, <code>repetition_penalty</code>)</li> <li><code>use_kv_cache</code> (sauf surcharge via flags CLI)</li> </ul>"},{"location":"fr/benchmarks/#schema-de-sortie-json-resume","title":"Sch\u00e9ma de sortie JSON (r\u00e9sum\u00e9)","text":"<pre><code>{\n  \"timestamp\": \"...\",\n  \"commit\": \"...\",\n  \"platform\": {\"os\": \"...\", \"python\": \"...\"},\n  \"torch\": {\"version\": \"...\", \"cuda\": \"...\"},\n  \"device\": {\"type\": \"cpu|cuda\", \"name\": \"...\"},\n  \"model\": {\"source\": \"local|hf\", \"params_m\": 0.0, \"block_size\": 0, \"n_layer\": 0, \"n_head\": 0, \"n_embd\": 0},\n  \"generation\": {\"prompt\": \"...\", \"gen_tokens\": 256, \"temperature\": 0.9, \"top_k\": 40, \"top_p\": 1.0, \"repetition_penalty\": 1.0, \"use_kv_cache\": true},\n  \"results\": {\"iters\": 3, \"tokens_per_sec\": {\"mean\": 0.0, \"min\": 0.0, \"max\": 0.0}, \"vram_peak_mib\": null}\n}\n</code></pre>"},{"location":"fr/benchmarks/#resultats-communaute","title":"R\u00e9sultats communaut\u00e9","text":"<p>Collez la ligne Markdown g\u00e9n\u00e9r\u00e9e (via <code>--md-out</code> ou la sortie terminal) dans ce tableau. Ajoutez le JSON dans la description de PR si disponible.</p> P\u00e9riph\u00e9rique Source Taille du mod\u00e8le (params M) KV-cache gen_tokens mean tok/s pic VRAM MiB votre r\u00e9sultat local/hf 0.000 on/off 256 0.00 N/A ou valeur"},{"location":"fr/configuration-reference/","title":"R\u00e9f\u00e9rence de configuration","text":"<p>Utilisez cette page comme r\u00e9f\u00e9rence unique des cl\u00e9s TOML utilis\u00e9es par <code>train.py</code> et des valeurs par d\u00e9faut. Pr\u00e9requis: conna\u00eetre le preset de r\u00e9f\u00e9rence (<code>configs/base.toml</code>).</p>"},{"location":"fr/configuration-reference/#valeurs-canoniques-utilisees-dans-les-guides","title":"Valeurs canoniques utilis\u00e9es dans les guides","text":"<ul> <li><code>CONFIG_EXAMPLE = configs/base.toml</code></li> <li><code>CHECKPOINT = checkpoints/ckpt_last.pt</code></li> <li><code>META_TXT = data/processed/meta.json</code></li> <li><code>META_BIN = data/meta.json</code></li> </ul>"},{"location":"fr/configuration-reference/#reference-des-sections-et-des-cles","title":"R\u00e9f\u00e9rence des sections et des cl\u00e9s","text":""},{"location":"fr/configuration-reference/#general","title":"<code>[general]</code>","text":"Cl\u00e9 Type Valeur typique Notes <code>run_name</code> string <code>\"tiny_char_baseline\"</code> Label informatif du run. <code>seed</code> int <code>1337</code> <code>seed</code> (graine d'initialisation) optionnelle. <code>tokenizer</code> string <code>\"char\"</code> ou <code>\"bpe\"</code> Valeur par d\u00e9faut: <code>\"char\"</code> si omis."},{"location":"fr/configuration-reference/#data","title":"<code>[data]</code>","text":"Cl\u00e9 Type Valeur typique Notes <code>dataset</code> string <code>\"tinyshakespeare\"</code> Label des m\u00e9tadonn\u00e9es. <code>processed_dir</code> string path <code>\"data/processed\"</code> Valeur par d\u00e9faut: <code>\"data/processed\"</code> si omis."},{"location":"fr/configuration-reference/#model","title":"<code>[model]</code>","text":"Cl\u00e9 Type Valeur typique Notes <code>vocab_size</code> int <code>65</code> (char) / <code>50257</code> (bpe) Peut \u00eatre inf\u00e9r\u00e9 via metadata/tokenizer si omis. <code>block_size</code> int <code>512</code> Longueur de s\u00e9quence. <code>n_layer</code> int <code>6</code> Profondeur du Transformer. <code>n_head</code> int <code>8</code> Nombre de t\u00eates d'attention. <code>n_embd</code> int <code>256</code> Taille des embeddings. <code>dropout</code> float <code>0.1</code> Taux de dropout. <code>bias</code> bool <code>true</code> Active/d\u00e9sactive le biais lin\u00e9aire. <code>use_rope</code> bool <code>false</code>/<code>true</code> Valeur par d\u00e9faut: <code>false</code>. <code>use_flash</code> bool <code>false</code>/<code>true</code> Valeur par d\u00e9faut: <code>false</code>."},{"location":"fr/configuration-reference/#optimizer","title":"<code>[optimizer]</code>","text":"Cl\u00e9 Type Valeur typique Notes <code>learning_rate</code> float <code>3e-4</code> Fallback vers <code>[training]</code> si absent. <code>weight_decay</code> float <code>0.01</code> Fallback vers <code>[training]</code> si absent. <code>beta1</code> float <code>0.9</code> AdamW beta1. <code>beta2</code> float <code>0.95</code> AdamW beta2. <code>grad_clip</code> float <code>1.0</code> Fallback vers <code>[training]</code> si absent."},{"location":"fr/configuration-reference/#training","title":"<code>[training]</code>","text":"Cl\u00e9 Type Valeur typique Notes <code>batch_size</code> int <code>8</code> Taille de batch (micro-batch) par step. <code>gradient_accumulation_steps</code> int <code>1</code> \u00e0 <code>8</code> Accumulation de gradients pour augmenter la taille de batch effective. <code>max_iters</code> int <code>5000</code> Valeur de r\u00e9f\u00e9rence de la documentation. <code>warmup_iters</code> int <code>0</code> \u00e0 <code>500</code> Steps de warmup du learning rate. <code>lr_decay_iters</code> int <code>5000</code> Souvent align\u00e9 sur <code>max_iters</code>. <code>min_lr</code> float <code>3e-5</code> Plancher du learning rate (cosine decay). <code>eval_interval</code> int <code>200</code> Cadence d'\u00e9valuation/checkpoint. <code>eval_iters</code> int <code>50</code> Nombre de batches de validation par \u00e9valuation. <code>log_interval</code> int <code>20</code> Cadence des logs console. <code>save_interval</code> int <code>200</code> ou <code>500</code> Cadence de sauvegarde suppl\u00e9mentaire. <code>device</code> string <code>\"cuda\"</code> ou <code>\"cpu\"</code> Surcharge possible via CLI <code>--device</code>. <code>checkpoint_dir</code> string path <code>\"checkpoints\"</code> Produit <code>ckpt_last.pt</code> et <code>train_log.json</code>. <code>data_format</code> string <code>\"txt\"</code> ou <code>\"bin\"</code> Valeur par d\u00e9faut: <code>\"txt\"</code> si omis. <code>early_stopping</code> bool <code>false</code> Active l'arr\u00eat anticip\u00e9 sur la loss de validation. <code>early_stopping_patience</code> int <code>5</code> Nombre d'\u00e9valuations sans am\u00e9lioration avant arr\u00eat. <code>early_stopping_min_delta</code> float <code>0.0</code> Gain minimal de loss de validation pour compter une am\u00e9lioration. <code>save_best</code> bool <code>true</code> Sauvegarde <code>ckpt_best.pt</code> sur nouvelle meilleure loss de validation."},{"location":"fr/configuration-reference/#generation","title":"<code>[generation]</code>","text":"Cl\u00e9 Type Valeur typique Notes <code>max_new_tokens</code> int <code>200</code> Valeur pratique pour les scripts docs. <code>temperature</code> float <code>0.6</code> \u00e0 <code>0.9</code> Contr\u00f4le de l'al\u00e9atoire du sampling. <code>top_k</code> int <code>40</code> \u00e0 <code>50</code> Troncature du sampling. <code>top_p</code> float <code>0.9</code> \u00e0 <code>1.0</code> Seuil nucleus sampling. <code>repetition_penalty</code> float <code>1.0</code> \u00e0 <code>1.2</code> P\u00e9nalit\u00e9 des tokens r\u00e9p\u00e9t\u00e9s. <code>use_kv_cache</code> bool <code>true</code> Active le KV-cache en inf\u00e9rence. <code>stream</code> bool <code>true</code> Active le streaming token-by-token en d\u00e9mo. <code>system_prompt</code> string <code>\"\"</code> Prompt syst\u00e8me du chat minimal. <code>max_history_turns</code> int <code>6</code> Nombre max de tours d'historique chat. <code>seed</code> int or null <code>1337</code> <code>seed</code> (graine d'initialisation) pour la reproductibilit\u00e9. <code>deterministic</code> bool <code>false</code> Active les algorithmes d\u00e9terministes PyTorch."},{"location":"fr/configuration-reference/#valeurs-par-defaut-appliquees-automatiquement","title":"Valeurs par d\u00e9faut appliqu\u00e9es automatiquement","text":"<p><code>load_config</code> garantit:</p> <ul> <li><code>general.tokenizer = \"char\"</code></li> <li><code>data.processed_dir = \"data/processed\"</code></li> <li><code>model.use_rope = false</code></li> <li><code>model.use_flash = false</code></li> <li><code>training.data_format = \"txt\"</code></li> <li><code>training.early_stopping = false</code></li> <li><code>training.early_stopping_patience = 5</code></li> <li><code>training.early_stopping_min_delta = 0.0</code></li> <li><code>training.save_best = true</code></li> <li><code>generation.top_p = 1.0</code></li> <li><code>generation.repetition_penalty = 1.0</code></li> <li><code>generation.use_kv_cache = true</code></li> <li><code>generation.stream = true</code></li> <li><code>generation.system_prompt = \"\"</code></li> <li><code>generation.max_history_turns = 6</code></li> <li><code>generation.seed = null</code></li> <li><code>generation.deterministic = false</code></li> </ul>"},{"location":"fr/configuration-reference/#exemple-configuration-minimale","title":"Exemple: configuration minimale","text":"<pre><code>[general]\ntokenizer = \"char\"\n\n[data]\ndataset = \"tinyshakespeare\"\nprocessed_dir = \"data/processed\"\n\n[model]\nblock_size = 256\nn_layer = 4\nn_head = 4\nn_embd = 192\nvocab_size = 65\ndropout = 0.1\nbias = true\nuse_rope = false\nuse_flash = false\n\n[training]\nbatch_size = 8\ngradient_accumulation_steps = 1\nmax_iters = 2000\neval_interval = 200\neval_iters = 50\nlog_interval = 20\ndevice = \"cpu\"\ncheckpoint_dir = \"checkpoints\"\ndata_format = \"txt\"\n</code></pre>"},{"location":"fr/configuration-reference/#exemple-point-de-depart-rtx-4060-8gb-a-valider-localement","title":"Exemple: point de d\u00e9part RTX 4060 (8GB, \u00e0 valider localement)","text":"<pre><code>[general]\ntokenizer = \"char\"\n\n[data]\ndataset = \"tinyshakespeare\"\nprocessed_dir = \"data/processed\"\n\n[model]\nblock_size = 512\nn_layer = 6\nn_head = 8\nn_embd = 256\ndropout = 0.1\nbias = true\nuse_rope = false\nuse_flash = false\n\n[optimizer]\nlearning_rate = 3e-4\nweight_decay = 0.01\nbeta1 = 0.9\nbeta2 = 0.95\ngrad_clip = 1.0\n\n[training]\nbatch_size = 8\ngradient_accumulation_steps = 2\nmax_iters = 5000\nwarmup_iters = 200\nlr_decay_iters = 5000\nmin_lr = 3e-5\neval_interval = 200\neval_iters = 50\nlog_interval = 20\nsave_interval = 200\ndevice = \"cuda\"\ncheckpoint_dir = \"checkpoints\"\ndata_format = \"txt\"\n</code></pre> <p>Note</p> <p>Le bloc RTX 4060 est un point de d\u00e9part, pas un profil benchmark garanti. Ajustez selon votre VRAM et votre stack driver.</p>"},{"location":"fr/configuration-reference/#liens","title":"Liens","text":"<ul> <li>Entra\u00eenement</li> <li>Op\u00e9rations</li> <li>Performances</li> </ul>"},{"location":"fr/data-pipeline/","title":"Pipeline de donn\u00e9es","text":"<p>Utilisez cette page pour pr\u00e9parer les donn\u00e9es et les m\u00e9tadonn\u00e9es avec une structure de sortie pr\u00e9visible. Pr\u00e9requis: d\u00e9pendances install\u00e9es depuis D\u00e9marrage.</p>"},{"location":"fr/data-pipeline/#commandes","title":"Commandes","text":"<p>Pipeline <code>txt</code> de r\u00e9f\u00e9rence (utilis\u00e9 par <code>configs/base.toml</code>):</p> <pre><code>python scripts/prepare_data.py \\\n  --dataset tinyshakespeare \\\n  --tokenizer char \\\n  --output-format txt \\\n  --raw-dir data/raw \\\n  --output-dir data/processed \\\n  --val-ratio 0.1\n</code></pre> <p>Pipeline <code>bin</code> alternatif:</p> <pre><code>python scripts/prepare_data.py \\\n  --dataset tinyshakespeare \\\n  --tokenizer char \\\n  --output-format bin \\\n  --raw-dir data/raw \\\n  --output-dir data/processed \\\n  --val-ratio 0.1\n</code></pre>"},{"location":"fr/data-pipeline/#fichiers-de-sortie-artefacts-produits","title":"Fichiers de sortie / artefacts produits","text":"<p>Format <code>txt</code> (<code>output-dir = data/processed</code>):</p> <ul> <li><code>data/processed/train.txt</code></li> <li><code>data/processed/val.txt</code></li> <li><code>data/processed/corpus.txt</code></li> <li><code>data/processed/train.npy</code></li> <li><code>data/processed/val.npy</code></li> <li><code>data/processed/meta.json</code> (<code>META_TXT</code>)</li> </ul> <p>Format <code>bin</code>:</p> <ul> <li><code>data/train.bin</code></li> <li><code>data/val.bin</code></li> <li><code>data/meta.json</code> (<code>META_BIN</code>)</li> </ul> <p>Note</p> <p>Avec <code>--output-format bin</code>, si <code>--output-dir</code> se termine par <code>processed</code>, les fichiers binaires sont \u00e9crits dans le parent (<code>data/</code>).</p>"},{"location":"fr/data-pipeline/#selection-du-format","title":"S\u00e9lection du format","text":"<ul> <li>Utilisez <code>txt</code> avec <code>training.data_format = \"txt\"</code> et la m\u00e9tadonn\u00e9e <code>data/processed/meta.json</code>.</li> <li>Utilisez <code>bin</code> avec <code>training.data_format = \"bin\"</code> et la m\u00e9tadonn\u00e9e <code>data/meta.json</code>.</li> </ul>"},{"location":"fr/data-pipeline/#erreurs-frequentes","title":"Erreurs fr\u00e9quentes","text":"<ul> <li>Binaires manquants: voir Binary shards not found.</li> <li>Mauvais chemin de m\u00e9tadonn\u00e9es: voir Meta path mismatch.</li> <li>Probl\u00e8me de vocabulaire char: voir Char vocab missing.</li> </ul>"},{"location":"fr/data-pipeline/#suite-liens","title":"Suite / liens","text":"<ul> <li>Entra\u00eenement</li> <li>Inf\u00e9rence et d\u00e9mo</li> <li>D\u00e9pannage</li> </ul>"},{"location":"fr/developer-guide/","title":"Guide d\u00e9veloppeur","text":"<p>Cette page cible les contributeurs qui travaillent sur le code du projet.</p>"},{"location":"fr/developer-guide/#structure-du-depot","title":"Structure du d\u00e9p\u00f4t","text":"<pre><code>src/labcore_llm/\n  config/      # loader TOML et defaults\n  data/        # abstractions dataset\n  model/       # implementation GPT\n  tokenizer/   # tokenizers char + BPE\n  trainer/     # boucle training, scheduler, checkpointing\n\nscripts/       # helpers data prep, export, quantize, fine-tune\nconfigs/       # presets TOML\ntests/         # tests unitaires\n</code></pre>"},{"location":"fr/developer-guide/#environnement-de-developpement-local","title":"Environnement de d\u00e9veloppement local","text":"<pre><code>python -m venv .venv\n## PowerShell\n.\\.venv\\Scripts\\Activate.ps1\npython -m pip install --upgrade pip\npython -m pip install -e \".[torch,dev]\"\n</code></pre>"},{"location":"fr/developer-guide/#commandes-de-validation","title":"Commandes de validation","text":"<p>Lancer les tests:</p> <pre><code>python -m pytest -q\n</code></pre> <p>Lancer le lint align\u00e9 sur la CI:</p> <pre><code>ruff check src scripts tests train.py generate.py demo_gradio.py --select E9,F63,F7,F82\n</code></pre>"},{"location":"fr/developer-guide/#workflows-ci","title":"Workflows CI","text":"<ul> <li><code>.github/workflows/ci.yml</code>: lint + tests</li> <li><code>.github/workflows/docs.yml</code>: build et d\u00e9ploiement MkDocs</li> </ul>"},{"location":"fr/developer-guide/#niveau-de-qualite-des-contributions","title":"Niveau de qualit\u00e9 des contributions","text":"<ul> <li>Gardez les commits focalis\u00e9s et atomiques.</li> <li>Mettez \u00e0 jour la documentation lors des changements de comportement/CLI.</li> <li>Ajoutez des tests pour les corrections de bugs et les nouvelles logiques.</li> <li>Ne commitez pas de gros artefacts de donn\u00e9es/mod\u00e8les.</li> </ul>"},{"location":"fr/developer-guide/#notes-de-packaging","title":"Notes de packaging","text":"<ul> <li>Le projet utilise une structure <code>src/</code> avec setuptools.</li> <li>Les groupes de d\u00e9pendances optionnelles sont dans <code>pyproject.toml</code>.</li> <li>Les scripts d'entr\u00e9e sont des fichiers Python, pas des wrappers console-script.</li> </ul>"},{"location":"fr/export-and-deployment/","title":"Export et d\u00e9ploiement","text":"<p>Utilisez cette page pour packager un checkpoint local vers Hugging Face puis, optionnellement, vers GGUF. Pr\u00e9requis: checkpoint valide (<code>checkpoints/ckpt_last.pt</code>) et m\u00e9tadonn\u00e9es correspondantes (<code>data/processed/meta.json</code> ou <code>data/meta.json</code>).</p>"},{"location":"fr/export-and-deployment/#export-hf-vs-export-gguf","title":"Export HF vs export GGUF","text":"<ul> <li>L'export HF cr\u00e9e des artefacts standards pour les workflows Hugging Face.</li> <li>La conversion GGUF cr\u00e9e des fichiers quantifi\u00e9s pour les runtimes <code>llama.cpp</code>.</li> </ul>"},{"location":"fr/export-and-deployment/#commandes","title":"Commandes","text":"<p>Exporter un checkpoint local au format HF:</p> <pre><code>python scripts/export_hf.py \\\n  --checkpoint checkpoints/ckpt_last.pt \\\n  --meta data/processed/meta.json \\\n  --output-dir outputs/hf_export\n</code></pre> <p>Pousser le dossier export\u00e9 vers HF Hub:</p> <pre><code>python scripts/export_hf.py \\\n  --checkpoint checkpoints/ckpt_last.pt \\\n  --meta data/processed/meta.json \\\n  --output-dir outputs/hf_export \\\n  --push \\\n  --repo-id GhostPunishR/labcore-llm-50M\n</code></pre> <p>Convertir l'export HF en GGUF et quantifier:</p> <pre><code>python scripts/quantize_gguf.py \\\n  --hf-dir outputs/hf_export \\\n  --llama-cpp-dir third_party/llama.cpp \\\n  --output-dir outputs/gguf \\\n  --quant-type Q4_K_M\n</code></pre>"},{"location":"fr/export-and-deployment/#fichiers-de-sortie-artefacts-produits","title":"Fichiers de sortie / artefacts produits","text":"<p><code>outputs/hf_export/</code>:</p> <ul> <li><code>model.safetensors</code></li> <li><code>config.json</code></li> <li><code>tokenizer.json</code></li> <li><code>README.md</code></li> </ul> <p><code>outputs/gguf/</code>:</p> <ul> <li><code>labcore-50m-f16.gguf</code></li> <li><code>labcore-50m-q4_k_m.gguf</code> (ou <code>q5_k_m</code> / les deux avec <code>--quant-type all</code>)</li> </ul> <p>Warning</p> <p>La conversion GGUF exige un checkout <code>llama.cpp</code> valide avec script de conversion et binaire de quantization disponibles.</p>"},{"location":"fr/export-and-deployment/#erreurs-frequentes","title":"Erreurs fr\u00e9quentes","text":"<ul> <li>Mauvais mapping des m\u00e9tadonn\u00e9es (<code>txt</code> vs <code>bin</code>): voir Meta path mismatch.</li> <li>D\u00e9pendances <code>huggingface_hub</code>/<code>safetensors</code> manquantes: voir Torch not installed.</li> <li>Outils <code>llama.cpp</code> manquants: voir Windows path and policy issues.</li> </ul>"},{"location":"fr/export-and-deployment/#suite-liens","title":"Suite / liens","text":"<ul> <li>Ajustement fin</li> <li>Op\u00e9rations</li> <li>D\u00e9pannage</li> </ul>"},{"location":"fr/fine-tuning/","title":"Ajustement fin","text":"<p>Utilisez cette page pour du LoRA instruction tuning sur des checkpoints CausalLM compatibles HF. Pr\u00e9requis: d\u00e9pendances HF + finetune et mod\u00e8le de base accessible.</p>"},{"location":"fr/fine-tuning/#commandes","title":"Commandes","text":"<pre><code>python scripts/fine_tune_instruction.py \\\n  --model-id GhostPunishR/labcore-llm-50M \\\n  --dataset yahma/alpaca-cleaned \\\n  --dataset-split train \\\n  --output-dir outputs/lora_instruction \\\n  --config configs/bpe_rope_flash/bpe_50M_rope_flash.toml \\\n  --max-samples 20000 \\\n  --epochs 1\n</code></pre> <p>D\u00e9pendances:</p> <pre><code>python -m pip install -e \".[torch,hf,finetune]\"\n</code></pre>"},{"location":"fr/fine-tuning/#fichiers-de-sortie-artefacts-produits","title":"Fichiers de sortie / artefacts produits","text":"<ul> <li><code>outputs/lora_instruction/</code> (adapter LoRA + fichiers tokenizer)</li> </ul>"},{"location":"fr/fine-tuning/#mapping-du-dataset","title":"Mapping du dataset","text":"<p>Le script accepte plusieurs alias de champs:</p> <ul> <li>Instruction: <code>instruction</code>, <code>question</code>, <code>prompt</code></li> <li>Input: <code>input</code>, <code>context</code></li> <li>Output: <code>output</code>, <code>response</code>, <code>answer</code></li> </ul>"},{"location":"fr/fine-tuning/#erreurs-frequentes","title":"Erreurs fr\u00e9quentes","text":"<ul> <li>D\u00e9pendances HF manquantes: voir Torch not installed.</li> <li>OOM en fine-tuning: voir Out of memory.</li> <li>Incompatibilit\u00e9 mod\u00e8le/tokenizer de base: v\u00e9rifier la config et la compatibilit\u00e9 mod\u00e8le avant lancement.</li> </ul> <p>Note</p> <p>Le fine-tuning utilise la stack HF Trainer et \u00e9crit dans <code>outputs/</code> plut\u00f4t que dans <code>checkpoints/</code>.</p>"},{"location":"fr/fine-tuning/#suite-liens","title":"Suite / liens","text":"<ul> <li>R\u00e9f\u00e9rence de configuration</li> <li>Export et d\u00e9ploiement</li> <li>Op\u00e9rations</li> </ul>"},{"location":"fr/getting-started/","title":"D\u00e9marrage","text":"<p>Utilisez cette page pour un premier run reproductible en environ 5 minutes et v\u00e9rifier que l'environnement est sain. Pr\u00e9requis: Python <code>3.11+</code>, <code>pip</code> et GPU CUDA optionnel.</p>"},{"location":"fr/getting-started/#commandes","title":"Commandes","text":"<p>Installer les d\u00e9pendances:</p> <pre><code>python -m pip install -e \".[torch,dev]\"\n</code></pre> <p>V\u00e9rifier CUDA:</p> <pre><code>python -c \"import torch; print(torch.cuda.is_available()); print(torch.version.cuda)\"\n</code></pre> <p>D\u00e9marrage rapide (preset de r\u00e9f\u00e9rence: tinyshakespeare + char + <code>configs/base.toml</code>):</p> <pre><code>python scripts/prepare_data.py --dataset tinyshakespeare --tokenizer char --output-format txt --output-dir data/processed\npython train.py --config configs/base.toml --tokenizer char --max-iters 5000\npython generate.py --checkpoint checkpoints/ckpt_last.pt --meta data/processed/meta.json --tokenizer char --prompt \"To be\"\n</code></pre> <p>Tip</p> <p>Pour un smoke test strict de 5 minutes, lancez l'entra\u00eenement, attendez la premi\u00e8re \u00e9valuation/checkpoint, puis stoppez et lancez la g\u00e9n\u00e9ration.</p>"},{"location":"fr/getting-started/#fichiers-de-sortie-artefacts-produits","title":"Fichiers de sortie / artefacts produits","text":"<ul> <li><code>data/processed/meta.json</code></li> <li><code>data/processed/train.txt</code>, <code>data/processed/val.txt</code></li> <li><code>checkpoints/ckpt_last.pt</code></li> <li><code>checkpoints/train_log.json</code></li> </ul>"},{"location":"fr/getting-started/#checklist-de-succes","title":"Checklist de succ\u00e8s","text":"<ul> <li>Les logs affichent au moins deux lignes <code>train_loss</code> et une tendance \u00e0 la baisse.</li> <li>Un checkpoint existe dans <code>checkpoints/ckpt_last.pt</code>.</li> <li><code>generate.py</code> retourne un texte non vide.</li> </ul>"},{"location":"fr/getting-started/#erreurs-frequentes","title":"Erreurs fr\u00e9quentes","text":"<ul> <li><code>ModuleNotFoundError: torch</code>: voir Torch not installed.</li> <li>CUDA attendu mais indisponible: voir CUDA not detected.</li> <li>Mauvais chemin de m\u00e9tadonn\u00e9es: voir Meta path mismatch.</li> </ul> <p>Warning</p> <p>Gardez <code>--checkpoint</code> et <code>--meta</code> align\u00e9s sur le m\u00eame run. M\u00e9langer des fichiers de runs diff\u00e9rents donne des r\u00e9sultats trompeurs.</p>"},{"location":"fr/getting-started/#suite-liens","title":"Suite / liens","text":"<ul> <li>Pipeline de donn\u00e9es</li> <li>Entra\u00eenement</li> <li>D\u00e9pannage</li> </ul>"},{"location":"fr/inference-and-demo/","title":"Inf\u00e9rence et d\u00e9mo","text":"<p>Utilisez cette page pour lancer une g\u00e9n\u00e9ration CLI d\u00e9terministe et la d\u00e9mo Gradio locale. Pr\u00e9requis: checkpoint entra\u00een\u00e9 (<code>checkpoints/ckpt_last.pt</code>) et m\u00e9tadonn\u00e9es correspondantes.</p>"},{"location":"fr/inference-and-demo/#commandes","title":"Commandes","text":"<p>G\u00e9n\u00e9ration CLI (chemins de r\u00e9f\u00e9rence):</p> <pre><code>python generate.py \\\n  --checkpoint checkpoints/ckpt_last.pt \\\n  --meta data/processed/meta.json \\\n  --tokenizer char \\\n  --prompt \"To be\" \\\n  --max-new-tokens 200 \\\n  --temperature 0.9 \\\n  --top-k 40 \\\n  --device cpu\n</code></pre> <p>D\u00e9mo Gradio depuis checkpoint local:</p> <pre><code>python demo_gradio.py \\\n  --source local \\\n  --checkpoint checkpoints/ckpt_last.pt \\\n  --meta data/processed/meta.json \\\n  --device cpu \\\n  --port 7860\n</code></pre> <p>D\u00e9mo Gradio depuis Hugging Face:</p> <pre><code>python demo_gradio.py \\\n  --source hf \\\n  --repo-id GhostPunishR/labcore-llm-50M \\\n  --device cpu \\\n  --port 7860\n</code></pre>"},{"location":"fr/inference-and-demo/#reglages-de-generation-stables-mode-debug","title":"R\u00e9glages de g\u00e9n\u00e9ration stables (mode debug)","text":"<p>Utilisez un sampling conservateur pour d\u00e9boguer la reproductibilit\u00e9:</p> <ul> <li><code>temperature = 0.2</code> pour r\u00e9duire l'al\u00e9atoire</li> <li><code>top-k = 20</code> (ou moins)</li> <li><code>max-new-tokens = 80</code> pour des checks rapides</li> </ul> <p>Tip</p> <p>Si la qualit\u00e9 chute brutalement, v\u00e9rifiez d'abord que <code>--meta</code> correspond au m\u00eame run tokenizer/checkpoint.</p>"},{"location":"fr/inference-and-demo/#controles-de-sampling","title":"Contr\u00f4les de sampling","text":"<ul> <li><code>temperature</code>: met \u00e0 l'\u00e9chelle les logits avant sampling.</li> <li><code>top_k</code>: conserve uniquement les <code>k</code> tokens les plus probables.</li> <li><code>top_p</code>: seuil nucleus sampling (<code>1.0</code> le d\u00e9sactive).</li> <li><code>repetition_penalty</code>: p\u00e9nalise les tokens d\u00e9j\u00e0 g\u00e9n\u00e9r\u00e9s (<code>1.0</code> le d\u00e9sactive).</li> <li><code>use_kv_cache</code>: active le KV-cache pendant le d\u00e9codage.</li> <li><code>stream</code>: active le streaming token-by-token.</li> </ul> <p><code>top_p</code> et <code>repetition_penalty</code> sont lus depuis <code>[generation]</code> avec <code>generate.py --config ...</code>.</p> <p>Exemple stable RTX 4060:</p> <pre><code>[generation]\ntemperature = 0.6\ntop_k = 50\ntop_p = 0.9\nrepetition_penalty = 1.1\nuse_kv_cache = true\nstream = true\nsystem_prompt = \"You are LabCore LLM.\"\nmax_history_turns = 6\n</code></pre>"},{"location":"fr/inference-and-demo/#generation-reproductible","title":"G\u00e9n\u00e9ration reproductible","text":"<ul> <li><code>seed</code> (graine d'initialisation): fige les RNG Python/NumPy/Torch pour un sampling reproductible.</li> <li><code>deterministic</code>: active les algorithmes d\u00e9terministes PyTorch (<code>warn_only=True</code>) et les r\u00e9glages cuDNN d\u00e9terministes.</li> </ul> <pre><code>[generation]\ntemperature = 0.6\ntop_k = 50\ntop_p = 0.9\nrepetition_penalty = 1.1\nuse_kv_cache = true\nstream = true\nsystem_prompt = \"You are LabCore LLM.\"\nmax_history_turns = 6\nseed = 1337\ndeterministic = true\n</code></pre>"},{"location":"fr/inference-and-demo/#kv-cache-streaming-et-chat","title":"KV-cache, streaming et chat","text":"<ul> <li>Le KV-cache acc\u00e9l\u00e8re la g\u00e9n\u00e9ration en r\u00e9utilisant les key/value attention pass\u00e9es, au lieu de recalculer tout le contexte.</li> <li>Le streaming met \u00e0 jour la sortie de d\u00e9mo de mani\u00e8re incr\u00e9mentale, token par token.</li> <li>Le chat multi-tour construit un prompt avec des marqueurs texte simples:</li> <li><code>&lt;|system|&gt;</code></li> <li><code>&lt;|user|&gt;</code></li> <li><code>&lt;|assistant|&gt;</code></li> <li><code>max_history_turns</code> conserve seulement les tours les plus r\u00e9cents pour borner la longueur du prompt.</li> </ul> <pre><code>[generation]\nuse_kv_cache = true\nstream = true\nsystem_prompt = \"You are LabCore LLM.\"\nmax_history_turns = 6\n</code></pre>"},{"location":"fr/inference-and-demo/#fichiers-de-sortie-artefacts-produits","title":"Fichiers de sortie / artefacts produits","text":"<ul> <li>CLI: texte g\u00e9n\u00e9r\u00e9 dans le terminal</li> <li>D\u00e9mo: texte g\u00e9n\u00e9r\u00e9 dans l'interface Gradio</li> <li>Aucun nouveau fichier mod\u00e8le, sauf export explicite</li> </ul>"},{"location":"fr/inference-and-demo/#erreurs-frequentes","title":"Erreurs fr\u00e9quentes","text":"<ul> <li>M\u00e9tadonn\u00e9es tokenizer char manquantes: voir Char vocab missing.</li> <li>Mauvais mapping des m\u00e9tadonn\u00e9es (<code>txt</code> vs <code>bin</code>): voir Meta path mismatch.</li> <li>Fallback CUDA: voir CUDA not detected.</li> </ul>"},{"location":"fr/inference-and-demo/#suite-liens","title":"Suite / liens","text":"<ul> <li>Export et d\u00e9ploiement</li> <li>Ajustement fin</li> <li>D\u00e9pannage</li> </ul>"},{"location":"fr/operations/","title":"Op\u00e9rations","text":"<p>Utilisez cette page pour l'hygi\u00e8ne des dossiers op\u00e9rationnels, les checks de validation et les workflows de release/s\u00e9curit\u00e9. Pr\u00e9requis: d\u00e9pendances du projet install\u00e9es dans l'environnement actif.</p>"},{"location":"fr/operations/#repertoires-dartefacts","title":"R\u00e9pertoires d'artefacts","text":"<ul> <li><code>data/</code>: datasets pr\u00e9par\u00e9s, incluant <code>data/meta.json</code> pour les runs <code>bin</code>.</li> <li><code>checkpoints/</code>: sorties d'entra\u00eenement (<code>ckpt_last.pt</code>, <code>train_log.json</code>, <code>ckpt_best.pt</code> si activ\u00e9).</li> <li><code>outputs/</code>: artefacts export\u00e9s (<code>hf_export/</code>, <code>gguf/</code>, sorties de fine-tuning).</li> <li><code>runs/</code>: logs d'exp\u00e9rience optionnels ou exports de trackers externes (pas cr\u00e9\u00e9s automatiquement par les scripts core).</li> </ul> <p>Note</p> <p>Les scripts core \u00e9crivent surtout dans <code>checkpoints/</code> et <code>outputs/</code>. Gardez ces dossiers dans vos notes de run, mais ne commitez pas de gros artefacts.</p>"},{"location":"fr/operations/#commandes","title":"Commandes","text":"<p>Checks qualit\u00e9 locaux:</p> <pre><code>python -m pytest -q\nruff check src scripts tests train.py generate.py demo_gradio.py --select E9,F63,F7,F82\n</code></pre> <p>Checks de build docs:</p> <pre><code>python -m pip install -r docs/requirements.txt\npython -m mkdocs build\n</code></pre>"},{"location":"fr/operations/#fichiers-de-sortie-artefacts-produits","title":"Fichiers de sortie / artefacts produits","text":"<ul> <li>Logs tests/lint dans le terminal</li> <li>Site docs dans <code>site/</code> apr\u00e8s <code>mkdocs build</code></li> <li>Pipelines CI: <code>.github/workflows/ci.yml</code> et <code>.github/workflows/docs.yml</code></li> </ul>"},{"location":"fr/operations/#securite-et-release","title":"S\u00e9curit\u00e9 et release","text":"<p>Signalement s\u00e9curit\u00e9 (voir <code>SECURITY.md</code>):</p> <ul> <li>Utiliser GitHub Security Advisories pour les vuln\u00e9rabilit\u00e9s</li> <li>Inclure l'impact, les composants affect\u00e9s et les \u00e9tapes de reproduction</li> </ul> <p>Flux de release (voir <code>RELEASE.md</code>):</p> <ol> <li>V\u00e9rifier que la CI est verte.</li> <li>Lancer les validations locales.</li> <li>Mettre \u00e0 jour la version et le changelog.</li> <li>Tagger et publier la release.</li> </ol>"},{"location":"fr/operations/#erreurs-frequentes","title":"Erreurs fr\u00e9quentes","text":"<ul> <li>D\u00e9pendances manquantes: voir Torch not installed.</li> <li>Confusion metadata/path: voir Meta path mismatch.</li> <li>CUDA attendu mais indisponible: voir CUDA not detected.</li> </ul>"},{"location":"fr/operations/#liens","title":"Liens","text":"<ul> <li>D\u00e9pannage</li> <li>Performances</li> <li>Guide d\u00e9veloppeur</li> </ul>"},{"location":"fr/training/","title":"Entra\u00eenement","text":"<p>Utilisez cette page pour lancer, monitorer et checkpoint un entra\u00eenement avec des r\u00e9glages coh\u00e9rents. Pr\u00e9requis: dataset et m\u00e9tadonn\u00e9es pr\u00e9par\u00e9s depuis Pipeline de donn\u00e9es.</p>"},{"location":"fr/training/#commandes","title":"Commandes","text":"<p>Commande de r\u00e9f\u00e9rence:</p> <pre><code>python train.py --config configs/base.toml --tokenizer char --max-iters 5000\n</code></pre> <p>Exemples de surcharge du device:</p> <pre><code>python train.py --config configs/base.toml --tokenizer char --max-iters 5000 --device cpu\npython train.py --config configs/base.toml --tokenizer char --max-iters 5000 --device cuda\n</code></pre> <p>Flags <code>train.py</code>:</p> <ul> <li><code>--config</code>: chemin du preset TOML (<code>CONFIG_EXAMPLE = configs/base.toml</code>)</li> <li><code>--max-iters</code>: surcharge runtime du nombre total d'it\u00e9rations</li> <li><code>--device</code>: <code>cpu</code> ou <code>cuda</code></li> <li><code>--tokenizer</code>: <code>char</code> ou <code>bpe</code></li> </ul>"},{"location":"fr/training/#precision-et-accumulation-de-gradients","title":"Pr\u00e9cision et accumulation de gradients","text":"<p>Configurez ces valeurs dans <code>[training]</code>:</p> <ul> <li><code>grad_accum_steps</code>: facteur d'accumulation de gradients (valeur par d\u00e9faut <code>1</code>)</li> <li><code>precision</code>: <code>fp32</code> (valeur par d\u00e9faut), <code>fp16</code> ou <code>bf16</code></li> </ul> <p><code>effective_batch_size = batch_size * grad_accum_steps</code></p> <p>La pr\u00e9cision mixte est activ\u00e9e seulement si <code>device = \"cuda\"</code> et <code>precision != \"fp32\"</code>. Sur CPU, l'entra\u00eenement repasse en <code>fp32</code>.</p> <p>Exemple RTX 4060:</p> <pre><code>[training]\nbatch_size = 8\ngrad_accum_steps = 4\nprecision = \"fp16\"\n</code></pre>"},{"location":"fr/training/#meilleur-checkpoint-et-arret-anticipe","title":"Meilleur checkpoint et arr\u00eat anticip\u00e9","text":"<ul> <li><code>save_best</code> sauvegarde <code>checkpoints/ckpt_best.pt</code> quand la loss de validation s'am\u00e9liore d'au moins <code>early_stopping_min_delta</code>.</li> <li><code>early_stopping</code> est d\u00e9sactiv\u00e9 par d\u00e9faut.</li> <li><code>early_stopping_patience</code> compte les \u00e9valuations sans am\u00e9lioration suffisante.</li> <li><code>early_stopping_min_delta</code> d\u00e9finit le seuil minimal d'am\u00e9lioration de <code>val_loss</code>.</li> </ul> <pre><code>[training]\nearly_stopping = true\nearly_stopping_patience = 3\nearly_stopping_min_delta = 0.001\nsave_best = true\n</code></pre>"},{"location":"fr/training/#data_format-et-mapping-des-metadonnees","title":"<code>data_format</code> et mapping des m\u00e9tadonn\u00e9es","text":"Mode entra\u00eenement Valeur config Artefacts attendus Chemin m\u00e9tadonn\u00e9es Pipeline texte <code>training.data_format = \"txt\"</code> <code>data/processed/train.txt</code> + <code>data/processed/val.txt</code> (ou <code>.npy</code>) <code>data/processed/meta.json</code> (<code>META_TXT</code>) Pipeline binaire <code>training.data_format = \"bin\"</code> <code>data/train.bin</code> + <code>data/val.bin</code> <code>data/meta.json</code> (<code>META_BIN</code>) <p>Note</p> <p>En mode binaire, si <code>data.processed_dir</code> pointe vers <code>data/processed</code>, <code>train.py</code> v\u00e9rifie automatiquement le parent (<code>data/</code>) pour <code>train.bin</code> et <code>val.bin</code>.</p>"},{"location":"fr/training/#checkpointing-et-reprise","title":"Checkpointing et reprise","text":"<p>Fichiers produits pendant l'entra\u00eenement:</p> <ul> <li><code>checkpoints/ckpt_last.pt</code> (<code>CHECKPOINT</code>)</li> <li><code>checkpoints/ckpt_best.pt</code> (si <code>save_best = true</code>)</li> <li><code>checkpoints/train_log.json</code></li> </ul> <p>Warning</p> <p>La reprise native depuis checkpoint n'est pas encore impl\u00e9ment\u00e9e dans <code>train.py</code>. <code>ckpt_last.pt</code> sert surtout \u00e0 l'inf\u00e9rence/export et \u00e0 l'inspection d'\u00e9tat.</p>"},{"location":"fr/training/#erreurs-frequentes","title":"Erreurs fr\u00e9quentes","text":"<ul> <li>Binaires manquants: voir Binary shards not found.</li> <li>Mauvais chemin de m\u00e9tadonn\u00e9es: voir Meta path mismatch.</li> <li>\u00c9chec d'inf\u00e9rence du vocabulaire: voir Char vocab missing.</li> <li>Avertissement de fallback CUDA: voir CUDA not detected.</li> </ul>"},{"location":"fr/training/#suite-liens","title":"Suite / liens","text":"<ul> <li>Inf\u00e9rence et d\u00e9mo</li> <li>Export et d\u00e9ploiement</li> <li>D\u00e9pannage</li> </ul>"},{"location":"fr/troubleshooting/","title":"D\u00e9pannage","text":"<p>Utilisez cette page pour diagnostiquer rapidement les erreurs fr\u00e9quentes de setup, de donn\u00e9es et d'ex\u00e9cution. Toutes les ancres ci-dessous sont r\u00e9f\u00e9renc\u00e9es depuis les pages des guides.</p>"},{"location":"fr/troubleshooting/#installation","title":"Installation","text":""},{"location":"fr/troubleshooting/#torch-not-installed","title":"Torch non install\u00e9","text":"<p>Sympt\u00f4mes: <code>ModuleNotFoundError: torch</code>, ou \u00e9chec des scripts \u00e0 l'import.</p> <p>Correctif:</p> <pre><code>python -m pip install -e \".[torch,dev]\"\n</code></pre> <p>Utilisez le s\u00e9lecteur officiel si vous avez besoin d'un build CUDA sp\u00e9cifique: https://pytorch.org/get-started/locally/</p>"},{"location":"fr/troubleshooting/#cuda-not-detected","title":"CUDA non d\u00e9tect\u00e9","text":"<p>Sympt\u00f4mes: <code>torch.cuda.is_available()</code> retourne <code>False</code>, ou fallback CPU dans les scripts.</p> <p>V\u00e9rification rapide:</p> <pre><code>python -c \"import torch; print(torch.cuda.is_available()); print(torch.version.cuda)\"\n</code></pre> <p>Points \u00e0 v\u00e9rifier:</p> <ul> <li>Driver NVIDIA install\u00e9 et \u00e0 jour</li> <li>Build PyTorch compatible avec votre runtime CUDA</li> <li>M\u00eame environnement Python utilis\u00e9 pour l'installation et l'ex\u00e9cution</li> </ul>"},{"location":"fr/troubleshooting/#donnees-et-metadonnees","title":"Donn\u00e9es et m\u00e9tadonn\u00e9es","text":""},{"location":"fr/troubleshooting/#meta-path-mismatch","title":"Chemin des m\u00e9tadonn\u00e9es incoh\u00e9rent","text":"<p>Sympt\u00f4mes: g\u00e9n\u00e9ration/entra\u00eenement incorrects ou impossibles \u00e0 charger via les m\u00e9tadonn\u00e9es tokenizer.</p> <p>Mapping attendu:</p> <ul> <li>Pipeline <code>txt</code> -&gt; <code>data/processed/meta.json</code></li> <li>Pipeline <code>bin</code> -&gt; <code>data/meta.json</code></li> </ul>"},{"location":"fr/troubleshooting/#binary-shards-not-found","title":"Shards binaires introuvables","text":"<p>Sympt\u00f4mes: <code>Binary shards not found</code> avec <code>training.data_format = \"bin\"</code>.</p> <p>Correctif:</p> <pre><code>python scripts/prepare_data.py --dataset tinyshakespeare --tokenizer char --output-format bin --output-dir data/processed\n</code></pre>"},{"location":"fr/troubleshooting/#char-vocab-missing","title":"Vocabulaire char manquant","text":"<p>Sympt\u00f4mes: <code>Char tokenizer requires vocab in meta.json</code>.</p> <p>Correctif:</p> <pre><code>python scripts/prepare_data.py --dataset tinyshakespeare --tokenizer char --output-format txt --output-dir data/processed\n</code></pre> <p>Puis passez le <code>--meta data/processed/meta.json</code> correspondant aux commandes de g\u00e9n\u00e9ration/export.</p>"},{"location":"fr/troubleshooting/#execution","title":"Ex\u00e9cution","text":""},{"location":"fr/troubleshooting/#oom-errors","title":"M\u00e9moire insuffisante","text":"<p>R\u00e9duire dans cet ordre:</p> <ol> <li><code>training.batch_size</code></li> <li><code>model.block_size</code></li> <li><code>training.gradient_accumulation_steps</code></li> <li>Taille du mod\u00e8le / complexit\u00e9 du preset</li> </ol>"},{"location":"fr/troubleshooting/#flashattention-not-available","title":"FlashAttention indisponible","text":"<p>LabCore applique des fallbacks automatiques:</p> <ul> <li>FlashAttention (priorit\u00e9, si disponible)</li> <li>Fallback SDPA PyTorch</li> <li>Fallback attention causale standard</li> </ul>"},{"location":"fr/troubleshooting/#windows-path-policy","title":"Probl\u00e8mes de chemin et de policy Windows","text":"<ul> <li>Activez le venv avant les commandes.</li> <li>Lancez les commandes depuis la racine du repo.</li> <li>Citez les chemins si des espaces sont pr\u00e9sents.</li> <li>Si la policy PowerShell bloque les scripts, utilisez les commandes <code>python ...</code> directement.</li> </ul>"}]}
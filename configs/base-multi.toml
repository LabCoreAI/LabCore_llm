[general]
run_name = "base_multi_20m"
seed = 1337
tokenizer = "bpe"

[data]
dataset = "wikitext"
processed_dir = "data"

[model]
block_size = 512
n_layer = 4
n_head = 8
n_embd = 304
vocab_size = 50257
dropout = 0.1
bias = false
use_rope = true
use_flash = true

[optimizer]
learning_rate = 3e-4
weight_decay = 0.1
beta1 = 0.9
beta2 = 0.95
grad_clip = 1.0

[training]
batch_size = 8
max_iters = 30000
eval_interval = 200
eval_iters = 50
log_interval = 20
grad_accum_steps = 8
precision = "fp16"
device = "cuda"
checkpoint_dir = "checkpoints_multi"
data_format = "bin"
warmup_iters = 1000
lr_decay_iters = 30000
min_lr = 3e-5
save_interval = 500

[generation]
max_new_tokens = 200
temperature = 0.7
top_k = 50
top_p = 0.95
repetition_penalty = 1.05
seed = 1337
deterministic = false

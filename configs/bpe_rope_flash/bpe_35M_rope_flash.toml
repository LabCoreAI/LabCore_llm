[general]
run_name = "bpe_35M_rope_flash"
seed = 1337
tokenizer = "bpe"

[data]
dataset = "wikitext"
processed_dir = "data"

[model]
n_layer = 5
n_head = 8
n_embd = 448
block_size = 512
vocab_size = 50257
dropout = 0.1
bias = false
use_rope = true
use_flash = true

[optimizer]
learning_rate = 3e-4
weight_decay = 0.1
beta1 = 0.9
beta2 = 0.95
grad_clip = 1.0

[training]
batch_size = 8
gradient_accumulation_steps = 8
max_iters = 10000
warmup_iters = 500
lr_decay_iters = 10000
min_lr = 3e-5
eval_interval = 200
eval_iters = 50
log_interval = 20
save_interval = 500
device = "cuda"
precision = "fp16"
checkpoint_dir = "checkpoints_multi"
data_format = "bin"

[generation]
max_new_tokens = 200
temperature = 0.7
top_k = 50
top_p = 0.95
repetition_penalty = 1.05
seed = 1337
deterministic = false

[general]
run_name = "bpe_medium_35M"
seed = 1337
tokenizer = "bpe"

[model]
n_layer = 5
n_head = 8
n_embd = 448
block_size = 512          # safe pour 8GB
vocab_size = 50257
dropout = 0.1
bias = false

[optimizer]
learning_rate = 3e-4
weight_decay = 0.1
beta1 = 0.9
beta2 = 0.95
grad_clip = 1.0

[training]
batch_size = 8
gradient_accumulation_steps = 8   #  batch effectif 64
max_iters = 10000
warmup_iters = 500
lr_decay_iters = 10000
min_lr = 3e-5
eval_interval = 200
eval_iters = 50
save_interval = 500

